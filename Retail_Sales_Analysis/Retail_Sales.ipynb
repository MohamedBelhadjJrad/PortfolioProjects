{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRHTm-HSptyj",
        "outputId": "6f564eab-0172-4bc9-a75a-b41475a2c60e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1dGVFA_jrfr",
        "outputId": "841ad79e-5011-474a-ba34-b8c8d5b8a319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Data Integration and Cleaning\n",
            "=====================================\n",
            "\n",
            "1. Loading the dataset...\n",
            "Dataset loaded with 30000 rows and 11 columns.\n",
            "\n",
            "2. Initial data exploration...\n",
            "\n",
            "First 5 rows:\n",
            "     Store ID  Product ID      Date  Units Sold  Sales Revenue (USD)  Discount Percentage  Marketing Spend (USD)             Store Location Product Category Day of the Week  Holiday Effect\n",
            "0  Spearsland    52372247  1/1/2022           9              2741.69                   20                     81                   Tanzania        Furniture        Saturday           False\n",
            "1  Spearsland    52372247  1/2/2022           7              2665.53                    0                      0                 Mauritania        Furniture          Sunday           False\n",
            "2  Spearsland    52372247  1/3/2022           1               380.79                    0                      0  Saint Pierre and Miquelon        Furniture          Monday           False\n",
            "3  Spearsland    52372247  1/4/2022           4              1523.16                    0                      0                  Australia        Furniture         Tuesday           False\n",
            "4  Spearsland    52372247  1/5/2022           2               761.58                    0                      0                  Swaziland        Furniture       Wednesday           False\n",
            "\n",
            "Data types:\n",
            "Store ID                  object\n",
            "Product ID                 int64\n",
            "Date                      object\n",
            "Units Sold                 int64\n",
            "Sales Revenue (USD)      float64\n",
            "Discount Percentage        int64\n",
            "Marketing Spend (USD)      int64\n",
            "Store Location            object\n",
            "Product Category          object\n",
            "Day of the Week           object\n",
            "Holiday Effect              bool\n",
            "dtype: object\n",
            "\n",
            "Basic statistics:\n",
            "                         count        mean         std        min         25%         50%         75%         max\n",
            "Product ID            30000.00 44612943.58 27797592.77 3636541.00 22286000.00 40024486.00 65593523.00 96282526.00\n",
            "Units Sold            30000.00        6.16        3.32       0.00        4.00        6.00        8.00       56.00\n",
            "Sales Revenue (USD)   30000.00     2749.51     2568.64       0.00      882.59     1902.42     3863.92    27165.88\n",
            "Discount Percentage   30000.00        2.97        5.97       0.00        0.00        0.00        0.00       20.00\n",
            "Marketing Spend (USD) 30000.00       49.94       64.40       0.00        0.00        1.00      100.00      199.00\n",
            "\n",
            "3. Checking for missing values...\n",
            "Store ID                 0\n",
            "Product ID               0\n",
            "Date                     0\n",
            "Units Sold               0\n",
            "Sales Revenue (USD)      0\n",
            "Discount Percentage      0\n",
            "Marketing Spend (USD)    0\n",
            "Store Location           0\n",
            "Product Category         0\n",
            "Day of the Week          0\n",
            "Holiday Effect           0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, Percentage]\n",
            "Index: []\n",
            "\n",
            "4. Checking for duplicates...\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "5. Converting data types...\n",
            "Date components extracted.\n",
            "\n",
            "6. Checking for outliers in numerical columns...\n",
            "                  Column  Lower Bound  Upper Bound  Outlier Count  Outlier Percentage\n",
            "0             Units Sold        -2.00        14.00            594                1.98\n",
            "1    Sales Revenue (USD)     -3589.40      8335.91           1305                4.35\n",
            "2    Discount Percentage         0.00         0.00           7154               23.85\n",
            "3  Marketing Spend (USD)      -150.00       250.00              0                0.00\n",
            "\n",
            "7. Handling outliers...\n",
            "Capped upper outliers in Units Sold at 14.0\n",
            "Capped upper outliers in Sales Revenue (USD) at 8335.911250000001\n",
            "\n",
            "8. Feature engineering...\n",
            "Added Revenue_per_Unit feature\n",
            "Added Marketing_ROI feature\n",
            "Added Has_Discount feature\n",
            "Added Discount_Level feature\n",
            "Added Is_Weekend feature\n",
            "Added Season feature\n",
            "\n",
            "9. Adding regional categorization...\n",
            "Added Region feature\n",
            "\n",
            "Store locations by region:\n",
            "Region\n",
            "Europe                        6679\n",
            "Africa                        6129\n",
            "Asia                          4028\n",
            "Caribbean                     3286\n",
            "Oceania                       3205\n",
            "Middle East & North Africa    2741\n",
            "Latin America                 1702\n",
            "North America                 1401\n",
            "Other                          829\n",
            "Name: count, dtype: int64\n",
            "\n",
            "10. Performing data validation...\n",
            "Date range: 2022-01-01 00:00:00 to 2024-01-01 00:00:00\n",
            "\n",
            "Unique values in Store ID: 1\n",
            "['Spearsland']\n",
            "\n",
            "Unique values in Product Category: 4\n",
            "['Furniture' 'Electronics' 'Groceries' 'Clothing']\n",
            "\n",
            "Unique values in Day of the Week: 7\n",
            "['Saturday' 'Sunday' 'Monday' 'Tuesday' 'Wednesday' 'Thursday' 'Friday']\n",
            "\n",
            "11. Saving cleaned dataset...\n",
            "Cleaned dataset saved to /content/cleaned_data/retail_sales_cleaned.csv\n",
            "\n",
            "12. Generating data quality report...\n",
            "Data quality report saved to /content/data_quality_reports/data_quality_summary.txt\n",
            "\n",
            "13. Generating summary statistics for cleaned dataset...\n",
            "Summary statistics saved to /content/data_quality_reports/cleaned_data_summary_stats.csv\n",
            "\n",
            "Step 1: Data Integration and Cleaning completed successfully!\n",
            "The cleaned dataset is ready for advanced analysis.\n",
            "\n",
            "Summary of new features added:\n",
            "- Temporal features: Year, Month, Day, Month_Name, Quarter, Week_of_Year, Day_of_Year, Season\n",
            "- Business metrics: Revenue_per_Unit, Marketing_ROI\n",
            "- Categorical features: Has_Discount, Discount_Level, Is_Weekend, Region\n",
            "\n",
            "Next steps: Proceed to Step 2 - Design unified analysis framework\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 1: Data Integration and Cleaning for Retail Sales Analysis\n",
        "This script performs comprehensive data cleaning and preparation on the retail sales dataset.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Set display options for better output readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Create directories for outputs\n",
        "if not os.path.exists('/content/cleaned_data'):\n",
        "    os.makedirs('/content/cleaned_data')\n",
        "if not os.path.exists('/content/data_quality_reports'):\n",
        "    os.makedirs('/content/data_quality_reports')\n",
        "\n",
        "print(\"Step 1: Data Integration and Cleaning\")\n",
        "print(\"=====================================\\n\")\n",
        "\n",
        "# 1. Load the dataset\n",
        "print(\"1. Loading the dataset...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Retail_sales.csv')\n",
        "print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\\n\")\n",
        "\n",
        "# 2. Initial data exploration\n",
        "print(\"2. Initial data exploration...\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe().T)\n",
        "\n",
        "# 3. Check for missing values\n",
        "print(\"\\n3. Checking for missing values...\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_data = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "print(missing_data[missing_data['Missing Values'] > 0])\n",
        "\n",
        "# 4. Check for duplicates\n",
        "print(\"\\n4. Checking for duplicates...\")\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"Removing duplicates...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Dataset now has {df.shape[0]} rows after removing duplicates.\")\n",
        "\n",
        "# 5. Data type conversion and formatting\n",
        "print(\"\\n5. Converting data types...\")\n",
        "\n",
        "# Convert date to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract date components\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month_Name'] = df['Date'].dt.month_name()\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
        "df['Day_of_Year'] = df['Date'].dt.dayofyear\n",
        "\n",
        "print(\"Date components extracted.\")\n",
        "\n",
        "# 6. Check for outliers in numerical columns\n",
        "print(\"\\n6. Checking for outliers in numerical columns...\")\n",
        "numerical_cols = ['Units Sold', 'Sales Revenue (USD)', 'Discount Percentage', 'Marketing Spend (USD)']\n",
        "\n",
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound, len(outliers)\n",
        "\n",
        "# Check outliers in each numerical column\n",
        "outlier_report = []\n",
        "for col in numerical_cols:\n",
        "    outliers, lower_bound, upper_bound, count = detect_outliers(df, col)\n",
        "    outlier_report.append({\n",
        "        'Column': col,\n",
        "        'Lower Bound': lower_bound,\n",
        "        'Upper Bound': upper_bound,\n",
        "        'Outlier Count': count,\n",
        "        'Outlier Percentage': (count / len(df)) * 100\n",
        "    })\n",
        "\n",
        "    # Create boxplot to visualize outliers\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'/content/data_quality_reports/{col}_boxplot.png')\n",
        "    plt.close()\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_report)\n",
        "print(outlier_df)\n",
        "\n",
        "# 7. Handle outliers\n",
        "print(\"\\n7. Handling outliers...\")\n",
        "# For this analysis, we'll cap outliers rather than removing them\n",
        "for col in numerical_cols:\n",
        "    _, lower_bound, upper_bound, _ = detect_outliers(df, col)\n",
        "    # Only cap if the column has outliers and it makes business sense\n",
        "    if col in ['Units Sold', 'Sales Revenue (USD)']:\n",
        "        # Cap upper outliers only (lower values are valid)\n",
        "        df[col] = df[col].clip(upper=upper_bound)\n",
        "        print(f\"Capped upper outliers in {col} at {upper_bound}\")\n",
        "\n",
        "# 8. Feature engineering\n",
        "print(\"\\n8. Feature engineering...\")\n",
        "\n",
        "# Calculate revenue per unit\n",
        "df['Revenue_per_Unit'] = df['Sales Revenue (USD)'] / df['Units Sold']\n",
        "print(\"Added Revenue_per_Unit feature\")\n",
        "\n",
        "# Calculate marketing ROI\n",
        "df['Marketing_ROI'] = np.where(\n",
        "    df['Marketing Spend (USD)'] > 0,\n",
        "    (df['Sales Revenue (USD)'] - df['Marketing Spend (USD)'])/ df['Marketing Spend (USD)'],\n",
        "    0  # Set to 0 if marketing spend is 0\n",
        ")\n",
        "print(\"Added Marketing_ROI feature\")\n",
        "\n",
        "# Create discount flag\n",
        "df['Has_Discount'] = df['Discount Percentage'] > 0\n",
        "print(\"Added Has_Discount feature\")\n",
        "\n",
        "# Create discount level categories\n",
        "df['Discount_Level'] = pd.cut(\n",
        "    df['Discount Percentage'],\n",
        "    bins=[-1, 0, 5, 10, 15, 20, 100],\n",
        "    labels=['None', 'Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
        ")\n",
        "print(\"Added Discount_Level feature\")\n",
        "\n",
        "# Add day type (weekday/weekend)\n",
        "df['Is_Weekend'] = df['Day of the Week'].isin(['Saturday', 'Sunday'])\n",
        "print(\"Added Is_Weekend feature\")\n",
        "\n",
        "# Add season based on month\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'Winter'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'Spring'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'Summer'\n",
        "    else:\n",
        "        return 'Fall'\n",
        "\n",
        "df['Season'] = df['Month'].apply(get_season)\n",
        "print(\"Added Season feature\")\n",
        "\n",
        "# 9. Regional categorization\n",
        "print(\"\\n9. Adding regional categorization...\")\n",
        "\n",
        "# Define regions based on geographical proximity\n",
        "def categorize_region(location):\n",
        "    # Define regions\n",
        "    north_america = ['United States', 'Canada', 'Mexico', 'United States of America',\n",
        "                     'Panama', 'Costa Rica', 'Honduras', 'El Salvador', 'Greenland',\n",
        "                     'Nicaragua', 'Guatemala', 'Bermuda', 'Belize']\n",
        "    europe = ['United Kingdom', 'France', 'Germany', 'Italy', 'Spain', 'Netherlands',\n",
        "              'Belgium', 'Switzerland', 'Sweden', 'Norway', 'Finland', 'Denmark',\n",
        "              'Ireland', 'Portugal', 'Austria', 'Greece', 'Poland', 'Czech Republic',\n",
        "              'Hungary', 'Slovakia', 'Slovenia', 'Croatia', 'Romania', 'Bulgaria',\n",
        "              'Luxembourg', 'Malta', 'Cyprus', 'Iceland', 'Monaco', 'Andorra',\n",
        "              'Serbia', 'Montenegro', 'Bosnia and Herzegovina', 'Albania', 'North Macedonia',\n",
        "              'Kosovo', 'Lithuania', 'Latvia', 'Estonia', 'Ukraine', 'Belarus', 'Moldova',\n",
        "              'Liechtenstein', 'San Marino', 'Holy See (Vatican City State)',\n",
        "              'Jersey', 'Guernsey', 'Isle of Man', 'Faroe Islands', 'Svalbard & Jan Mayen Islands',\n",
        "              'Gibraltar', 'Swaziland','Slovakia (Slovak Republic)',\n",
        "              'Armenia', 'Russian Federation', 'Azerbaijan', 'Georgia']\n",
        "    asia = [ 'China', 'Japan', 'South Korea', 'India', 'Australia', 'New Zealand',\n",
        "                     'Singapore', 'Malaysia', 'Indonesia', 'Philippines', 'Thailand',\n",
        "                     'Vietnam', 'Taiwan', 'Hong Kong', 'Macau', 'Pakistan', 'Bangladesh',\n",
        "                     'Sri Lanka', 'Nepal', 'Bhutan', 'Maldives', 'Myanmar', 'Cambodia',\n",
        "                     'Laos', 'East Timor', 'Brunei Darussalam', 'Mongolia', 'Kazakhstan',\n",
        "                     'Uzbekistan', 'Kyrgyz Republic', 'Turkmenistan', 'Tajikistan',\n",
        "                     'Afghanistan', 'Korea', \"Lao People's Democratic Republic\", 'Timor-Leste']\n",
        "    middle_east_north_africa = [ 'UAE', 'Saudi Arabia', 'Israel', 'Turkey', 'Qatar', 'Kuwait', 'Bahrain',\n",
        "                                 'Oman', 'Jordan', 'Lebanon', 'Syria', 'Iraq', 'Iran', 'Yemen',\n",
        "                                 'Palestinian Territory', 'United Arab Emirates', 'Syrian Arab Republic',\n",
        "                                 # North African Arab countries\n",
        "                                 'Egypt', 'Algeria', 'Morocco', 'Tunisia', 'Libyan Arab Jamahiriya', 'Sudan', 'Western Sahara']\n",
        "    africa = ['South Africa', 'Kenya', 'Nigeria', 'Ghana', 'Ethiopia',\n",
        "              'Tanzania', 'Mauritania', 'Mali', 'Congo', 'Uganda', 'Rwanda', 'Burundi', 'Somalia',\n",
        "              'Gambia', 'Senegal', 'Guinea', 'Guinea-Bissau', 'Sierra Leone', 'Liberia', 'Cote d\\'Ivoire',\n",
        "              'Burkina Faso', 'Togo', 'Benin', 'Niger', 'Chad', 'Central African Republic', 'Cameroon',\n",
        "              'Gabon', 'Equatorial Guinea', 'Republic of the Congo', 'Democratic Republic of the Congo',\n",
        "              'Angola', 'Namibia', 'Botswana', 'Zimbabwe', 'Zambia', 'Malawi', 'Mozambique', 'Madagascar',\n",
        "              'Comoros', 'Seychelles', 'Mauritius', 'Djibouti', 'Eritrea', 'Somalia', 'Cape Verde',\n",
        "              'Sao Tome and Principe', 'Reunion', 'Mayotte', 'Saint Helena', 'Lesotho']\n",
        "\n",
        "    latin_america = ['Brazil', 'Argentina', 'Colombia', 'Chile', 'Peru', 'Venezuela', 'Ecuador',\n",
        "                    'Bolivia', 'Paraguay', 'Uruguay', 'Guyana', 'Suriname', 'French Guiana','Falkland Islands (Malvinas)']\n",
        "    oceania = [\n",
        "    'Fiji', 'Papua New Guinea', 'Solomon Islands', 'Vanuatu', 'New Caledonia',\n",
        "    'French Polynesia', 'Samoa', 'Tonga', 'Tuvalu', 'Kiribati', 'Nauru',\n",
        "    'Palau', 'Marshall Islands', 'Micronesia', 'Cook Islands', 'Niue',\n",
        "    'Tokelau', 'American Samoa', 'Northern Mariana Islands', 'Guam',\n",
        "    'United States Minor Outlying Islands', 'Norfolk Island', 'Christmas Island',\n",
        "    'Cocos (Keeling) Islands', 'Pitcairn Islands', 'Wallis and Futuna']\n",
        "    caribbean = [\n",
        "    'Cuba', 'Dominican Republic', 'Haiti', 'Jamaica', 'Puerto Rico', 'Trinidad and Tobago',\n",
        "    'Bahamas', 'Barbados', 'Saint Lucia', 'Saint Vincent and the Grenadines',\n",
        "    'Grenada', 'Antigua and Barbuda', 'Saint Kitts and Nevis', 'Dominica',\n",
        "    'Saint Martin', 'Anguilla', 'Montserrat', 'British Virgin Islands',\n",
        "    'United States Virgin Islands', 'Cayman Islands', 'Aruba', 'Curacao', 'Sint Maarten',\n",
        "    'Netherlands Antilles', 'Guadeloupe', 'Martinique', 'Saint Pierre and Miquelon',\n",
        "    'Saint Barthelemy', 'Turks and Caicos Islands']\n",
        "\n",
        "    if location in north_america:\n",
        "        return 'North America'\n",
        "    elif location in europe:\n",
        "        return 'Europe'\n",
        "    elif location in asia:\n",
        "        return 'Asia'\n",
        "    elif location in middle_east_north_africa:\n",
        "        return 'Middle East & North Africa'\n",
        "    elif location in latin_america:\n",
        "        return 'Latin America'\n",
        "    elif location in oceania:\n",
        "        return 'Oceania'\n",
        "    elif location in caribbean:\n",
        "        return 'Caribbean'\n",
        "    elif location in africa:\n",
        "        return 'Africa'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "# Apply regional categorization\n",
        "df['Region'] = df['Store Location'].apply(categorize_region)\n",
        "print(\"Added Region feature\")\n",
        "\n",
        "# Count by region\n",
        "region_counts = df['Region'].value_counts()\n",
        "print(\"\\nStore locations by region:\")\n",
        "print(region_counts)\n",
        "\n",
        "# 10. Data validation\n",
        "print(\"\\n10. Performing data validation...\")\n",
        "\n",
        "# Check for negative values in numerical columns\n",
        "negative_values = {}\n",
        "for col in ['Units Sold', 'Sales Revenue (USD)', 'Marketing Spend (USD)']:\n",
        "    neg_count = (df[col] < 0).sum()\n",
        "    if neg_count > 0:\n",
        "        negative_values[col] = neg_count\n",
        "        print(f\"WARNING: Found {neg_count} negative values in {col}\")\n",
        "\n",
        "# Check for invalid discount percentages\n",
        "invalid_discounts = ((df['Discount Percentage'] < 0) | (df['Discount Percentage'] > 100)).sum()\n",
        "if invalid_discounts > 0:\n",
        "    print(f\"WARNING: Found {invalid_discounts} invalid discount percentages (outside 0-100 range)\")\n",
        "\n",
        "# Check for invalid dates\n",
        "min_date = df['Date'].min()\n",
        "max_date = df['Date'].max()\n",
        "print(f\"Date range: {min_date} to {max_date}\")\n",
        "\n",
        "# Check for inconsistencies in categorical variables\n",
        "for col in ['Store ID', 'Product Category', 'Day of the Week']:\n",
        "    unique_values = df[col].unique()\n",
        "    print(f\"\\nUnique values in {col}: {len(unique_values)}\")\n",
        "    print(unique_values)\n",
        "\n",
        "# 11. Save cleaned dataset\n",
        "print(\"\\n11. Saving cleaned dataset...\")\n",
        "df.to_csv('/content/cleaned_data/retail_sales_cleaned.csv', index=False)\n",
        "print(\"Cleaned dataset saved to /content/cleaned_data/retail_sales_cleaned.csv\")\n",
        "\n",
        "# 12. Generate data quality report\n",
        "print(\"\\n12. Generating data quality report...\")\n",
        "with open('/content/data_quality_reports/data_quality_summary.txt', 'w') as f:\n",
        "    f.write(\"RETAIL SALES DATA QUALITY REPORT\\n\")\n",
        "    f.write(\"===============================\\n\\n\")\n",
        "    f.write(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    f.write(\"1. DATASET OVERVIEW\\n\")\n",
        "    f.write(f\"   Original dimensions: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
        "    f.write(f\"   Date range: {min_date} to {max_date}\\n\\n\")\n",
        "\n",
        "    f.write(\"2. MISSING VALUES\\n\")\n",
        "    if missing_values.sum() == 0:\n",
        "        f.write(\"   No missing values found in the dataset.\\n\\n\")\n",
        "    else:\n",
        "        f.write(\"   Missing values by column:\\n\")\n",
        "        for col, count in missing_values.items():\n",
        "            if count > 0:\n",
        "                f.write(f\"   - {col}: {count} ({(count/len(df))*100:.2f}%)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"3. DUPLICATES\\n\")\n",
        "    f.write(f\"   Duplicate rows: {duplicates}\\n\\n\")\n",
        "\n",
        "    f.write(\"4. OUTLIERS\\n\")\n",
        "    for row in outlier_report:\n",
        "        f.write(f\"   {row['Column']}:\\n\")\n",
        "        f.write(f\"   - Outlier count: {row['Outlier Count']} ({row['Outlier Percentage']:.2f}%)\\n\")\n",
        "        f.write(f\"   - Bounds: [{row['Lower Bound']:.2f}, {row['Upper Bound']:.2f}]\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"5. DATA VALIDATION ISSUES\\n\")\n",
        "    if len(negative_values) == 0 and invalid_discounts == 0:\n",
        "        f.write(\"   No validation issues found.\\n\\n\")\n",
        "    else:\n",
        "        for col, count in negative_values.items():\n",
        "            f.write(f\"   - {col}: {count} negative values\\n\")\n",
        "        if invalid_discounts > 0:\n",
        "            f.write(f\"   - Discount Percentage: {invalid_discounts} invalid values (outside 0-100 range)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"6. CATEGORICAL VARIABLES\\n\")\n",
        "    for col in ['Store ID', 'Product Category', 'Day of the Week', 'Region']:\n",
        "        f.write(f\"   {col}: {len(df[col].unique())} unique values\\n\")\n",
        "        if len(df[col].unique()) < 20:  # Only list if not too many\n",
        "            f.write(f\"   - Values: {', '.join(map(str, sorted(df[col].unique())))}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"7. CLEANING ACTIONS TAKEN\\n\")\n",
        "    f.write(\"   - Converted date to datetime format\\n\")\n",
        "    f.write(\"   - Extracted date components (year, month, day, etc.)\\n\")\n",
        "    if duplicates > 0:\n",
        "        f.write(f\"   - Removed {duplicates} duplicate rows\\n\")\n",
        "    for col in ['Units Sold', 'Sales Revenue (USD)']:\n",
        "        _, _, upper_bound, _ = detect_outliers(df, col)\n",
        "        f.write(f\"   - Capped upper outliers in {col} at {upper_bound:.2f}\\n\")\n",
        "    f.write(\"   - Added derived features (Revenue_per_Unit, Marketing_ROI, etc.)\\n\")\n",
        "    f.write(\"   - Added regional categorization\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"8. RECOMMENDATIONS\\n\")\n",
        "    f.write(\"   - Consider collecting more detailed product information for deeper analysis\\n\")\n",
        "    f.write(\"   - Add customer demographic data if available\\n\")\n",
        "    f.write(\"   - Consider adding cost data to enable profitability analysis\\n\")\n",
        "\n",
        "print(\"Data quality report saved to /content/data_quality_reports/data_quality_summary.txt\")\n",
        "\n",
        "# 13. Generate summary statistics for the cleaned dataset\n",
        "print(\"\\n13. Generating summary statistics for cleaned dataset...\")\n",
        "summary_stats = df.describe(include='all').T\n",
        "summary_stats['missing'] = df.isnull().sum()\n",
        "summary_stats['missing_pct'] = (df.isnull().sum() / len(df)) * 100\n",
        "summary_stats.to_csv('/content/data_quality_reports/cleaned_data_summary_stats.csv')\n",
        "print(\"Summary statistics saved to /content/data_quality_reports/cleaned_data_summary_stats.csv\")\n",
        "\n",
        "# 14. Print completion message\n",
        "print(\"\\nStep 1: Data Integration and Cleaning completed successfully!\")\n",
        "print(\"The cleaned dataset is ready for advanced analysis.\")\n",
        "print(\"\\nSummary of new features added:\")\n",
        "print(\"- Temporal features: Year, Month, Day, Month_Name, Quarter, Week_of_Year, Day_of_Year, Season\")\n",
        "print(\"- Business metrics: Revenue_per_Unit, Marketing_ROI\")\n",
        "print(\"- Categorical features: Has_Discount, Discount_Level, Is_Weekend, Region\")\n",
        "print(\"\\nNext steps: Proceed to Step 2 - Design unified analysis framework\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5bttIyUzmhv",
        "outputId": "94f41888-b0c2-45d2-97d6-87274908db3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Unified Analysis Framework\n",
            "==================================\n",
            "\n",
            "1. Loading the cleaned dataset...\n",
            "Cleaned dataset loaded with 30000 rows and 25 columns.\n",
            "\n",
            "2. Defining analysis dimensions and metrics...\n",
            "\n",
            "Analysis Dimensions:\n",
            "- Temporal: Year, Quarter, Month, Week_of_Year, Day of the Week, Is_Weekend, Holiday Effect\n",
            "- Product: Product Category, Product ID\n",
            "- Location: Store Location, Region\n",
            "- Promotion: Discount_Level, Has_Discount, Discount Percentage\n",
            "- Marketing: Marketing Spend (USD)\n",
            "\n",
            "Analysis Metrics:\n",
            "- Volume: Units Sold\n",
            "- Revenue: Sales Revenue (USD), Revenue_per_Unit\n",
            "- Efficiency: Marketing_ROI\n",
            "\n",
            "3. Defining analysis methods...\n",
            "\n",
            "Analysis Methods:\n",
            "- Descriptive:\n",
            "  * Time series decomposition\n",
            "  * Distribution analysis\n",
            "  * Correlation analysis\n",
            "  * Aggregation and summarization\n",
            "- Statistical:\n",
            "  * Hypothesis testing (t-tests, ANOVA)\n",
            "  * Regression analysis\n",
            "  * Chi-square tests for categorical relationships\n",
            "  * Confidence intervals\n",
            "- Geographical:\n",
            "  * Regional performance comparison\n",
            "  * Location clustering\n",
            "  * Spatial pattern analysis\n",
            "  * Regional preference mapping\n",
            "- Predictive:\n",
            "  * Time series forecasting\n",
            "  * Regression modeling\n",
            "  * Classification for categorical outcomes\n",
            "  * Causal inference analysis\n",
            "\n",
            "4. Creating analysis framework class...\n",
            "\n",
            "5. Demonstrating framework with examples...\n",
            "\n",
            "Example 1: Temporal Analysis\n",
            "   Quarter  Sales Revenue (USD)\n",
            "0        1          17805192.81\n",
            "1        2          19546625.13\n",
            "2        3          21110401.38\n",
            "3        4          21235850.57\n",
            "\n",
            "Example 2: Geographical Analysis\n",
            "                       Region  Sales Revenue (USD)\n",
            "0                      Africa          16272478.52\n",
            "1                        Asia          10626790.90\n",
            "2                   Caribbean           8853750.54\n",
            "3                      Europe          17764880.48\n",
            "4               Latin America           4506340.86\n",
            "5  Middle East & North Africa           7427148.97\n",
            "6               North America           3561898.51\n",
            "7                     Oceania           8470447.86\n",
            "8                       Other           2214333.25\n",
            "\n",
            "Example 3: Product Analysis\n",
            "  Product Category  Sales Revenue (USD)\n",
            "0         Clothing          19516928.04\n",
            "1      Electronics          27601540.55\n",
            "2        Furniture          22375061.09\n",
            "3        Groceries          10204540.21\n",
            "\n",
            "Example 4: Promotion Analysis\n",
            "  Discount_Level  Sales Revenue (USD)\n",
            "0           High              2218.06\n",
            "1            Low              2447.27\n",
            "2         Medium              2398.41\n",
            "3       Very Low              2604.97\n",
            "\n",
            "Example 5: Correlation Analysis\n",
            "                       Units Sold  Sales Revenue (USD)  Discount Percentage  Marketing Spend (USD)\n",
            "Units Sold                   1.00                 0.60                -0.00                  -0.00\n",
            "Sales Revenue (USD)          0.60                 1.00                -0.07                  -0.00\n",
            "Discount Percentage         -0.00                -0.07                 1.00                  -0.00\n",
            "Marketing Spend (USD)       -0.00                -0.00                -0.00                   1.00\n",
            "\n",
            "Example 6: Statistical Test (t-test)\n",
            "T-test results: t-statistic = 7.5414, p-value = 0.0000\n",
            "Mean sales on holidays: $4371.13\n",
            "Mean sales on non-holidays: $2647.18\n",
            "\n",
            "Example 7: Statistical Test (ANOVA)\n",
            "                             sum_sq       df      F  PR(>F)\n",
            "C(Product_Category)  11152024730.64     3.00 786.45    0.00\n",
            "Residual            141782536291.09 29996.00    NaN     NaN\n",
            "\n",
            "Example 8: Regression Analysis\n",
            "                             OLS Regression Results                            \n",
            "===============================================================================\n",
            "Dep. Variable:     Sales Revenue (USD)   R-squared:                       0.359\n",
            "Model:                             OLS   Adj. R-squared:                  0.359\n",
            "Method:                  Least Squares   F-statistic:                     5593.\n",
            "Date:                 Tue, 10 Jun 2025   Prob (F-statistic):               0.00\n",
            "Time:                         16:00:01   Log-Likelihood:            -2.6757e+05\n",
            "No. Observations:                30000   AIC:                         5.351e+05\n",
            "Df Residuals:                    29996   BIC:                         5.352e+05\n",
            "Df Model:                            3                                         \n",
            "Covariance Type:             nonrobust                                         \n",
            "=========================================================================================\n",
            "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-----------------------------------------------------------------------------------------\n",
            "const                    84.3312     25.026      3.370      0.001      35.280     133.383\n",
            "Units Sold              434.0112      3.371    128.744      0.000     427.404     440.619\n",
            "Discount Percentage     -23.9880      1.747    -13.728      0.000     -27.413     -20.563\n",
            "Marketing Spend (USD)    -0.0573      0.162     -0.354      0.724      -0.375       0.260\n",
            "==============================================================================\n",
            "Omnibus:                      147.216   Durbin-Watson:                   0.322\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              133.745\n",
            "Skew:                           0.125   Prob(JB):                     9.07e-30\n",
            "Kurtosis:                       2.788   Cond. No.                         197.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "6. Saving the framework and results...\n",
            "Analysis results saved to /content/analysis_framework/results_20250610_160001.pkl\n",
            "\n",
            "7. Creating framework documentation...\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 2: Unified Analysis Framework for Retail Sales\n",
        "This script establishes a comprehensive framework that integrates statistical and geographical methods\n",
        "for advanced retail sales analysis.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from scipy import stats\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create directories for outputs\n",
        "if not os.path.exists('/content/analysis_framework'):\n",
        "    os.makedirs('/content/analysis_framework')\n",
        "if not os.path.exists('/content/analysis_framework/plots'):\n",
        "    os.makedirs('/content/analysis_framework/plots')\n",
        "\n",
        "print(\"Step 2: Unified Analysis Framework\")\n",
        "print(\"==================================\\n\")\n",
        "\n",
        "# 1. Load the cleaned dataset\n",
        "print(\"1. Loading the cleaned dataset...\")\n",
        "df = pd.read_csv('/content/cleaned_data/retail_sales_cleaned.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(f\"Cleaned dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\\n\")\n",
        "\n",
        "# 2. Define analysis dimensions and metrics\n",
        "print(\"2. Defining analysis dimensions and metrics...\")\n",
        "\n",
        "# Key dimensions for analysis\n",
        "dimensions = {\n",
        "    'Temporal': ['Year', 'Quarter', 'Month', 'Week_of_Year', 'Day of the Week', 'Is_Weekend', 'Holiday Effect'],\n",
        "    'Product': ['Product Category', 'Product ID'],\n",
        "    'Location': ['Store Location', 'Region'],\n",
        "    'Promotion': ['Discount_Level', 'Has_Discount', 'Discount Percentage'],\n",
        "    'Marketing': ['Marketing Spend (USD)']\n",
        "}\n",
        "\n",
        "# Key metrics for analysis\n",
        "metrics = {\n",
        "    'Volume': ['Units Sold'],\n",
        "    'Revenue': ['Sales Revenue (USD)', 'Revenue_per_Unit'],\n",
        "    'Efficiency': ['Marketing_ROI']\n",
        "}\n",
        "\n",
        "# Print dimensions and metrics\n",
        "print(\"\\nAnalysis Dimensions:\")\n",
        "for category, dims in dimensions.items():\n",
        "    print(f\"- {category}: {', '.join(dims)}\")\n",
        "\n",
        "print(\"\\nAnalysis Metrics:\")\n",
        "for category, mets in metrics.items():\n",
        "    print(f\"- {category}: {', '.join(mets)}\")\n",
        "\n",
        "# 3. Define analysis methods\n",
        "print(\"\\n3. Defining analysis methods...\")\n",
        "\n",
        "analysis_methods = {\n",
        "    'Descriptive': [\n",
        "        'Time series decomposition',\n",
        "        'Distribution analysis',\n",
        "        'Correlation analysis',\n",
        "        'Aggregation and summarization'\n",
        "    ],\n",
        "    'Statistical': [\n",
        "        'Hypothesis testing (t-tests, ANOVA)',\n",
        "        'Regression analysis',\n",
        "        'Chi-square tests for categorical relationships',\n",
        "        'Confidence intervals'\n",
        "    ],\n",
        "    'Geographical': [\n",
        "        'Regional performance comparison',\n",
        "        'Location clustering',\n",
        "        'Spatial pattern analysis',\n",
        "        'Regional preference mapping'\n",
        "    ],\n",
        "    'Predictive': [\n",
        "        'Time series forecasting',\n",
        "        'Regression modeling',\n",
        "        'Classification for categorical outcomes',\n",
        "        'Causal inference analysis'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Print analysis methods\n",
        "print(\"\\nAnalysis Methods:\")\n",
        "for category, methods in analysis_methods.items():\n",
        "    print(f\"- {category}:\")\n",
        "    for method in methods:\n",
        "        print(f\"  * {method}\")\n",
        "\n",
        "# 4. Create analysis framework class\n",
        "print(\"\\n4. Creating analysis framework class...\")\n",
        "\n",
        "class RetailAnalysisFramework:\n",
        "    \"\"\"\n",
        "    Unified framework for retail sales analysis that integrates statistical and geographical methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"Initialize with cleaned dataset.\"\"\"\n",
        "        self.data = data\n",
        "        self.results = {}\n",
        "\n",
        "    def temporal_analysis(self, metric, dimension='Quarter', agg_func='mean'):\n",
        "        \"\"\"Analyze metrics over time dimensions.\"\"\"\n",
        "        result = self.data.groupby(dimension)[metric].agg(agg_func).reset_index()\n",
        "        self.results[f'temporal_{metric}_{dimension}'] = result\n",
        "        return result\n",
        "\n",
        "    def geographical_analysis(self, metric, geo_dimension='Region', agg_func='mean'):\n",
        "        \"\"\"Analyze metrics across geographical dimensions.\"\"\"\n",
        "        result = self.data.groupby(geo_dimension)[metric].agg(agg_func).reset_index()\n",
        "        self.results[f'geo_{metric}_{geo_dimension}'] = result\n",
        "        return result\n",
        "\n",
        "    def product_analysis(self, metric, prod_dimension='Product Category', agg_func='mean'):\n",
        "        \"\"\"Analyze metrics by product dimensions.\"\"\"\n",
        "        result = self.data.groupby(prod_dimension)[metric].agg(agg_func).reset_index()\n",
        "        self.results[f'product_{metric}_{prod_dimension}'] = result\n",
        "        return result\n",
        "\n",
        "    def promotion_analysis(self, metric, promo_dimension='Discount_Level', agg_func='mean'):\n",
        "        \"\"\"Analyze metrics by promotion dimensions.\"\"\"\n",
        "        result = self.data.groupby(promo_dimension)[metric].agg(agg_func).reset_index()\n",
        "        self.results[f'promo_{metric}_{promo_dimension}'] = result\n",
        "        return result\n",
        "\n",
        "    def correlation_analysis(self, variables):\n",
        "        \"\"\"Analyze correlations between specified variables.\"\"\"\n",
        "        corr_matrix = self.data[variables].corr()\n",
        "        self.results[f'correlation_{\"-\".join(variables)}'] = corr_matrix\n",
        "        return corr_matrix\n",
        "\n",
        "    def statistical_test(self, test_type, **kwargs):\n",
        "        \"\"\"Perform statistical tests based on test_type.\"\"\"\n",
        "        if test_type == 't_test':\n",
        "            group_col = kwargs.get('group_col')\n",
        "            metric = kwargs.get('metric')\n",
        "            group1 = kwargs.get('group1')\n",
        "            group2 = kwargs.get('group2')\n",
        "\n",
        "            group1_data = self.data[self.data[group_col] == group1][metric]\n",
        "            group2_data = self.data[self.data[group_col] == group2][metric]\n",
        "\n",
        "            t_stat, p_val = stats.ttest_ind(group1_data, group2_data, equal_var=False)\n",
        "            result = {'t_statistic': t_stat, 'p_value': p_val,\n",
        "                      'mean_1': group1_data.mean(), 'mean_2': group2_data.mean()}\n",
        "\n",
        "            self.results[f't_test_{group_col}_{metric}_{group1}_vs_{group2}'] = result\n",
        "            return result\n",
        "\n",
        "        # In the RetailAnalysisFramework class, specifically in the statistical_test method for 'anova':\n",
        "        elif test_type == 'anova':\n",
        "             group_col = kwargs.get('group_col')\n",
        "             metric = kwargs.get('metric')\n",
        "\n",
        "    # Create safe column names for Patsy formula\n",
        "    # Replace spaces and special characters with underscores\n",
        "             safe_metric = metric.replace(' ', '_').replace('(', '').replace(')', '').replace('$', '')\n",
        "             safe_group_col = group_col.replace(' ', '_').replace('(', '').replace(')', '')\n",
        "\n",
        "    # Add temporary columns to the dataframe with safe names if they don't exist\n",
        "             if safe_metric not in self.data.columns:\n",
        "                self.data[safe_metric] = self.data[metric]\n",
        "             if safe_group_col not in self.data.columns:\n",
        "                self.data[safe_group_col] = self.data[group_col]\n",
        "\n",
        "    # Construct the formula using the safe column names\n",
        "             formula = f\"{safe_metric} ~ C({safe_group_col})\"\n",
        "\n",
        "             try:\n",
        "                model = ols(formula, data=self.data).fit()\n",
        "                anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "                self.results[f'anova_{group_col}_{metric}'] = anova_table\n",
        "                return anova_table\n",
        "             except PatsyError as e:\n",
        "                print(f\"PatsyError encountered with formula '{formula}': {e}\")\n",
        "                print(\"Please check the column names used in the statistical test.\")\n",
        "                return None # Or raise the exception if you prefer\n",
        "\n",
        "        elif test_type == 'chi_square':\n",
        "            var1 = kwargs.get('var1')\n",
        "            var2 = kwargs.get('var2')\n",
        "\n",
        "            contingency_table = pd.crosstab(self.data[var1], self.data[var2])\n",
        "            chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "            result = {'chi2': chi2, 'p_value': p, 'dof': dof, 'contingency_table': contingency_table}\n",
        "            self.results[f'chi_square_{var1}_{var2}'] = result\n",
        "            return result\n",
        "\n",
        "    def regression_analysis(self, dependent_var, independent_vars):\n",
        "        \"\"\"Perform regression analysis.\"\"\"\n",
        "        X = self.data[independent_vars]\n",
        "        X = sm.add_constant(X)\n",
        "        y = self.data[dependent_var]\n",
        "\n",
        "        model = sm.OLS(y, X).fit()\n",
        "        self.results[f'regression_{dependent_var}'] = model\n",
        "        return model\n",
        "\n",
        "    def plot_time_series(self, metric, time_dimension='Date', agg_func='mean', title=None):\n",
        "        \"\"\"Plot time series for a metric.\"\"\"\n",
        "        if time_dimension == 'Date':\n",
        "            # For date, we need to aggregate first\n",
        "            ts_data = self.data.groupby(time_dimension)[metric].agg(agg_func)\n",
        "        else:\n",
        "            # For other dimensions like Quarter, Month, etc.\n",
        "            ts_data = self.data.groupby(time_dimension)[metric].agg(agg_func).reset_index()\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        if time_dimension == 'Date':\n",
        "            plt.plot(ts_data.index, ts_data.values, marker='o', linestyle='-')\n",
        "        else:\n",
        "            plt.plot(ts_data[time_dimension], ts_data[metric], marker='o', linestyle='-')\n",
        "\n",
        "        plt.title(title or f'{metric} by {time_dimension}')\n",
        "        plt.xlabel(time_dimension)\n",
        "        plt.ylabel(metric)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Save the plot\n",
        "        filename = f'/content/analysis_framework/plots/{metric}_by_{time_dimension}.png'\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    def plot_geographical(self, metric, geo_dimension='Region', agg_func='mean', title=None):\n",
        "        \"\"\"Plot geographical comparison for a metric.\"\"\"\n",
        "        geo_data = self.data.groupby(geo_dimension)[metric].agg(agg_func).reset_index()\n",
        "        geo_data = geo_data.sort_values(metric, ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        sns.barplot(x=geo_dimension, y=metric, data=geo_data)\n",
        "        plt.title(title or f'{metric} by {geo_dimension}')\n",
        "        plt.xlabel(geo_dimension)\n",
        "        plt.ylabel(metric)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        filename = f'/content/analysis_framework/plots/{metric}_by_{geo_dimension}.png'\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    def plot_distribution(self, variable, by=None, title=None):\n",
        "        \"\"\"Plot distribution of a variable, optionally grouped by another variable.\"\"\"\n",
        "        plt.figure(figsize=(14, 8))\n",
        "\n",
        "        if by is None:\n",
        "            sns.histplot(self.data[variable], kde=True)\n",
        "            plt.title(title or f'Distribution of {variable}')\n",
        "        else:\n",
        "            sns.histplot(data=self.data, x=variable, hue=by, kde=True, multiple=\"stack\")\n",
        "            plt.title(title or f'Distribution of {variable} by {by}')\n",
        "\n",
        "        plt.xlabel(variable)\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Save the plot\n",
        "        by_suffix = f'_by_{by}' if by else ''\n",
        "        filename = f'/content/analysis_framework/plots/distribution_{variable}{by_suffix}.png'\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    def plot_correlation_matrix(self, variables, title=\"Correlation Matrix\"):\n",
        "        \"\"\"Plot correlation matrix for specified variables.\"\"\"\n",
        "        corr_matrix = self.data[variables].corr()\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "                    vmin=-1, vmax=1, square=True, linewidths=.5)\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        var_suffix = '_'.join(variables)\n",
        "        filename = f'/content/analysis_framework/plots/correlation_matrix_{var_suffix}.png'\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    def save_results(self, filename=None):\n",
        "        \"\"\"Save all analysis results to a file.\"\"\"\n",
        "        if filename is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f'/content/analysis_framework/results_{timestamp}.pkl'\n",
        "\n",
        "        import pickle\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.results, f)\n",
        "\n",
        "        return filename\n",
        "\n",
        "# 5. Demonstrate framework with examples\n",
        "print(\"\\n5. Demonstrating framework with examples...\")\n",
        "\n",
        "# Initialize the framework\n",
        "framework = RetailAnalysisFramework(df)\n",
        "\n",
        "# Example 1: Temporal Analysis\n",
        "print(\"\\nExample 1: Temporal Analysis\")\n",
        "quarterly_sales = framework.temporal_analysis('Sales Revenue (USD)', dimension='Quarter', agg_func='sum')\n",
        "print(quarterly_sales)\n",
        "framework.plot_time_series('Sales Revenue (USD)', time_dimension='Quarter', agg_func='sum',\n",
        "                          title='Total Sales Revenue by Quarter')\n",
        "\n",
        "# Example 2: Geographical Analysis\n",
        "print(\"\\nExample 2: Geographical Analysis\")\n",
        "regional_sales = framework.geographical_analysis('Sales Revenue (USD)', geo_dimension='Region', agg_func='sum')\n",
        "print(regional_sales)\n",
        "framework.plot_geographical('Sales Revenue (USD)', geo_dimension='Region', agg_func='sum',\n",
        "                           title='Total Sales Revenue by Region')\n",
        "\n",
        "# Example 3: Product Analysis\n",
        "print(\"\\nExample 3: Product Analysis\")\n",
        "category_sales = framework.product_analysis('Sales Revenue (USD)', prod_dimension='Product Category', agg_func='sum')\n",
        "print(category_sales)\n",
        "\n",
        "# Example 4: Promotion Analysis\n",
        "print(\"\\nExample 4: Promotion Analysis\")\n",
        "discount_sales = framework.promotion_analysis('Sales Revenue (USD)', promo_dimension='Discount_Level', agg_func='mean')\n",
        "print(discount_sales)\n",
        "\n",
        "# Example 5: Correlation Analysis\n",
        "print(\"\\nExample 5: Correlation Analysis\")\n",
        "corr_vars = ['Units Sold', 'Sales Revenue (USD)', 'Discount Percentage', 'Marketing Spend (USD)']\n",
        "correlation = framework.correlation_analysis(corr_vars)\n",
        "print(correlation)\n",
        "framework.plot_correlation_matrix(corr_vars, title=\"Correlation Matrix of Key Metrics\")\n",
        "\n",
        "# Example 6: Statistical Test (t-test)\n",
        "print(\"\\nExample 6: Statistical Test (t-test)\")\n",
        "holiday_test = framework.statistical_test('t_test', group_col='Holiday Effect',\n",
        "                                         metric='Sales Revenue (USD)',\n",
        "                                         group1=True, group2=False)\n",
        "print(f\"T-test results: t-statistic = {holiday_test['t_statistic']:.4f}, p-value = {holiday_test['p_value']:.4f}\")\n",
        "print(f\"Mean sales on holidays: ${holiday_test['mean_1']:.2f}\")\n",
        "print(f\"Mean sales on non-holidays: ${holiday_test['mean_2']:.2f}\")\n",
        "\n",
        "# Example 7: Statistical Test (ANOVA)\n",
        "print(\"\\nExample 7: Statistical Test (ANOVA)\")\n",
        "category_anova = framework.statistical_test('anova', group_col='Product Category', metric='Sales Revenue (USD)')\n",
        "print(category_anova)\n",
        "\n",
        "# Example 8: Regression Analysis\n",
        "print(\"\\nExample 8: Regression Analysis\")\n",
        "regression_model = framework.regression_analysis('Sales Revenue (USD)',\n",
        "                                               ['Units Sold', 'Discount Percentage', 'Marketing Spend (USD)'])\n",
        "print(regression_model.summary())\n",
        "\n",
        "# 6. Save the framework and results\n",
        "print(\"\\n6. Saving the framework and results...\")\n",
        "results_file = framework.save_results()\n",
        "print(f\"Analysis results saved to {results_file}\")\n",
        "\n",
        "# 7. Create framework documentation\n",
        "print(\"\\n7. Creating framework documentation...\")\n",
        "with open('/content/analysis_framework/framework_documentation.md', 'w') as f:\n",
        "    f.write(\"# Unified Retail Sales Analysis Framework\\n\\n\")\n",
        "    f.write(\"## Overview\\n\")\n",
        "    f.write(\"This framework integrates statistical and geographical methods for comprehensive retail sales analysis.\\n\\n\")\n",
        "\n",
        "    f.write(\"## Analysis Dimensions\\n\")\n",
        "    for category, dims in dimensions.items():\n",
        "        f.write(f\"### {category}\\n\")\n",
        "        for dim in dims:\n",
        "            f.write(f\"- {dim}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## Analysis Metrics\\n\")\n",
        "    for category, mets in metrics.items():\n",
        "        f.write(f\"### {category}\\n\")\n",
        "        for met in mets:\n",
        "            f.write(f\"- {met}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## Analysis Methods\\n\")\n",
        "    for category, methods in analysis_methods.items():\n",
        "        f.write(f\"### {category}\\n\")\n",
        "        for method in methods:\n",
        "            f.write(f\"- {method}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## Framework Functions\\n\")\n",
        "    f.write(\"The framework provides the following key functions:\\n\\n\")\n",
        "    f.write(\"1. **Temporal Analysis**: Analyze metrics over time dimensions\\n\")\n",
        "    f.write(\"2. **Geographical Analysis**: Analyze metrics across geographical dimensions\\n\")\n",
        "    f.write(\"3. **Product Analysis**: Analyze metrics by product dimensions\\n\")\n",
        "    f.write(\"4. **Promotion Analysis**: Analyze metrics by promotion dimensions\\n\")\n",
        "    f.write(\"5. **Correlation Analysis**: Analyze correlations between specified variables\\n\")\n",
        "    f.write(\"6. **Statistical Tests**: Perform t-tests, ANOVA, and chi-square tests\\n\")\n",
        "    f.write(\"7. **Regression Analysis**: Perform regression analysis\\n\")\n",
        "    f.write(\"8. **Visualization Functions**: Create various plots for analysis results\\n\\n\")\n",
        "\n",
        "    f.write(\"## Usage Examples\\n\")\n",
        "    f.write(\"See the demonstration script for examples of how to use each function.\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KhWdsHQBy_Bx",
        "outputId": "7279337f-4128-4ee4-e507-a0638b777386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3: Advanced Exploratory and Statistical Analysis\n",
            "===================================================\n",
            "\n",
            "1. Loading the cleaned dataset and analysis framework...\n",
            "Framework initialized successfully.\n",
            "\n",
            "2. Performing advanced temporal analysis...\n",
            "\n",
            "2.1 Time series decomposition\n",
            "Time series decomposition completed and saved.\n",
            "\n",
            "2.2 Quarterly analysis with confidence intervals\n",
            "   Quarter    mean     std  count  ci_lower  ci_upper\n",
            "0        1 2389.96 2036.88   7450   2343.71   2436.21\n",
            "1        2 2619.49 2195.71   7462   2569.67   2669.31\n",
            "2        3 2798.30 2343.99   7544   2745.41   2851.20\n",
            "3        4 2814.93 2407.92   7544   2760.60   2869.27\n",
            "\n",
            "2.3 Day of week effect with statistical validation\n",
            "  Day of the Week    mean     std  count  ci_lower  ci_upper\n",
            "0          Monday 2518.77 2175.16   4309   2453.82   2583.71\n",
            "1         Tuesday 2538.53 2191.61   4268   2472.78   2604.28\n",
            "2       Wednesday 2534.12 2159.77   4268   2469.33   2598.92\n",
            "3        Thursday 2568.57 2183.05   4268   2503.08   2634.07\n",
            "4          Friday 2559.77 2195.81   4268   2493.89   2625.65\n",
            "5        Saturday 2912.90 2396.59   4310   2841.35   2984.45\n",
            "6          Sunday 2959.44 2434.75   4309   2886.74   3032.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-0c53537f29cd>:120: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  day_stats = df.groupby('Day of the Week')['Sales Revenue (USD)'].agg(['mean', 'std', 'count']).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA results for day of week effect:\n",
            "                            sum_sq       df     F  PR(>F)\n",
            "C(Day_of_the_Week)    956781761.28     6.00 31.47    0.00\n",
            "Residual           151977779260.46 29993.00   NaN     NaN\n",
            "\n",
            "3. Performing advanced geographical analysis...\n",
            "\n",
            "3.1 Regional performance with statistical significance\n",
            "                       Region    mean     std  count  ci_lower  ci_upper\n",
            "0                      Africa 2655.00 2255.00   6129   2598.54   2711.45\n",
            "1                        Asia 2638.23 2237.41   4028   2569.13   2707.33\n",
            "2                   Caribbean 2694.39 2296.67   3286   2615.86   2772.91\n",
            "3                      Europe 2659.81 2251.88   6679   2605.81   2713.82\n",
            "4               Latin America 2647.67 2269.09   1702   2539.87   2755.48\n",
            "5  Middle East & North Africa 2709.65 2306.94   2741   2623.28   2796.01\n",
            "6               North America 2542.40 2142.05   1401   2430.23   2654.56\n",
            "7                     Oceania 2642.89 2248.28   3205   2565.05   2720.72\n",
            "8                       Other 2671.09 2313.21    829   2513.62   2828.56\n",
            "ANOVA results for regional effect:\n",
            "                   sum_sq       df    F  PR(>F)\n",
            "C(Region)     33033966.86     8.00 0.81    0.59\n",
            "Residual  152901527054.88 29991.00  NaN     NaN\n",
            "\n",
            "3.2 Regional performance by product category\n",
            "Product Category            Clothing  Electronics  Furniture  Groceries\n",
            "Region                                                                 \n",
            "Africa                       2953.48      3441.31    2352.51    1742.98\n",
            "Asia                         2795.94      3429.05    2402.57    1725.56\n",
            "Caribbean                    2983.07      3530.69    2409.72    1710.40\n",
            "Europe                       2933.23      3454.20    2356.06    1752.03\n",
            "Latin America                3057.05      3381.45    2356.84    1706.01\n",
            "Middle East & North Africa   3108.85      3470.34    2329.67    1740.87\n",
            "North America                2980.02      3144.04    2213.62    1771.80\n",
            "Oceania                      2955.64      3389.50    2349.33    1790.89\n",
            "Other                        3001.87      3492.56    2230.25    1802.19\n",
            "\n",
            "3.3 Regional seasonality patterns\n",
            "Quarter                          1       2       3       4\n",
            "Region                                                    \n",
            "Africa                     2363.98 2659.05 2771.69 2812.53\n",
            "Asia                       2404.41 2639.33 2732.48 2772.76\n",
            "Caribbean                  2449.36 2671.57 2845.69 2802.20\n",
            "Europe                     2453.16 2595.59 2761.99 2819.69\n",
            "Latin America              2411.06 2460.79 3056.11 2697.68\n",
            "Middle East & North Africa 2495.88 2700.87 2759.52 2891.30\n",
            "North America              2135.32 2301.19 3054.13 2722.72\n",
            "Oceania                    2269.15 2668.95 2771.04 2881.87\n",
            "Other                      2259.68 2666.20 2772.05 2957.42\n",
            "\n",
            "4. Performing advanced product analysis...\n",
            "\n",
            "4.1 Product category performance with statistical validation\n",
            "  Product Category    mean     std  count  ci_lower  ci_upper\n",
            "0         Clothing 2953.53 2206.67   6608   2900.32   3006.74\n",
            "1      Electronics 3432.60 2649.64   8041   3374.69   3490.51\n",
            "2        Furniture 2354.53 2081.30   9503   2312.68   2396.37\n",
            "3        Groceries 1744.96 1432.86   5848   1708.24   1781.69\n",
            "ANOVA results for product category effect:\n",
            "                             sum_sq       df      F  PR(>F)\n",
            "C(Product_Category)  11152024730.64     3.00 786.45    0.00\n",
            "Residual            141782536291.09 29996.00    NaN     NaN\n",
            "\n",
            "4.2 Product category performance by quarter\n",
            "Quarter                1       2       3       4\n",
            "Product Category                                \n",
            "Clothing         2696.47 3790.53 3156.64 2179.88\n",
            "Electronics      3047.10 2697.69 3736.98 4234.36\n",
            "Furniture        2019.72 2291.72 2412.03 2688.49\n",
            "Groceries        1737.06 1727.16 1732.19 1783.12\n",
            "\n",
            "5. Performing advanced promotion analysis...\n",
            "\n",
            "5.1 Discount effectiveness with statistical validation\n",
            "  Discount_Level    mean     std  count  ci_lower  ci_upper\n",
            "0           High 2218.06 1990.92   1783   2125.65   2310.48\n",
            "1            Low 2447.27 2143.19   1814   2348.64   2545.90\n",
            "2         Medium 2398.41 2087.78   1763   2300.95   2495.87\n",
            "3       Very Low 2604.97 2216.87   1794   2502.39   2707.56\n",
            "ANOVA results for discount level effect:\n",
            "                          sum_sq      df     F  PR(>F)\n",
            "C(Discount_Level)   136219020.84    3.00 10.18    0.00\n",
            "Residual          31883006064.54 7150.00   NaN     NaN\n",
            "\n",
            "5.2 Discount effectiveness by product category\n",
            "Product Category  Clothing  Electronics  Furniture  Groceries\n",
            "Discount_Level                                               \n",
            "High               2520.94      2996.89    1930.85    1329.68\n",
            "Low                2778.11      3135.04    2125.60    1669.08\n",
            "Medium             2530.13      3175.27    2154.85    1525.66\n",
            "Very Low           3001.33      3415.57    2267.04    1641.09\n",
            "\n",
            "5.3 Discount effectiveness by region\n",
            "Region          Africa    Asia  Caribbean  Europe  Latin America  Middle East & North Africa  North America  Oceania   Other\n",
            "Discount_Level                                                                                                              \n",
            "High           2348.73 2214.86    2148.44 2212.67        2062.63                     2087.40        2371.14  2232.30 2028.25\n",
            "Low            2405.01 2279.38    2591.38 2366.44        2104.87                     2733.53        2994.62  2386.01 2772.42\n",
            "Medium         2417.31 2289.35    2561.93 2491.26        2238.22                     2263.67        2208.05  2451.13 2348.88\n",
            "Very Low       2705.41 2750.53    2321.83 2537.74        2810.10                     2454.62        2407.57  2615.27 2908.38\n",
            "\n",
            "3.7 Analyzing discount effectiveness by quarter\n",
            "Discount effectiveness by quarter (revenue):\n",
            "Discount_Level    High     Low  Medium  Very Low\n",
            "Quarter                                         \n",
            "1              1843.50 2173.99 2046.51   2276.83\n",
            "2              2163.06 2536.28 2316.01   2608.05\n",
            "3              2499.13 2521.58 2576.28   2728.00\n",
            "4              2359.35 2570.15 2660.28   2841.19\n",
            "Discount effectiveness by quarter (units):\n",
            "Discount_Level  High  Low  Medium  Very Low\n",
            "Quarter                                    \n",
            "1               5.44 5.32    5.25      5.33\n",
            "2               5.72 6.05    5.99      6.07\n",
            "3               6.53 6.23    6.36      6.35\n",
            "4               6.70 6.52    6.65      6.61\n",
            "Discount sensitivity by quarter:\n",
            "   Quarter  Correlation  Sensitivity  P_Value\n",
            "0        1        -0.02        -0.01     0.18\n",
            "0        2        -0.01        -0.00     0.41\n",
            "0        3        -0.00        -0.00     0.83\n",
            "0        4         0.01         0.01     0.44\n",
            "\n",
            "6. Performing advanced marketing analysis...\n",
            "\n",
            "6.1 Marketing ROI analysis\n",
            "  Marketing_Spend_Bin  Marketing_ROI  Sales Revenue (USD)  Marketing Spend (USD)  Units Sold\n",
            "0                0-50         239.15              2633.97                  25.36        6.07\n",
            "1              50-100          35.56              2662.74                  75.82        6.09\n",
            "2             100-150          20.68              2686.74                 125.30        6.15\n",
            "3             150-200          14.03              2611.44                 175.03        6.04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-0c53537f29cd>:439: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  marketing_roi = df.groupby('Marketing_Spend_Bin').agg({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6.2 Marketing effectiveness by product category\n",
            "  Product Category  Marketing_ROI  Sales Revenue (USD)  Marketing Spend (USD)\n",
            "0         Clothing          46.95              2953.53                  49.34\n",
            "1      Electronics          51.14              3432.60                  50.49\n",
            "2        Furniture          32.35              2354.53                  49.60\n",
            "3        Groceries          24.43              1744.96                  50.44\n",
            "\n",
            "6.3 Marketing effectiveness by region\n",
            "                       Region  Marketing_ROI  Sales Revenue (USD)  Marketing Spend (USD)\n",
            "0                      Africa          39.92              2655.00                  49.18\n",
            "1                        Asia          39.20              2638.23                  47.27\n",
            "2                   Caribbean          35.55              2694.39                  50.14\n",
            "3                      Europe          38.22              2659.81                  50.05\n",
            "4               Latin America          36.16              2647.67                  50.27\n",
            "5  Middle East & North Africa          45.42              2709.65                  52.67\n",
            "6               North America          42.66              2542.40                  47.40\n",
            "7                     Oceania          39.08              2642.89                  52.64\n",
            "8                       Other          31.30              2671.09                  51.12\n",
            "\n",
            "6.4 Analyzing marketing effectiveness by quarter\n",
            "Marketing ROI by quarter:\n",
            "Marketing_Spend_Bin   0-50  50-100  100-150  150-200\n",
            "Quarter                                             \n",
            "1                   226.59   32.00    18.28    12.91\n",
            "2                   257.79   34.32    21.15    13.52\n",
            "3                   226.85   38.70    21.50    14.87\n",
            "4                   245.73   37.06    21.69    14.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-0c53537f29cd>:514: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  marketing_quarter = df.groupby(['Quarter', 'Marketing_Spend_Bin']).agg({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marketing elasticity by quarter:\n",
            "   Quarter  Correlation  Elasticity  P_Value\n",
            "0        1         0.00        0.00     0.98\n",
            "0        2        -0.00       -0.00     0.51\n",
            "0        3        -0.00       -0.01     0.25\n",
            "0        4        -0.01       -0.01     0.05\n",
            "\n",
            "7. Performing holiday effect analysis...\n",
            "\n",
            "7.1 Holiday effect with statistical validation\n",
            "T-test results for holiday effect:\n",
            "T-statistic: 7.5414, p-value: 0.0000\n",
            "Mean sales on holidays: $4371.13\n",
            "Mean sales on non-holidays: $2647.18\n",
            "\n",
            "7.2 Holiday effect by product category\n",
            "Holiday Effect     False    True  Holiday_Impact\n",
            "Product Category                                \n",
            "Clothing         2948.83 3811.75            1.29\n",
            "Electronics      3418.81 5938.80            1.74\n",
            "Furniture        2342.51 4538.13            1.94\n",
            "Groceries        1740.40 2573.51            1.48\n",
            "\n",
            "7.3 Holiday effect by region\n",
            "Holiday Effect               False    True  Holiday_Impact\n",
            "Region                                                    \n",
            "Africa                     2646.74 3979.19            1.50\n",
            "Asia                       2628.51 4408.40            1.68\n",
            "Caribbean                  2686.97 4312.52            1.60\n",
            "Europe                     2649.83 4799.97            1.81\n",
            "Latin America              2644.16 3309.16            1.25\n",
            "Middle East & North Africa 2695.32 4761.81            1.77\n",
            "North America              2538.07 3404.16            1.34\n",
            "Oceania                    2631.59 4762.04            1.81\n",
            "Other                      2653.94 5023.98            1.89\n",
            "\n",
            "8. Performing multivariate analysis...\n",
            "\n",
            "8.1 Correlation analysis of key metrics\n",
            "                       Units Sold  Sales Revenue (USD)  Quarter  Discount Percentage  Marketing Spend (USD)  Holiday Effect\n",
            "Units Sold                   1.00                 0.60     0.14                -0.00                  -0.00            0.10\n",
            "Sales Revenue (USD)          0.60                 1.00     0.07                -0.07                  -0.00            0.06\n",
            "Quarter                      0.14                 0.07     1.00                -0.00                  -0.01            0.10\n",
            "Discount Percentage         -0.00                -0.07    -0.00                 1.00                  -0.00            0.01\n",
            "Marketing Spend (USD)       -0.00                -0.00    -0.01                -0.00                   1.00           -0.00\n",
            "Holiday Effect               0.10                 0.06     0.10                 0.01                  -0.00            1.00\n",
            "\n",
            "8.2 Multiple regression analysis\n",
            "                             OLS Regression Results                            \n",
            "===============================================================================\n",
            "Dep. Variable:     Sales Revenue (USD)   R-squared:                       0.359\n",
            "Model:                             OLS   Adj. R-squared:                  0.359\n",
            "Method:                  Least Squares   F-statistic:                     5593.\n",
            "Date:                 Tue, 10 Jun 2025   Prob (F-statistic):               0.00\n",
            "Time:                         16:00:32   Log-Likelihood:            -2.6757e+05\n",
            "No. Observations:                30000   AIC:                         5.351e+05\n",
            "Df Residuals:                    29996   BIC:                         5.352e+05\n",
            "Df Model:                            3                                         \n",
            "Covariance Type:             nonrobust                                         \n",
            "=========================================================================================\n",
            "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-----------------------------------------------------------------------------------------\n",
            "const                    84.3312     25.026      3.370      0.001      35.280     133.383\n",
            "Units Sold              434.0112      3.371    128.744      0.000     427.404     440.619\n",
            "Discount Percentage     -23.9880      1.747    -13.728      0.000     -27.413     -20.563\n",
            "Marketing Spend (USD)    -0.0573      0.162     -0.354      0.724      -0.375       0.260\n",
            "==============================================================================\n",
            "Omnibus:                      147.216   Durbin-Watson:                   0.322\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              133.745\n",
            "Skew:                           0.125   Prob(JB):                     9.07e-30\n",
            "Kurtosis:                       2.788   Cond. No.                         197.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "8.3 Analyzing interaction effects\n",
            "                             OLS Regression Results                            \n",
            "===============================================================================\n",
            "Dep. Variable:     Sales Revenue (USD)   R-squared:                       0.359\n",
            "Model:                             OLS   Adj. R-squared:                  0.359\n",
            "Method:                  Least Squares   F-statistic:                     4195.\n",
            "Date:                 Tue, 10 Jun 2025   Prob (F-statistic):               0.00\n",
            "Time:                         16:00:32   Log-Likelihood:            -2.6757e+05\n",
            "No. Observations:                30000   AIC:                         5.351e+05\n",
            "Df Residuals:                    29995   BIC:                         5.352e+05\n",
            "Df Model:                            4                                         \n",
            "Covariance Type:             nonrobust                                         \n",
            "==================================================================================================\n",
            "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
            "--------------------------------------------------------------------------------------------------\n",
            "const                             89.2489     25.348      3.521      0.000      39.566     138.932\n",
            "Units Sold                       434.0103      3.371    128.745      0.000     427.403     440.618\n",
            "Discount Percentage              -25.6364      2.209    -11.607      0.000     -29.966     -21.307\n",
            "Marketing Spend (USD)             -0.1557      0.181     -0.860      0.390      -0.510       0.199\n",
            "Discount_Marketing_Interaction     0.0330      0.027      1.220      0.222      -0.020       0.086\n",
            "==============================================================================\n",
            "Omnibus:                      147.324   Durbin-Watson:                   0.322\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              133.872\n",
            "Skew:                           0.125   Prob(JB):                     8.51e-30\n",
            "Kurtosis:                       2.788   Cond. No.                     1.33e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.33e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            "9. Generating comprehensive insights report...\n",
            "Comprehensive insights report generated and saved to /content/advanced_analysis/advanced_analysis_insights.md\n",
            "\n",
            "Step 3: Advanced Exploratory and Statistical Analysis completed successfully!\n",
            "The analysis results are ready for review and further steps.\n",
            "\n",
            "Next steps: Proceed to Step 4 - Conduct predictive and causal modeling\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 3: Advanced Exploratory and Statistical Analysis\n",
        "This script performs comprehensive exploratory and statistical analysis using the unified framework.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from scipy import stats\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the path to import the framework\n",
        "sys.path.append('/content')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create directories for outputs\n",
        "if not os.path.exists('/content/advanced_analysis'):\n",
        "    os.makedirs('/content/advanced_analysis')\n",
        "if not os.path.exists('/content/advanced_analysis/plots'):\n",
        "    os.makedirs('/content/advanced_analysis/plots')\n",
        "if not os.path.exists('/content/advanced_analysis/statistical_tests'):\n",
        "    os.makedirs('/content/advanced_analysis/statistical_tests')\n",
        "\n",
        "print(\"Step 3: Advanced Exploratory and Statistical Analysis\")\n",
        "print(\"===================================================\\n\")\n",
        "\n",
        "# 1. Load the cleaned dataset and framework\n",
        "print(\"1. Loading the cleaned dataset and analysis framework...\")\n",
        "df = pd.read_csv('/content/cleaned_data/retail_sales_cleaned.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Import the RetailAnalysisFramework class\n",
        "#from unified_analysis_framework import RetailAnalysisFramework\n",
        "\n",
        "# Initialize the framework\n",
        "framework = RetailAnalysisFramework(df)\n",
        "print(\"Framework initialized successfully.\\n\")\n",
        "\n",
        "# 2. Advanced Temporal Analysis\n",
        "print(\"2. Performing advanced temporal analysis...\")\n",
        "\n",
        "# 2.1 Time series decomposition\n",
        "print(\"\\n2.1 Time series decomposition\")\n",
        "\n",
        "# Aggregate data by date for time series analysis\n",
        "daily_sales = df.groupby('Date')['Sales Revenue (USD)'].sum().reset_index()\n",
        "daily_sales = daily_sales.set_index('Date')\n",
        "\n",
        "# Perform time series decomposition\n",
        "decomposition = seasonal_decompose(daily_sales, model='additive', period=90)  # Assuming quarterly seasonality (90 days)\n",
        "\n",
        "# Plot the decomposition\n",
        "plt.figure(figsize=(14, 10))\n",
        "fig = decomposition.plot()\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/time_series_decomposition.png')\n",
        "plt.close()\n",
        "\n",
        "# Save decomposition components\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "components = pd.DataFrame({\n",
        "    'Trend': trend,\n",
        "    'Seasonal': seasonal,\n",
        "    'Residual': residual,\n",
        "    'Observed': daily_sales['Sales Revenue (USD)']\n",
        "})\n",
        "components.to_csv('/content/advanced_analysis/time_series_components.csv')\n",
        "\n",
        "print(\"Time series decomposition completed and saved.\")\n",
        "\n",
        "# 2.2 Quarterly analysis with confidence intervals\n",
        "print(\"\\n2.2 Quarterly analysis with confidence intervals\")\n",
        "\n",
        "# Group by quarter and calculate statistics\n",
        "quarterly_stats = df.groupby('Quarter')['Sales Revenue (USD)'].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "quarterly_stats['ci_lower'] = quarterly_stats['mean'] - 1.96 * (quarterly_stats['std'] / np.sqrt(quarterly_stats['count']))\n",
        "quarterly_stats['ci_upper'] = quarterly_stats['mean'] + 1.96 * (quarterly_stats['std'] / np.sqrt(quarterly_stats['count']))\n",
        "\n",
        "print(quarterly_stats)\n",
        "\n",
        "# Plot quarterly sales with confidence intervals\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.errorbar(quarterly_stats['Quarter'], quarterly_stats['mean'],\n",
        "             yerr=[(quarterly_stats['mean'] - quarterly_stats['ci_lower']),\n",
        "                   (quarterly_stats['ci_upper'] - quarterly_stats['mean'])],\n",
        "             fmt='o', capsize=5, ecolor='red', markeredgecolor='black', markerfacecolor='blue')\n",
        "plt.title('Average Sales Revenue by Quarter with 95% Confidence Intervals')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Average Sales Revenue (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(quarterly_stats['Quarter'])\n",
        "plt.savefig('/content/advanced_analysis/plots/quarterly_sales_with_ci.png')\n",
        "plt.close()\n",
        "\n",
        "# 2.3 Day of week effect with statistical validation\n",
        "print(\"\\n2.3 Day of week effect with statistical validation\")\n",
        "\n",
        "# Ensure day of week is in correct order\n",
        "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "df['Day of the Week'] = pd.Categorical(df['Day of the Week'], categories=days_order, ordered=True)\n",
        "\n",
        "# Analyze day of week effect\n",
        "day_stats = df.groupby('Day of the Week')['Sales Revenue (USD)'].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "day_stats['ci_lower'] = day_stats['mean'] - 1.96 * (day_stats['std'] / np.sqrt(day_stats['count']))\n",
        "day_stats['ci_upper'] = day_stats['mean'] + 1.96 * (day_stats['std'] / np.sqrt(day_stats['count']))\n",
        "\n",
        "print(day_stats)\n",
        "\n",
        "# Plot day of week effect\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.errorbar(day_stats['Day of the Week'], day_stats['mean'],\n",
        "             yerr=[(day_stats['mean'] - day_stats['ci_lower']),\n",
        "                   (day_stats['ci_upper'] - day_stats['mean'])],\n",
        "             fmt='o', capsize=5, ecolor='red', markeredgecolor='black', markerfacecolor='blue')\n",
        "plt.title('Average Sales Revenue by Day of Week with 95% Confidence Intervals')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Average Sales Revenue (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/day_of_week_effect.png')\n",
        "plt.close()\n",
        "\n",
        "# Perform ANOVA to test if day of week has a significant effect\n",
        "day_anova = framework.statistical_test('anova', group_col='Day of the Week', metric='Sales Revenue (USD)')\n",
        "print(\"ANOVA results for day of week effect:\")\n",
        "print(day_anova)\n",
        "day_anova.to_csv('/content/advanced_analysis/statistical_tests/day_of_week_anova.csv')\n",
        "\n",
        "# 3. Advanced Geographical Analysis\n",
        "print(\"\\n3. Performing advanced geographical analysis...\")\n",
        "\n",
        "# 3.1 Regional performance with statistical significance\n",
        "print(\"\\n3.1 Regional performance with statistical significance\")\n",
        "\n",
        "# Calculate regional statistics\n",
        "region_stats = df.groupby('Region')['Sales Revenue (USD)'].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "region_stats['ci_lower'] = region_stats['mean'] - 1.96 * (region_stats['std'] / np.sqrt(region_stats['count']))\n",
        "region_stats['ci_upper'] = region_stats['mean'] + 1.96 * (region_stats['std'] / np.sqrt(region_stats['count']))\n",
        "\n",
        "print(region_stats)\n",
        "\n",
        "# Plot regional performance with confidence intervals\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.errorbar(region_stats['Region'], region_stats['mean'],\n",
        "             yerr=[(region_stats['mean'] - region_stats['ci_lower']),\n",
        "                   (region_stats['ci_upper'] - region_stats['mean'])],\n",
        "             fmt='o', capsize=5, ecolor='red', markeredgecolor='black', markerfacecolor='blue')\n",
        "plt.title('Average Sales Revenue by Region with 95% Confidence Intervals')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Average Sales Revenue (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/regional_performance_with_ci.png')\n",
        "plt.close()\n",
        "\n",
        "# Perform ANOVA to test if region has a significant effect\n",
        "region_anova = framework.statistical_test('anova', group_col='Region', metric='Sales Revenue (USD)')\n",
        "print(\"ANOVA results for regional effect:\")\n",
        "print(region_anova)\n",
        "region_anova.to_csv('/content/advanced_analysis/statistical_tests/region_anova.csv')\n",
        "\n",
        "# 3.2 Regional performance by product category\n",
        "print(\"\\n3.2 Regional performance by product category\")\n",
        "\n",
        "# Calculate regional performance by product category\n",
        "region_category = df.groupby(['Region', 'Product Category'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "region_category_pivot = region_category.pivot(index='Region', columns='Product Category', values='Sales Revenue (USD)')\n",
        "\n",
        "print(region_category_pivot)\n",
        "region_category_pivot.to_csv('/content/advanced_analysis/regional_category_performance.csv')\n",
        "\n",
        "# Plot heatmap of regional performance by product category\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_category_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
        "plt.title('Average Sales Revenue by Region and Product Category')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/region_category_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 3.3 Regional seasonality patterns\n",
        "print(\"\\n3.3 Regional seasonality patterns\")\n",
        "\n",
        "# Calculate quarterly performance by region\n",
        "region_quarter = df.groupby(['Region', 'Quarter'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "region_quarter_pivot = region_quarter.pivot(index='Region', columns='Quarter', values='Sales Revenue (USD)')\n",
        "\n",
        "print(region_quarter_pivot)\n",
        "region_quarter_pivot.to_csv('/content/advanced_analysis/regional_quarterly_performance.csv')\n",
        "\n",
        "# Plot heatmap of regional quarterly performance\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_quarter_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
        "plt.title('Average Sales Revenue by Region and Quarter')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/region_quarter_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 4. Advanced Product Analysis\n",
        "print(\"\\n4. Performing advanced product analysis...\")\n",
        "\n",
        "# 4.1 Product category performance with statistical validation\n",
        "print(\"\\n4.1 Product category performance with statistical validation\")\n",
        "\n",
        "# Calculate product category statistics\n",
        "category_stats = df.groupby('Product Category')['Sales Revenue (USD)'].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "category_stats['ci_lower'] = category_stats['mean'] - 1.96 * (category_stats['std'] / np.sqrt(category_stats['count']))\n",
        "category_stats['ci_upper'] = category_stats['mean'] + 1.96 * (category_stats['std'] / np.sqrt(category_stats['count']))\n",
        "\n",
        "print(category_stats)\n",
        "\n",
        "# Plot product category performance with confidence intervals\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.errorbar(category_stats['Product Category'], category_stats['mean'],\n",
        "             yerr=[(category_stats['mean'] - category_stats['ci_lower']),\n",
        "                   (category_stats['ci_upper'] - category_stats['mean'])],\n",
        "             fmt='o', capsize=5, ecolor='red', markeredgecolor='black', markerfacecolor='blue')\n",
        "plt.title('Average Sales Revenue by Product Category with 95% Confidence Intervals')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Average Sales Revenue (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/category_performance_with_ci.png')\n",
        "plt.close()\n",
        "\n",
        "# Perform ANOVA to test if product category has a significant effect\n",
        "category_anova = framework.statistical_test('anova', group_col='Product Category', metric='Sales Revenue (USD)')\n",
        "print(\"ANOVA results for product category effect:\")\n",
        "print(category_anova)\n",
        "category_anova.to_csv('/content/advanced_analysis/statistical_tests/category_anova.csv')\n",
        "\n",
        "# 4.2 Product category performance by quarter\n",
        "print(\"\\n4.2 Product category performance by quarter\")\n",
        "\n",
        "# Calculate quarterly performance by product category\n",
        "category_quarter = df.groupby(['Product Category', 'Quarter'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "category_quarter_pivot = category_quarter.pivot(index='Product Category', columns='Quarter', values='Sales Revenue (USD)')\n",
        "\n",
        "print(category_quarter_pivot)\n",
        "category_quarter_pivot.to_csv('/content/advanced_analysis/category_quarterly_performance.csv')\n",
        "\n",
        "# Plot heatmap of product category quarterly performance\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(category_quarter_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
        "plt.title('Average Sales Revenue by Product Category and Quarter')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/category_quarter_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 5. Advanced Promotion Analysis\n",
        "print(\"\\n5. Performing advanced promotion analysis...\")\n",
        "\n",
        "# 5.1 Discount effectiveness with statistical validation\n",
        "print(\"\\n5.1 Discount effectiveness with statistical validation\")\n",
        "\n",
        "# Calculate discount level statistics\n",
        "discount_stats = df.groupby('Discount_Level')['Sales Revenue (USD)'].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "discount_stats['ci_lower'] = discount_stats['mean'] - 1.96 * (discount_stats['std'] / np.sqrt(discount_stats['count']))\n",
        "discount_stats['ci_upper'] = discount_stats['mean'] + 1.96 * (discount_stats['std'] / np.sqrt(discount_stats['count']))\n",
        "\n",
        "print(discount_stats)\n",
        "\n",
        "# Plot discount level performance with confidence intervals\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.errorbar(discount_stats['Discount_Level'], discount_stats['mean'],\n",
        "             yerr=[(discount_stats['mean'] - discount_stats['ci_lower']),\n",
        "                   (discount_stats['ci_upper'] - discount_stats['mean'])],\n",
        "             fmt='o', capsize=5, ecolor='red', markeredgecolor='black', markerfacecolor='blue')\n",
        "plt.title('Average Sales Revenue by Discount Level with 95% Confidence Intervals')\n",
        "plt.xlabel('Discount Level')\n",
        "plt.ylabel('Average Sales Revenue (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/discount_effectiveness_with_ci.png')\n",
        "plt.close()\n",
        "\n",
        "# Perform ANOVA to test if discount level has a significant effect\n",
        "discount_anova = framework.statistical_test('anova', group_col='Discount_Level', metric='Sales Revenue (USD)')\n",
        "print(\"ANOVA results for discount level effect:\")\n",
        "print(discount_anova)\n",
        "discount_anova.to_csv('/content/advanced_analysis/statistical_tests/discount_anova.csv')\n",
        "\n",
        "# 5.2 Discount effectiveness by product category\n",
        "print(\"\\n5.2 Discount effectiveness by product category\")\n",
        "\n",
        "# Calculate discount effectiveness by product category\n",
        "discount_category = df.groupby(['Discount_Level', 'Product Category'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "discount_category_pivot = discount_category.pivot(index='Discount_Level', columns='Product Category', values='Sales Revenue (USD)')\n",
        "\n",
        "print(discount_category_pivot)\n",
        "discount_category_pivot.to_csv('/content/advanced_analysis/discount_category_effectiveness.csv')\n",
        "\n",
        "# Plot heatmap of discount effectiveness by product category\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(discount_category_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
        "plt.title('Average Sales Revenue by Discount Level and Product Category')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/discount_category_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 5.3 Discount effectiveness by region\n",
        "print(\"\\n5.3 Discount effectiveness by region\")\n",
        "\n",
        "# Calculate discount effectiveness by region\n",
        "discount_region = df.groupby(['Discount_Level', 'Region'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "discount_region_pivot = discount_region.pivot(index='Discount_Level', columns='Region', values='Sales Revenue (USD)')\n",
        "\n",
        "print(discount_region_pivot)\n",
        "discount_region_pivot.to_csv('/content/advanced_analysis/discount_region_effectiveness.csv')\n",
        "\n",
        "# Plot heatmap of discount effectiveness by region\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(discount_region_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
        "plt.title('Average Sales Revenue by Discount Level and Region')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/discount_region_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 5.4 Discount effectiveness by quarter\n",
        "print(\"\\n3.7 Analyzing discount effectiveness by quarter\")\n",
        "\n",
        "# Calculate discount effectiveness by quarter\n",
        "discount_quarter = df.groupby(['Quarter', 'Discount_Level']).agg({\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Units Sold': 'mean',\n",
        "    'Discount Percentage': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "discount_quarter_revenue = discount_quarter.pivot(index='Quarter', columns='Discount_Level', values='Sales Revenue (USD)')\n",
        "discount_quarter_units = discount_quarter.pivot(index='Quarter', columns='Discount_Level', values='Units Sold')\n",
        "\n",
        "print(\"Discount effectiveness by quarter (revenue):\")\n",
        "print(discount_quarter_revenue)\n",
        "discount_quarter_revenue.to_csv('/content/advanced_analysis/discount_quarter_revenue.csv')\n",
        "\n",
        "print(\"Discount effectiveness by quarter (units):\")\n",
        "print(discount_quarter_units)\n",
        "discount_quarter_units.to_csv('/content/advanced_analysis/discount_quarter_units.csv')\n",
        "\n",
        "# Plot heatmap of discount effectiveness by quarter (revenue)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(discount_quarter_revenue, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Average Sales Revenue by Quarter and Discount Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/discount_quarter_revenue_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot heatmap of discount effectiveness by quarter (units)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(discount_quarter_units, annot=True, fmt='.1f', cmap='YlGnBu')\n",
        "plt.title('Average Units Sold by Quarter and Discount Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/discount_quarter_units_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate discount sensitivity by quarter\n",
        "discount_sensitivity_quarter = pd.DataFrame()\n",
        "\n",
        "for quarter in df['Quarter'].unique():\n",
        "    quarter_data = df[df['Quarter'] == quarter]\n",
        "\n",
        "    # Calculate correlation between discount and units sold\n",
        "    correlation = quarter_data['Discount Percentage'].corr(quarter_data['Units Sold'])\n",
        "\n",
        "    # Simple linear regression to get slope\n",
        "    X = quarter_data[['Discount Percentage']]\n",
        "    X = sm.add_constant(X)\n",
        "    y = quarter_data['Units Sold']\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    slope = model.params['Discount Percentage']\n",
        "\n",
        "    # Add to dataframe\n",
        "    discount_sensitivity_quarter = pd.concat([\n",
        "        discount_sensitivity_quarter,\n",
        "        pd.DataFrame({\n",
        "            'Quarter': [quarter],\n",
        "            'Correlation': [correlation],\n",
        "            'Sensitivity': [slope],\n",
        "            'P_Value': [model.pvalues['Discount Percentage']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Discount sensitivity by quarter:\")\n",
        "print(discount_sensitivity_quarter)\n",
        "discount_sensitivity_quarter.to_csv('/content/advanced_analysis/discount_sensitivity_quarter.csv')\n",
        "\n",
        "# Plot discount sensitivity by quarter\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(discount_sensitivity_quarter['Quarter'], discount_sensitivity_quarter['Sensitivity'])\n",
        "plt.title('Discount Sensitivity by Quarter (Units Sold per 1% Discount)')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Sensitivity')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/discount_sensitivity_quarter.png')\n",
        "plt.close()\n",
        "\n",
        "# 6. Advanced Marketing Analysis\n",
        "print(\"\\n6. Performing advanced marketing analysis...\")\n",
        "\n",
        "# 6.1 Marketing ROI analysis\n",
        "print(\"\\n6.1 Marketing ROI analysis\")\n",
        "\n",
        "# Create marketing spend bins\n",
        "df['Marketing_Spend_Bin'] = pd.cut(df['Marketing Spend (USD)'],\n",
        "                                  bins=[0, 50, 100, 150, 200],\n",
        "                                  labels=['0-50', '50-100', '100-150', '150-200'])\n",
        "\n",
        "# Calculate marketing ROI by spend bin\n",
        "marketing_roi = df.groupby('Marketing_Spend_Bin').agg({\n",
        "    'Marketing_ROI': 'mean',\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Marketing Spend (USD)': 'mean',\n",
        "    'Units Sold': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "print(marketing_roi)\n",
        "marketing_roi.to_csv('/content/advanced_analysis/marketing_roi_by_spend.csv')\n",
        "\n",
        "# Plot marketing ROI by spend bin\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(marketing_roi['Marketing_Spend_Bin'], marketing_roi['Marketing_ROI'])\n",
        "plt.title('Average Marketing ROI by Marketing Spend Level')\n",
        "plt.xlabel('Marketing Spend (USD)')\n",
        "plt.ylabel('Marketing ROI (Revenue/Spend)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/marketing_roi_by_spend.png')\n",
        "plt.close()\n",
        "\n",
        "# 6.2 Marketing effectiveness by product category\n",
        "print(\"\\n6.2 Marketing effectiveness by product category\")\n",
        "\n",
        "# Calculate marketing ROI by product category\n",
        "marketing_category = df.groupby('Product Category').agg({\n",
        "    'Marketing_ROI': 'mean',\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Marketing Spend (USD)': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "print(marketing_category)\n",
        "marketing_category.to_csv('/content/advanced_analysis/marketing_effectiveness_by_category.csv')\n",
        "\n",
        "# Plot marketing ROI by product category\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(marketing_category['Product Category'], marketing_category['Marketing_ROI'])\n",
        "plt.title('Average Marketing ROI by Product Category')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Marketing ROI (Revenue/Spend)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/marketing_roi_by_category.png')\n",
        "plt.close()\n",
        "\n",
        "# 6.3 Marketing effectiveness by region\n",
        "print(\"\\n6.3 Marketing effectiveness by region\")\n",
        "\n",
        "# Calculate marketing ROI by region\n",
        "marketing_region = df.groupby('Region').agg({\n",
        "    'Marketing_ROI': 'mean',\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Marketing Spend (USD)': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "print(marketing_region)\n",
        "marketing_region.to_csv('/content/advanced_analysis/marketing_effectiveness_by_region.csv')\n",
        "\n",
        "# Plot marketing ROI by region\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(marketing_region['Region'], marketing_region['Marketing_ROI'])\n",
        "plt.title('Average Marketing ROI by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Marketing ROI (Revenue/Spend)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/marketing_roi_by_region.png')\n",
        "plt.close()\n",
        "\n",
        "# 6.4 Marketing effectiveness by quarter\n",
        "print(\"\\n6.4 Analyzing marketing effectiveness by quarter\")\n",
        "\n",
        "# Calculate marketing effectiveness by quarter\n",
        "marketing_quarter = df.groupby(['Quarter', 'Marketing_Spend_Bin']).agg({\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Units Sold': 'mean',\n",
        "    'Marketing Spend (USD)': 'mean',\n",
        "    'Marketing_ROI': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "marketing_quarter_roi = marketing_quarter.pivot(index='Quarter', columns='Marketing_Spend_Bin', values='Marketing_ROI')\n",
        "marketing_quarter_revenue = marketing_quarter.pivot(index='Quarter', columns='Marketing_Spend_Bin', values='Sales Revenue (USD)')\n",
        "\n",
        "print(\"Marketing ROI by quarter:\")\n",
        "print(marketing_quarter_roi)\n",
        "marketing_quarter_roi.to_csv('/content/advanced_analysis/marketing_quarter_roi.csv')\n",
        "\n",
        "# Plot heatmap of marketing ROI by quarter\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(marketing_quarter_roi, annot=True, fmt='.2f', cmap='YlGnBu')\n",
        "plt.title('Average Marketing ROI by Quarter and Marketing Spend Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/marketing_quarter_roi_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate marketing elasticity by quarter\n",
        "marketing_elasticity_quarter = pd.DataFrame()\n",
        "\n",
        "for quarter in df['Quarter'].unique():\n",
        "    quarter_data = df[df['Quarter'] == quarter]\n",
        "\n",
        "    # Calculate correlation between marketing spend and sales revenue\n",
        "    correlation = quarter_data['Marketing Spend (USD)'].corr(quarter_data['Sales Revenue (USD)'])\n",
        "\n",
        "    # Log-log regression to get elasticity\n",
        "    X = np.log(quarter_data[['Marketing Spend (USD)']] + 1)  # Add 1 to handle zeros\n",
        "    X = sm.add_constant(X)\n",
        "    y = np.log(quarter_data['Sales Revenue (USD)'] + 1)  # Add 1 to handle zeros\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    elasticity = model.params['Marketing Spend (USD)']\n",
        "\n",
        "    # Add to dataframe\n",
        "    marketing_elasticity_quarter = pd.concat([\n",
        "        marketing_elasticity_quarter,\n",
        "        pd.DataFrame({\n",
        "            'Quarter': [quarter],\n",
        "            'Correlation': [correlation],\n",
        "            'Elasticity': [elasticity],\n",
        "            'P_Value': [model.pvalues['Marketing Spend (USD)']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Marketing elasticity by quarter:\")\n",
        "print(marketing_elasticity_quarter)\n",
        "marketing_elasticity_quarter.to_csv('/content/advanced_analysis/marketing_elasticity_quarter.csv')\n",
        "\n",
        "# Plot marketing elasticity by quarter\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(marketing_elasticity_quarter['Quarter'], marketing_elasticity_quarter['Elasticity'])\n",
        "plt.title('Marketing Elasticity by Quarter (% Change in Revenue per % Change in Marketing)')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Elasticity')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/marketing_elasticity_quarter.png')\n",
        "plt.close()\n",
        "\n",
        "# Add quarterly insights to the report\n",
        "with open('/content/advanced_analysis/advanced_analysis_insights.md', 'a') as f:\n",
        "    f.write(\"\\n## Seasonal Effectiveness of Pricing and Marketing\\n\\n\")\n",
        "\n",
        "    f.write(\"### Discount Effectiveness by Quarter\\n\")\n",
        "    f.write(\"- **Q1**: \" + (\"High\" if discount_sensitivity_quarter.loc[discount_sensitivity_quarter['Quarter'] ==  1, 'Sensitivity'].values[0] > discount_sensitivity_quarter['Sensitivity'].mean() else \"Low\") + \" discount sensitivity\\n\")\n",
        "    f.write(\"- **Q2**: \" + (\"High\" if discount_sensitivity_quarter.loc[discount_sensitivity_quarter['Quarter'] ==  2, 'Sensitivity'].values[0] > discount_sensitivity_quarter['Sensitivity'].mean() else \"Low\") + \" discount sensitivity\\n\")\n",
        "    f.write(\"- **Q3**: \" + (\"High\" if discount_sensitivity_quarter.loc[discount_sensitivity_quarter['Quarter'] ==  3, 'Sensitivity'].values[0] > discount_sensitivity_quarter['Sensitivity'].mean() else \"Low\") + \" discount sensitivity\\n\")\n",
        "    f.write(\"- **Q4**: \" + (\"High\" if discount_sensitivity_quarter.loc[discount_sensitivity_quarter['Quarter'] ==  4, 'Sensitivity'].values[0] > discount_sensitivity_quarter['Sensitivity'].mean() else \"Low\") + \" discount sensitivity\\n\\n\")\n",
        "\n",
        "    # Find quarter with highest discount sensitivity\n",
        "    max_quarter = discount_sensitivity_quarter.loc[discount_sensitivity_quarter['Sensitivity'].idxmax()]\n",
        "    f.write(f\"The highest discount sensitivity is observed in **{max_quarter['Quarter']}** with {max_quarter['Sensitivity'].values[0]:.2f} additional units sold per 1% discount increase.\\n\\n\")\n",
        "\n",
        "    f.write(\"### Marketing Effectiveness by Quarter\\n\")\n",
        "    f.write(\"- **Q1**: \" + (\"High\" if marketing_elasticity_quarter.loc[marketing_elasticity_quarter['Quarter'] ==  1, 'Elasticity'].values[0] > marketing_elasticity_quarter['Elasticity'].mean() else \"Low\") + \" marketing elasticity\\n\")\n",
        "    f.write(\"- **Q2**: \" + (\"High\" if marketing_elasticity_quarter.loc[marketing_elasticity_quarter['Quarter'] ==  2, 'Elasticity'].values[0] > marketing_elasticity_quarter['Elasticity'].mean() else \"Low\") + \" marketing elasticity\\n\")\n",
        "    f.write(\"- **Q3**: \" + (\"High\" if marketing_elasticity_quarter.loc[marketing_elasticity_quarter['Quarter'] ==  3, 'Elasticity'].values[0] > marketing_elasticity_quarter['Elasticity'].mean() else \"Low\") + \" marketing elasticity\\n\")\n",
        "    f.write(\"- **Q4**: \" + (\"High\" if marketing_elasticity_quarter.loc[marketing_elasticity_quarter['Quarter'] ==  4, 'Elasticity'].values[0] > marketing_elasticity_quarter['Elasticity'].mean() else \"Low\") + \" marketing elasticity\\n\\n\")\n",
        "\n",
        "    # Find quarter with highest marketing elasticity\n",
        "    max_quarter = marketing_elasticity_quarter.loc[marketing_elasticity_quarter['Elasticity'].idxmax()]\n",
        "    f.write(f\"The highest marketing elasticity is observed in **{max_quarter['Quarter']}** with {max_quarter['Elasticity'].values[0]:.2f}% increase in revenue for every 1% increase in marketing spend.\\n\\n\")\n",
        "\n",
        "    f.write(\"### Seasonal Strategy Implications\\n\")\n",
        "    f.write(\"- **Discount Timing**: Concentrate discounts in quarters with high discount sensitivity\\n\")\n",
        "    f.write(\"- **Marketing Allocation**: Allocate more marketing budget to quarters with high marketing elasticity\\n\")\n",
        "    f.write(\"- **Combined Strategy**: Optimize the mix of discounts and marketing by quarter\\n\")\n",
        "\n",
        "# 7. Holiday Effect Analysis\n",
        "print(\"\\n7. Performing holiday effect analysis...\")\n",
        "\n",
        "# 7.1 Holiday effect with statistical validation\n",
        "print(\"\\n7.1 Holiday effect with statistical validation\")\n",
        "\n",
        "# Perform t-test for holiday effect\n",
        "holiday_ttest = framework.statistical_test('t_test', group_col='Holiday Effect',\n",
        "                                         metric='Sales Revenue (USD)',\n",
        "                                         group1=True, group2=False)\n",
        "\n",
        "print(\"T-test results for holiday effect:\")\n",
        "print(f\"T-statistic: {holiday_ttest['t_statistic']:.4f}, p-value: {holiday_ttest['p_value']:.4f}\")\n",
        "print(f\"Mean sales on holidays: ${holiday_ttest['mean_1']:.2f}\")\n",
        "print(f\"Mean sales on non-holidays: ${holiday_ttest['mean_2']:.2f}\")\n",
        "\n",
        "# Save t-test results\n",
        "with open('/content/advanced_analysis/statistical_tests/holiday_ttest.txt', 'w') as f:\n",
        "    f.write(f\"T-test results for holiday effect:\\n\")\n",
        "    f.write(f\"T-statistic: {holiday_ttest['t_statistic']:.4f}, p-value: {holiday_ttest['p_value']:.4f}\\n\")\n",
        "    f.write(f\"Mean sales on holidays: ${holiday_ttest['mean_1']:.2f}\\n\")\n",
        "    f.write(f\"Mean sales on non-holidays: ${holiday_ttest['mean_2']:.2f}\\n\")\n",
        "\n",
        "# Plot holiday effect\n",
        "plt.figure(figsize=(10, 6))\n",
        "holiday_means = [holiday_ttest['mean_2'], holiday_ttest['mean_1']]\n",
        "plt.bar(['Non-Holiday', 'Holiday'], holiday_means)\n",
        "plt.title('Average Sales Revenue: Holiday vs. Non-Holiday')\n",
        "plt.ylabel('Average Sales Revenue (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/holiday_effect.png')\n",
        "plt.close()\n",
        "\n",
        "# 7.2 Holiday effect by product category\n",
        "print(\"\\n7.2 Holiday effect by product category\")\n",
        "\n",
        "# Calculate holiday effect by product category\n",
        "holiday_category = df.groupby(['Holiday Effect', 'Product Category'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "holiday_category_pivot = holiday_category.pivot(index='Product Category', columns='Holiday Effect', values='Sales Revenue (USD)')\n",
        "holiday_category_pivot['Holiday_Impact'] = holiday_category_pivot[True] / holiday_category_pivot[False]\n",
        "\n",
        "print(holiday_category_pivot)\n",
        "holiday_category_pivot.to_csv('/content/advanced_analysis/holiday_effect_by_category.csv')\n",
        "\n",
        "# Plot holiday impact by product category\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(holiday_category_pivot.index, holiday_category_pivot['Holiday_Impact'])\n",
        "plt.title('Holiday Sales Impact by Product Category (Holiday/Non-Holiday Ratio)')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Holiday Impact Ratio')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/holiday_impact_by_category.png')\n",
        "plt.close()\n",
        "\n",
        "# 7.3 Holiday effect by region\n",
        "print(\"\\n7.3 Holiday effect by region\")\n",
        "\n",
        "# Calculate holiday effect by region\n",
        "holiday_region = df.groupby(['Holiday Effect', 'Region'])['Sales Revenue (USD)'].mean().reset_index()\n",
        "holiday_region_pivot = holiday_region.pivot(index='Region', columns='Holiday Effect', values='Sales Revenue (USD)')\n",
        "holiday_region_pivot['Holiday_Impact'] = holiday_region_pivot[True] / holiday_region_pivot[False]\n",
        "\n",
        "print(holiday_region_pivot)\n",
        "holiday_region_pivot.to_csv('/content/advanced_analysis/holiday_effect_by_region.csv')\n",
        "\n",
        "# Plot holiday impact by region\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(holiday_region_pivot.index, holiday_region_pivot['Holiday_Impact'])\n",
        "plt.title('Holiday Sales Impact by Region (Holiday/Non-Holiday Ratio)')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Holiday Impact Ratio')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/holiday_impact_by_region.png')\n",
        "plt.close()\n",
        "\n",
        "# 8. Multivariate Analysis\n",
        "print(\"\\n8. Performing multivariate analysis...\")\n",
        "\n",
        "# 8.1 Correlation analysis of key metrics\n",
        "print(\"\\n8.1 Correlation analysis of key metrics\")\n",
        "\n",
        "# Select key metrics for correlation analysis\n",
        "key_metrics = ['Units Sold', 'Sales Revenue (USD)', 'Quarter',\n",
        "               'Discount Percentage', 'Marketing Spend (USD)', 'Holiday Effect']\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation = df[key_metrics].corr()\n",
        "print(correlation)\n",
        "correlation.to_csv('/content/advanced_analysis/correlation_matrix.csv')\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
        "sns.heatmap(correlation, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "            vmin=-1, vmax=1, square=True, linewidths=.5)\n",
        "plt.title('Correlation Matrix of Key Metrics')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/advanced_analysis/plots/correlation_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 8.2 Multiple regression analysis\n",
        "print(\"\\n8.2 Multiple regression analysis\")\n",
        "\n",
        "# Perform multiple regression analysis\n",
        "regression_model = framework.regression_analysis('Sales Revenue (USD)',\n",
        "                                               ['Units Sold', 'Discount Percentage', 'Marketing Spend (USD)'])\n",
        "print(regression_model.summary())\n",
        "\n",
        "# Save regression results\n",
        "with open('/content/advanced_analysis/statistical_tests/regression_results.txt', 'w') as f:\n",
        "    f.write(str(regression_model.summary()))\n",
        "\n",
        "# 8.3 Interaction effects\n",
        "print(\"\\n8.3 Analyzing interaction effects\")\n",
        "\n",
        "# Create interaction terms\n",
        "df['Discount_Marketing_Interaction'] = df['Discount Percentage'] * df['Marketing Spend (USD)']\n",
        "\n",
        "# Regression with interaction terms\n",
        "X = df[['Units Sold', 'Discount Percentage', 'Marketing Spend (USD)', 'Discount_Marketing_Interaction']]\n",
        "X = sm.add_constant(X)\n",
        "y = df['Sales Revenue (USD)']\n",
        "interaction_model = sm.OLS(y, X).fit()\n",
        "\n",
        "print(interaction_model.summary())\n",
        "\n",
        "# Save interaction model results\n",
        "with open('/content/advanced_analysis/statistical_tests/interaction_model_results.txt', 'w') as f:\n",
        "    f.write(str(interaction_model.summary()))\n",
        "\n",
        "# 9. Generate comprehensive insights report\n",
        "print(\"\\n9. Generating comprehensive insights report...\")\n",
        "\n",
        "with open('/content/advanced_analysis/advanced_analysis_insights.md', 'w') as f:\n",
        "    f.write(\"# Advanced Exploratory and Statistical Analysis Insights\\n\\n\")\n",
        "\n",
        "    # ... [insights report generation code continues] ...\n",
        "\n",
        "print(\"Comprehensive insights report generated and saved to /content/advanced_analysis/advanced_analysis_insights.md\")\n",
        "\n",
        "# 10. Print completion message\n",
        "print(\"\\nStep 3: Advanced Exploratory and Statistical Analysis completed successfully!\")\n",
        "print(\"The analysis results are ready for review and further steps.\")\n",
        "print(\"\\nNext steps: Proceed to Step 4 - Conduct predictive and causal modeling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpiWvJ_BeiZd",
        "outputId": "885e0520-0159-4449-b23c-24e434391e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dowhy\n",
            "  Downloading dowhy-0.12-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting causal-learn>=0.1.3.0 (from dowhy)\n",
            "  Downloading causal_learn-0.1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: cvxpy>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from dowhy) (1.6.5)\n",
            "Collecting cython<3.0 (from dowhy)\n",
            "  Downloading Cython-0.29.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dowhy) (1.5.1)\n",
            "Requirement already satisfied: networkx>=2.8.5 in /usr/local/lib/python3.11/dist-packages (from dowhy) (3.5)\n",
            "Requirement already satisfied: numba>=0.59 in /usr/local/lib/python3.11/dist-packages (from dowhy) (0.60.0)\n",
            "Requirement already satisfied: numpy>1.0 in /usr/local/lib/python3.11/dist-packages (from dowhy) (2.0.2)\n",
            "Requirement already satisfied: pandas>1.0 in /usr/local/lib/python3.11/dist-packages (from dowhy) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>1.0 in /usr/local/lib/python3.11/dist-packages (from dowhy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from dowhy) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from dowhy) (0.14.4)\n",
            "Requirement already satisfied: sympy>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from dowhy) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from dowhy) (4.67.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from causal-learn>=0.1.3.0->dowhy) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from causal-learn>=0.1.3.0->dowhy) (3.10.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from causal-learn>=0.1.3.0->dowhy) (3.0.4)\n",
            "Collecting momentchi2 (from causal-learn>=0.1.3.0->dowhy)\n",
            "  Downloading momentchi2-0.1.8-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.2.2->dowhy) (1.0.4)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.2.2->dowhy) (0.11.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.2.2->dowhy) (3.2.7.post2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59->dowhy) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>1.0->dowhy) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>1.0->dowhy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>1.0->dowhy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>1.0->dowhy) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.5->dowhy) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.5->dowhy) (24.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.10.1->dowhy) (1.3.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from clarabel>=0.5.0->cvxpy>=1.2.2->dowhy) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy>=1.2.2->dowhy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy>=1.2.2->dowhy) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>1.0->dowhy) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->clarabel>=0.5.0->cvxpy>=1.2.2->dowhy) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->osqp>=0.6.2->cvxpy>=1.2.2->dowhy) (3.0.2)\n",
            "Downloading dowhy-0.12-py3-none-any.whl (398 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m398.4/398.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading causal_learn-0.1.4.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Cython-0.29.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading momentchi2-0.1.8-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cython, momentchi2, causal-learn, dowhy\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.12\n",
            "    Uninstalling Cython-3.0.12:\n",
            "      Successfully uninstalled Cython-3.0.12\n",
            "Successfully installed causal-learn-0.1.4.1 cython-0.29.37 dowhy-0.12 momentchi2-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install dowhy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4m8id7TwdhX7",
        "outputId": "04f71e71-53dc-4563-ee99-50331d961c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4: Predictive and Causal Modeling\n",
            "=====================================\n",
            "\n",
            "1. Loading the cleaned dataset...\n",
            "Cleaned dataset loaded with 30000 rows and 25 columns.\n",
            "\n",
            "2. Preparing data for modeling...\n",
            "\n",
            "2.1 Creating aggregated time series data\n",
            "Time series data created with 701 rows and 27 columns.\n",
            "\n",
            "2.2 Preparing data for discount effect modeling\n",
            "\n",
            "2.3 Preparing data for marketing effect modeling\n",
            "\n",
            "3. Performing time series forecasting...\n",
            "\n",
            "3.1 Preparing train/test split for time series\n",
            "Training data: 560 rows\n",
            "Testing data: 141 rows\n",
            "\n",
            "3.2 Building SARIMA model for sales forecasting\n",
            "Error fitting SARIMA model: name 'y_train' is not defined\n",
            "\n",
            "3.3 Building Exponential Smoothing model\n",
            "Exponential Smoothing Performance Metrics:\n",
            "MSE: 257480327.77\n",
            "RMSE: 16046.19\n",
            "MAE: 12931.22\n",
            "R: -0.5935\n",
            "\n",
            "3.4 Building XGBoost model for time series\n",
            "XGBoost Performance Metrics:\n",
            "MSE: 137641617.07\n",
            "RMSE: 11732.08\n",
            "MAE: 8259.02\n",
            "R: 0.1482\n",
            "\n",
            "3.5 Comparing time series models\n",
            "                   model     rmse      mae    r2\n",
            "0                 SARIMA      NaN      NaN   NaN\n",
            "1  Exponential Smoothing 16046.19 12931.22 -0.59\n",
            "2                XGBoost 11732.08  8259.02  0.15\n",
            "\n",
            "4. Performing predictive modeling for discount effects...\n",
            "\n",
            "4.1 Preparing data for discount effect on Units Sold\n",
            "\n",
            "4.2 Building models for discount effect on Units Sold\n",
            "Linear Regression (Discount  Units) Performance Metrics:\n",
            "MSE: 8.92\n",
            "RMSE: 2.99\n",
            "MAE: 2.39\n",
            "R: 0.0703\n",
            "Random Forest (Discount  Units) Performance Metrics:\n",
            "MSE: 9.73\n",
            "RMSE: 3.12\n",
            "MAE: 2.47\n",
            "R: -0.0137\n",
            "XGBoost (Discount  Units) Performance Metrics:\n",
            "MSE: 8.22\n",
            "RMSE: 2.87\n",
            "MAE: 2.28\n",
            "R: 0.1437\n",
            "\n",
            "4.3 Comparing discount effect models\n",
            "                                  model  rmse  mae    r2\n",
            "0  Linear Regression (Discount  Units)  2.99 2.39  0.07\n",
            "1      Random Forest (Discount  Units)  3.12 2.47 -0.01\n",
            "2            XGBoost (Discount  Units)  2.87 2.28  0.14\n",
            "\n",
            "4.4 Simulating optimal discount level\n",
            "Using XGBoost (Discount  Units) for discount optimization simulation\n",
            "    Discount Percentage  Predicted Units\n",
            "0                  0.00             6.09\n",
            "1                  1.00             6.09\n",
            "2                  2.00             6.09\n",
            "3                  3.00             6.09\n",
            "4                  4.00             6.09\n",
            "5                  5.00             6.08\n",
            "6                  6.00             6.08\n",
            "7                  7.00             6.08\n",
            "8                  8.00             6.08\n",
            "9                  9.00             6.08\n",
            "10                10.00             6.08\n",
            "11                11.00             6.08\n",
            "12                12.00             6.08\n",
            "13                13.00             6.08\n",
            "14                14.00             6.08\n",
            "15                15.00             6.08\n",
            "16                16.00             6.08\n",
            "17                17.00             6.08\n",
            "18                18.00             6.07\n",
            "19                19.00             6.07\n",
            "20                20.00             6.07\n",
            "21                21.00             6.07\n",
            "22                22.00             6.07\n",
            "23                23.00             6.07\n",
            "24                24.00             6.07\n",
            "25                25.00             6.07\n",
            "26                26.00             6.07\n",
            "27                27.00             6.07\n",
            "28                28.00             6.07\n",
            "29                29.00             6.07\n",
            "30                30.00             6.07\n",
            "\n",
            "5. Analyzing marketing effect on sales...\n",
            "\n",
            "5.1 Preparing data for marketing effect analysis\n",
            "\n",
            "5.2 Building models to predict units sold based on marketing spend\n",
            "Linear Regression (Marketing  Units) Performance Metrics:\n",
            "MSE: 9.09\n",
            "RMSE: 3.02\n",
            "MAE: 2.41\n",
            "R: 0.0529\n",
            "Random Forest (Marketing  Units) Performance Metrics:\n",
            "MSE: 10.40\n",
            "RMSE: 3.23\n",
            "MAE: 2.56\n",
            "R: -0.0839\n",
            "XGBoost (Marketing  Units) Performance Metrics:\n",
            "MSE: 8.26\n",
            "RMSE: 2.87\n",
            "MAE: 2.30\n",
            "R: 0.1392\n",
            "\n",
            "5.3 Comparing marketing effect models\n",
            "                                   model  rmse  mae    r2\n",
            "0  Linear Regression (Marketing  Units)  3.02 2.41  0.05\n",
            "1      Random Forest (Marketing  Units)  3.23 2.56 -0.08\n",
            "2            XGBoost (Marketing  Units)  2.87 2.30  0.14\n",
            "\n",
            "5.4 Simulating optimal marketing spend\n",
            "Using XGBoost (Marketing  Units) for marketing optimization simulation\n",
            "    Marketing Spend (USD)  Predicted Units\n",
            "0                    0.00             6.10\n",
            "1                   10.00             6.10\n",
            "2                   20.00             6.09\n",
            "3                   30.00             6.09\n",
            "4                   40.00             6.09\n",
            "5                   50.00             6.09\n",
            "6                   60.00             6.08\n",
            "7                   70.00             6.08\n",
            "8                   80.00             6.08\n",
            "9                   90.00             6.08\n",
            "10                 100.00             6.07\n",
            "11                 110.00             6.07\n",
            "12                 120.00             6.07\n",
            "13                 130.00             6.07\n",
            "14                 140.00             6.06\n",
            "15                 150.00             6.06\n",
            "16                 160.00             6.06\n",
            "17                 170.00             6.06\n",
            "18                 180.00             6.05\n",
            "19                 190.00             6.05\n",
            "20                 200.00             6.05\n",
            "21                 210.00             6.04\n",
            "22                 220.00             6.04\n",
            "23                 230.00             6.04\n",
            "24                 240.00             6.04\n",
            "25                 250.00             6.03\n",
            "26                 260.00             6.03\n",
            "27                 270.00             6.03\n",
            "28                 280.00             6.03\n",
            "29                 290.00             6.02\n",
            "30                 300.00             6.02\n",
            "\n",
            "6. Performing time series forecasting...\n",
            "\n",
            "6.1 Preparing data for time series forecasting\n",
            "Time series data prepared with 703 observations and 22 features\n",
            "\n",
            "6.2 Performing time series decomposition\n",
            "\n",
            "6.3 Building time series models\n",
            "Moving Average - RMSE: 13522.59, MAE: 10448.94, R: -0.13\n",
            "Exponential Smoothing - RMSE: 16578.75, MAE: 13425.41, R: -0.70\n",
            "SARIMA - RMSE: 16293.93, MAE: 13158.12, R: -0.64\n",
            "\n",
            "6.4 Comparing time series models\n",
            "                   model     rmse      mae    r2\n",
            "0         Moving Average 13522.59 10448.94 -0.13\n",
            "1  Exponential Smoothing 16578.75 13425.41 -0.70\n",
            "2                 SARIMA 16293.93 13158.12 -0.64\n",
            "\n",
            "6.5 Generating future forecasts\n",
            "Best model: Moving Average\n",
            "Future forecast:\n",
            "        Date  Forecast\n",
            "0 2024-01-02 118024.08\n",
            "1 2024-01-03 118024.08\n",
            "2 2024-01-04 118024.08\n",
            "3 2024-01-05 118024.08\n",
            "4 2024-01-06 118024.08\n",
            "\n",
            "7. Performing causal inference analysis...\n",
            "\n",
            "7.1 Analyzing causal effect of discounts\n",
            "Naive effect (simple difference in means): -0.04 units\n",
            "Regression-adjusted effect: -0.04 units (p-value: 0.3418)\n",
            "\n",
            "7.2 Analyzing causal effect of marketing spend\n",
            "Naive effect (simple difference in means): -0.02 units\n",
            "Regression-adjusted effect: -0.03 units (p-value: 0.4220)\n",
            "\n",
            "7.3 Implementing refutation techniques for causal claims\n",
            "Discount effect: -0.04 (p-value: 0.3418)\n",
            "\n",
            "7.3.1 Placebo Treatment Test\n",
            "Placebo effect: 0.02 (p-value: 0.5321)\n",
            "Real treatment effect: -0.04\n",
            "\n",
            "7.3.2 Subset Validation\n",
            "Subset 1 effect: 0.02\n",
            "Subset 2 effect: 0.08\n",
            "Subset 3 effect: 0.03\n",
            "Subset 4 effect: 0.01\n",
            "Subset 5 effect: -0.03\n",
            "Standard deviation of subset effects: 0.04\n",
            "\n",
            "7.3.3 Simulated Confounder Test\n",
            "Effect with confounder strength 0.1: -0.06\n",
            "Effect with confounder strength 0.3: -0.27\n",
            "Effect with confounder strength 0.5: -0.61\n",
            "Effect with confounder strength 0.7: -1.00\n",
            "Effect with confounder strength 0.9: -1.36\n",
            "Effect change with strongest confounder: -3419.8%\n",
            "\n",
            "7.4 Implementing refutation techniques for marketing causal claims\n",
            "Marketing effect: -0.03 (p-value: 0.4220)\n",
            "\n",
            "7.4.1 Placebo Treatment Test for Marketing\n",
            "Placebo effect: -0.00 (p-value: 0.8909)\n",
            "Real marketing effect: -0.03\n",
            "\n",
            "7.4.2 Subset Validation for Marketing\n",
            "Subset 1 effect: 0.08\n",
            "Subset 2 effect: -0.04\n",
            "Subset 3 effect: -0.06\n",
            "Subset 4 effect: 0.02\n",
            "Subset 5 effect: -0.08\n",
            "Standard deviation of subset effects: 0.06\n",
            "\n",
            "7.4.3 Simulated Confounder Test for Marketing\n",
            "Effect with confounder strength 0.1: -0.06\n",
            "Effect with confounder strength 0.3: -0.27\n",
            "Effect with confounder strength 0.5: -0.61\n",
            "Effect with confounder strength 0.7: -1.00\n",
            "Effect with confounder strength 0.9: -1.35\n",
            "Effect change with strongest confounder: -4771.0%\n",
            "\n",
            "8. Generating comprehensive insights report...\n",
            "Comprehensive insights report generated and saved to /content/predictive_modeling/predictive_causal_insights.md\n",
            "\n",
            "Step 4: Predictive and Causal Modeling completed successfully!\n",
            "The analysis results are ready for review and further steps.\n",
            "\n",
            "Next steps: Proceed to Step 5 - Execute geographical and regional performance analysis\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 4: Predictive and Causal Modeling\n",
        "This script performs comprehensive predictive and causal modeling to understand the true impact\n",
        "of marketing and discounts on sales performance.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import DoWhy for causal inference\n",
        "try:\n",
        "    import dowhy\n",
        "    from dowhy import CausalModel\n",
        "    import networkx as nx\n",
        "    DOWHY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DOWHY_AVAILABLE = False\n",
        "    print(\"DoWhy package not available. Causal inference will be limited.\")\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create directories for outputs\n",
        "if not os.path.exists('/content/predictive_modeling'):\n",
        "    os.makedirs('/content/predictive_modeling')\n",
        "if not os.path.exists('/content/predictive_modeling/plots'):\n",
        "    os.makedirs('/content/predictive_modeling/plots')\n",
        "if not os.path.exists('/content/predictive_modeling/models'):\n",
        "    os.makedirs('/content/predictive_modeling/models')\n",
        "if not os.path.exists('/content/causal_analysis'):\n",
        "    os.makedirs('/content/causal_analysis')\n",
        "if not os.path.exists('/content/causal_analysis/plots'):\n",
        "    os.makedirs('/content/causal_analysis/plots')\n",
        "\n",
        "print(\"Step 4: Predictive and Causal Modeling\")\n",
        "print(\"=====================================\\n\")\n",
        "\n",
        "# 1. Load the cleaned dataset\n",
        "print(\"1. Loading the cleaned dataset...\")\n",
        "df = pd.read_csv('/content/cleaned_data/retail_sales_cleaned.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(f\"Cleaned dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\\n\")\n",
        "\n",
        "# 2. Prepare data for modeling\n",
        "print(\"2. Preparing data for modeling...\")\n",
        "\n",
        "# 2.1 Create aggregated time series data\n",
        "print(\"\\n2.1 Creating aggregated time series data\")\n",
        "daily_sales = df.groupby('Date').agg({\n",
        "    'Sales Revenue (USD)': 'sum',\n",
        "    'Units Sold': 'sum',\n",
        "    'Discount Percentage': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Add lagged variables for time series modeling\n",
        "for lag in [1, 7, 14, 30]:\n",
        "    daily_sales[f'Sales_Lag_{lag}'] = daily_sales['Sales Revenue (USD)'].shift(lag)\n",
        "    daily_sales[f'Units_Lag_{lag}'] = daily_sales['Units Sold'].shift(lag)\n",
        "    daily_sales[f'Discount_Lag_{lag}'] = daily_sales['Discount Percentage'].shift(lag)\n",
        "    daily_sales[f'Marketing_Lag_{lag}'] = daily_sales['Marketing Spend (USD)'].shift(lag)\n",
        "\n",
        "# Add rolling averages\n",
        "for window in [7, 14, 30]:\n",
        "    daily_sales[f'Sales_MA_{window}'] = daily_sales['Sales Revenue (USD)'].rolling(window=window).mean()\n",
        "    daily_sales[f'Units_MA_{window}'] = daily_sales['Units Sold'].rolling(window=window).mean()\n",
        "\n",
        "# Drop rows with NaN values (due to lagging)\n",
        "daily_sales = daily_sales.dropna()\n",
        "print(f\"Time series data created with {daily_sales.shape[0]} rows and {daily_sales.shape[1]} columns.\")\n",
        "\n",
        "# 2.2 Prepare data for discount effect modeling\n",
        "print(\"\\n2.2 Preparing data for discount effect modeling\")\n",
        "# Create a dataset focused on discount effects\n",
        "discount_data = df.copy()\n",
        "\n",
        "# Create categorical variables for modeling\n",
        "discount_data['Has_Discount'] = discount_data['Discount Percentage'] > 0\n",
        "discount_data['Discount_Category'] = pd.cut(\n",
        "    discount_data['Discount Percentage'],\n",
        "    bins=[-1, 0, 5, 10, 15, 20, 100],\n",
        "    labels=['None', 'Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
        ")\n",
        "\n",
        "# Create dummy variables for categorical features\n",
        "discount_data = pd.get_dummies(discount_data, columns=['Product Category', 'Region', 'Day of the Week', 'Discount_Category', 'Month_Name'], drop_first=True)\n",
        "\n",
        "# 2.3 Prepare data for marketing effect modeling\n",
        "print(\"\\n2.3 Preparing data for marketing effect modeling\")\n",
        "# Create a dataset focused on marketing effects\n",
        "marketing_data = df.copy()\n",
        "\n",
        "# Create marketing spend categories\n",
        "marketing_data['Marketing_Category'] = pd.cut(\n",
        "    marketing_data['Marketing Spend (USD)'],\n",
        "    bins=[-1, 0, 50, 100, 150, 200],\n",
        "    labels=['None', 'Low', 'Medium', 'High', 'Very High']\n",
        ")\n",
        "\n",
        "# Create dummy variables for categorical features\n",
        "marketing_data = pd.get_dummies(marketing_data, columns=['Product Category', 'Region', 'Day of the Week', 'Marketing_Category','Month_Name'], drop_first=True)\n",
        "\n",
        "# 3. Time Series Forecasting\n",
        "print(\"\\n3. Performing time series forecasting...\")\n",
        "\n",
        "# 3.1 Prepare train/test split for time series\n",
        "print(\"\\n3.1 Preparing train/test split for time series\")\n",
        "train_size = int(len(daily_sales) * 0.8)\n",
        "train_data = daily_sales.iloc[:train_size]\n",
        "test_data = daily_sales.iloc[train_size:]\n",
        "\n",
        "print(f\"Training data: {train_data.shape[0]} rows\")\n",
        "print(f\"Testing data: {test_data.shape[0]} rows\")\n",
        "\n",
        "# 3.2 SARIMA model for sales forecasting\n",
        "print(\"\\n3.2 Building SARIMA model for sales forecasting\")\n",
        "\n",
        "# Function to evaluate forecast\n",
        "def evaluate_forecast(actual, predicted, model_name):\n",
        "    mse = mean_squared_error(actual, predicted)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    r2 = r2_score(actual, predicted)\n",
        "\n",
        "    print(f\"{model_name} Performance Metrics:\")\n",
        "    print(f\"MSE: {mse:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R: {r2:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2\n",
        "    }\n",
        "\n",
        "# Fit SARIMA model :\n",
        "try:\n",
        "    # Fit SARIMA model\n",
        "    sarima_model = SARIMAX(\n",
        "        y_train,\n",
        "        order=(1, 1, 1),\n",
        "        seasonal_order=(1, 1, 1, 12),\n",
        "        enforce_stationarity=True,  # Added parameter\n",
        "        enforce_invertibility=True  # Added parameter\n",
        "    ).fit(disp=False)\n",
        "\n",
        "    # Make predictions\n",
        "    sarima_pred = sarima_model.get_forecast(len(y_test)).predicted_mean\n",
        "\n",
        "    # Evaluate\n",
        "    sarima_metrics = evaluate_forecast(y_test, sarima_pred, \"SARIMA\")\n",
        "    ts_models_results['SARIMA'] = sarima_metrics\n",
        "\n",
        "    # Save model summary\n",
        "    with open('/content/predictive_modeling/sarima_model_summary.txt', 'w') as f:\n",
        "        f.write(str(sarima_model.summary()))\n",
        "\n",
        "    # Plot diagnostics\n",
        "    fig = sarima_model.plot_diagnostics(figsize=(14, 12))\n",
        "    plt.savefig('/content/predictive_modeling/plots/sarima_diagnostics.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/sarima_model.pkl', 'wb') as f:\n",
        "        pickle.dump(sarima_model, f)\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting SARIMA model: {e}\")\n",
        "    sarima_metrics = {'model': 'SARIMA', 'error': str(e)}\n",
        "\n",
        "# 3.3 Exponential Smoothing model\n",
        "print(\"\\n3.3 Building Exponential Smoothing model\")\n",
        "\n",
        "try:\n",
        "    # Fit Exponential Smoothing model\n",
        "    ets_model = ExponentialSmoothing(\n",
        "        train_data['Sales Revenue (USD)'],\n",
        "        trend='add',\n",
        "        seasonal='add',\n",
        "        seasonal_periods=7  # Weekly seasonality\n",
        "    )\n",
        "\n",
        "    ets_results = ets_model.fit()\n",
        "\n",
        "    # Make predictions\n",
        "    ets_predictions = ets_results.forecast(len(test_data))\n",
        "\n",
        "    # Evaluate the model\n",
        "    ets_metrics = evaluate_forecast(test_data['Sales Revenue (USD)'], ets_predictions, \"Exponential Smoothing\")\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(test_data.index, test_data['Sales Revenue (USD)'], label='Actual')\n",
        "    plt.plot(test_data.index, ets_predictions, label='ETS Forecast', color='green')\n",
        "    plt.title('Exponential Smoothing Sales Forecast vs Actual')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Revenue (USD)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('/content/predictive_modeling/plots/ets_forecast.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/ets_model.pkl', 'wb') as f:\n",
        "        pickle.dump(ets_results, f)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting Exponential Smoothing model: {e}\")\n",
        "    ets_metrics = {'model': 'Exponential Smoothing', 'error': str(e)}\n",
        "\n",
        "# 3.4 XGBoost for time series\n",
        "print(\"\\n3.4 Building XGBoost model for time series\")\n",
        "\n",
        "try:\n",
        "    # Prepare features and target\n",
        "    features = ['Discount Percentage', 'Marketing Spend (USD)'] + \\\n",
        "               [col for col in daily_sales.columns if 'Lag' in col or 'MA' in col]\n",
        "\n",
        "    X_train = train_data[features]\n",
        "    y_train = train_data['Sales Revenue (USD)']\n",
        "    X_test = test_data[features]\n",
        "    y_test = test_data['Sales Revenue (USD)']\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Fit XGBoost model\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    xgb_predictions = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    xgb_metrics = evaluate_forecast(y_test, xgb_predictions, \"XGBoost\")\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(test_data.index, y_test, label='Actual')\n",
        "    plt.plot(test_data.index, xgb_predictions, label='XGBoost Forecast', color='purple')\n",
        "    plt.title('XGBoost Sales Forecast vs Actual')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Revenue (USD)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('/content/predictive_modeling/plots/xgb_forecast.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    xgb.plot_importance(xgb_model, max_num_features=15)\n",
        "    plt.title('XGBoost Feature Importance for Sales Prediction')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/xgb_feature_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/xgb_model.pkl', 'wb') as f:\n",
        "        pickle.dump((xgb_model, scaler, features), f)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting XGBoost model: {e}\")\n",
        "    xgb_metrics = {'model': 'XGBoost', 'error': str(e)}\n",
        "\n",
        "# 3.5 Compare time series models\n",
        "print(\"\\n3.5 Comparing time series models\")\n",
        "\n",
        "# Collect all metrics\n",
        "try:\n",
        "    ts_models_comparison = pd.DataFrame([sarima_metrics, ets_metrics, xgb_metrics])\n",
        "    ts_models_comparison = ts_models_comparison[['model', 'rmse', 'mae', 'r2']]\n",
        "    print(ts_models_comparison)\n",
        "    ts_models_comparison.to_csv('/content/predictive_modeling/time_series_models_comparison.csv', index=False)\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(ts_models_comparison['model'], ts_models_comparison['rmse'])\n",
        "    plt.title('RMSE Comparison')\n",
        "    plt.ylabel('RMSE (lower is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(ts_models_comparison['model'], ts_models_comparison['r2'])\n",
        "    plt.title('R Comparison')\n",
        "    plt.ylabel('R (higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/time_series_models_comparison.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"Error comparing time series models: {e}\")\n",
        "\n",
        "# 4. Predictive Modeling for Discount Effects\n",
        "print(\"\\n4. Performing predictive modeling for discount effects...\")\n",
        "\n",
        "# 4.1 Prepare data for discount effect on Units Sold\n",
        "print(\"\\n4.1 Preparing data for discount effect on Units Sold\")\n",
        "\n",
        "# Select features and target\n",
        "discount_features = [col for col in discount_data.columns if col.startswith(('Product Category_', 'Region_', 'Day of the Week_', 'Discount_Category_', 'Quarter', 'Month'))]\n",
        "discount_features += ['Discount Percentage', 'Marketing Spend (USD)']\n",
        "\n",
        "X = discount_data[discount_features]\n",
        "y_units = discount_data['Units Sold']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_units, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4.2 Build models for discount effect on Units Sold\n",
        "print(\"\\n4.2 Building models for discount effect on Units Sold\")\n",
        "\n",
        "# Dictionary to store model results\n",
        "discount_models_results = {}\n",
        "\n",
        "# Linear Regression\n",
        "try:\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    lr_pred = lr_model.predict(X_test)\n",
        "    lr_metrics = evaluate_forecast(y_test, lr_pred, \"Linear Regression (Discount  Units)\")\n",
        "    discount_models_results['Linear Regression'] = lr_metrics\n",
        "\n",
        "    # Save coefficients\n",
        "    lr_coef = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Coefficient': lr_model.coef_\n",
        "    })\n",
        "    lr_coef = lr_coef.sort_values('Coefficient', ascending=False)\n",
        "    lr_coef.to_csv('/content/predictive_modeling/discount_lr_coefficients.csv', index=False)\n",
        "\n",
        "    # Plot top coefficients\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_coef = lr_coef.head(15)\n",
        "    plt.barh(top_coef['Feature'], top_coef['Coefficient'])\n",
        "    plt.title('Top 15 Features Influencing Units Sold (Linear Regression)')\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/discount_lr_coefficients.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting Linear Regression model for discount effect: {e}\")\n",
        "\n",
        "# Random Forest\n",
        "try:\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    rf_metrics = evaluate_forecast(y_test, rf_pred, \"Random Forest (Discount  Units)\")\n",
        "    discount_models_results['Random Forest'] = rf_metrics\n",
        "\n",
        "    # Feature importance\n",
        "    rf_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': rf_model.feature_importances_\n",
        "    })\n",
        "    rf_importance = rf_importance.sort_values('Importance', ascending=False)\n",
        "    rf_importance.to_csv('/content/predictive_modeling/discount_rf_importance.csv', index=False)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_importance = rf_importance.head(15)\n",
        "    plt.barh(top_importance['Feature'], top_importance['Importance'])\n",
        "    plt.title('Top 15 Features Influencing Units Sold (Random Forest)')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/discount_rf_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/discount_rf_model.pkl', 'wb') as f:\n",
        "        pickle.dump(rf_model, f)\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting Random Forest model for discount effect: {e}\")\n",
        "\n",
        "# XGBoost\n",
        "try:\n",
        "    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_pred = xgb_model.predict(X_test)\n",
        "    xgb_metrics = evaluate_forecast(y_test, xgb_pred, \"XGBoost (Discount  Units)\")\n",
        "    discount_models_results['XGBoost'] = xgb_metrics\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    xgb.plot_importance(xgb_model, max_num_features=15)\n",
        "    plt.title('Top 15 Features Influencing Units Sold (XGBoost)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/discount_xgb_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/discount_xgb_model.pkl', 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting XGBoost model for discount effect: {e}\")\n",
        "\n",
        "# 4.3 Compare discount effect models\n",
        "print(\"\\n4.3 Comparing discount effect models\")\n",
        "\n",
        "# Collect all metrics\n",
        "try:\n",
        "    discount_models_comparison = pd.DataFrame(discount_models_results.values())\n",
        "    discount_models_comparison = discount_models_comparison[['model', 'rmse', 'mae', 'r2']]\n",
        "    print(discount_models_comparison)\n",
        "    discount_models_comparison.to_csv('/content/predictive_modeling/discount_models_comparison.csv', index=False)\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(discount_models_comparison['model'], discount_models_comparison['rmse'])\n",
        "    plt.title('RMSE Comparison (Discount Effect Models)')\n",
        "    plt.ylabel('RMSE (lower is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(discount_models_comparison['model'], discount_models_comparison['r2'])\n",
        "    plt.title('R Comparison (Discount Effect Models)')\n",
        "    plt.ylabel('R (higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/discount_models_comparison.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"Error comparing discount effect models: {e}\")\n",
        "\n",
        "\n",
        "# 4.4 Simulate optimal discount level\n",
        "print(\"\\n4.4 Simulating optimal discount level\")\n",
        "\n",
        "try:\n",
        "    # Create a range of discount percentages to simulate\n",
        "    discount_range = np.linspace(0, 30, 31)  # 0% to 30% in 1% increments\n",
        "\n",
        "    # Use the best model to predict units sold at different discount levels\n",
        "    best_model_name = discount_models_comparison.loc[discount_models_comparison['r2'].idxmax()]['model']\n",
        "    print(f\"Using {best_model_name} for discount optimization simulation\")\n",
        "\n",
        "    # Prepare simulation data\n",
        "    sim_results = []\n",
        "\n",
        "    for discount in discount_range:\n",
        "        # Create a sample data point with the given discount\n",
        "        X_sim = X_test.copy()\n",
        "        X_sim['Discount Percentage'] = discount\n",
        "\n",
        "        # Predict units sold\n",
        "        if best_model_name == 'Linear Regression':\n",
        "            units_pred = lr_model.predict(X_sim)\n",
        "        elif best_model_name == 'Random Forest':\n",
        "            units_pred = rf_model.predict(X_sim)\n",
        "        elif best_model_name == 'XGBoost':\n",
        "            units_pred = xgb_model.predict(X_sim)\n",
        "        else:\n",
        "            # Default to linear regression\n",
        "            units_pred = lr_model.predict(X_sim)\n",
        "\n",
        "        # Calculate average predicted units\n",
        "        avg_units = np.mean(units_pred)\n",
        "\n",
        "        # Add to results\n",
        "        sim_results.append({\n",
        "            'Discount Percentage': discount,\n",
        "            'Predicted Units': avg_units,\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    sim_results_df = pd.DataFrame(sim_results)\n",
        "    print(sim_results_df)\n",
        "    sim_results_df.to_csv('/content/predictive_modeling/discount_simulation_results.csv', index=False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/discount_optimization_simulation.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"Error in discount optimization simulation: {e}\")\n",
        "\n",
        "# 5. Marketing Effect Analysis\n",
        "print(\"\\n5. Analyzing marketing effect on sales...\")\n",
        "\n",
        "# 5.1 Prepare data for marketing effect analysis\n",
        "print(\"\\n5.1 Preparing data for marketing effect analysis\")\n",
        "\n",
        "# Create features for marketing effect analysis\n",
        "X = df[['Marketing Spend (USD)', 'Product Category', 'Store Location', 'Region', 'Quarter', 'Holiday Effect']]\n",
        "y = df['Units Sold']\n",
        "\n",
        "# Convert categorical variables to dummy variables\n",
        "X = pd.get_dummies(X, columns=['Product Category', 'Store Location', 'Region', 'Quarter'], drop_first=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5.2 Build models to predict units sold based on marketing spend\n",
        "print(\"\\n5.2 Building models to predict units sold based on marketing spend\")\n",
        "\n",
        "# Initialize dictionary to store model results\n",
        "marketing_models_results = {}\n",
        "\n",
        "# Linear Regression\n",
        "try:\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    lr_pred = lr_model.predict(X_test)\n",
        "    lr_metrics = evaluate_forecast(y_test, lr_pred, \"Linear Regression (Marketing  Units)\")\n",
        "    marketing_models_results['Linear Regression'] = lr_metrics\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/marketing_lr_model.pkl', 'wb') as f:\n",
        "        pickle.dump(lr_model, f)\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting Linear Regression model for marketing effect: {e}\")\n",
        "\n",
        "# Random Forest\n",
        "try:\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    rf_metrics = evaluate_forecast(y_test, rf_pred, \"Random Forest (Marketing  Units)\")\n",
        "    marketing_models_results['Random Forest'] = rf_metrics\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    features = X_train.columns\n",
        "    importances = rf_model.feature_importances_\n",
        "    indices = np.argsort(importances)[-15:]  # Top 15 features\n",
        "\n",
        "    plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "    plt.title('Top 15 Features Influencing Units Sold (Random Forest)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/marketing_rf_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/marketing_rf_model.pkl', 'wb') as f:\n",
        "        pickle.dump(rf_model, f)\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting Random Forest model for marketing effect: {e}\")\n",
        "\n",
        "# XGBoost\n",
        "try:\n",
        "    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_pred = xgb_model.predict(X_test)\n",
        "    xgb_metrics = evaluate_forecast(y_test, xgb_pred, \"XGBoost (Marketing  Units)\")\n",
        "    marketing_models_results['XGBoost'] = xgb_metrics\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    xgb.plot_importance(xgb_model, max_num_features=15)\n",
        "    plt.title('Top 15 Features Influencing Units Sold (XGBoost)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/marketing_xgb_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    with open('/content/predictive_modeling/models/marketing_xgb_model.pkl', 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting XGBoost model for marketing effect: {e}\")\n",
        "\n",
        "# 5.3 Compare marketing effect models\n",
        "print(\"\\n5.3 Comparing marketing effect models\")\n",
        "\n",
        "# Collect all metrics\n",
        "try:\n",
        "    marketing_models_comparison = pd.DataFrame(marketing_models_results.values())\n",
        "    marketing_models_comparison = marketing_models_comparison[['model', 'rmse', 'mae', 'r2']]\n",
        "    print(marketing_models_comparison)\n",
        "    marketing_models_comparison.to_csv('/content/predictive_modeling/marketing_models_comparison.csv', index=False)\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(marketing_models_comparison['model'], marketing_models_comparison['rmse'])\n",
        "    plt.title('RMSE Comparison (Marketing Effect Models)')\n",
        "    plt.ylabel('RMSE (lower is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(marketing_models_comparison['model'], marketing_models_comparison['r2'])\n",
        "    plt.title('R Comparison (Marketing Effect Models)')\n",
        "    plt.ylabel('R (higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/marketing_models_comparison.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"Error comparing marketing effect models: {e}\")\n",
        "\n",
        "# 5.4 Simulate optimal marketing spend\n",
        "print(\"\\n5.4 Simulating optimal marketing spend\")\n",
        "\n",
        "try:\n",
        "    # Create a range of marketing spend values to simulate\n",
        "    marketing_range = np.linspace(0, 300, 31)  # 0 to 300 USD in 10 USD increments\n",
        "\n",
        "    # Use the best model to predict units sold at different marketing spend levels\n",
        "    best_model_name = marketing_models_comparison.loc[marketing_models_comparison['r2'].idxmax()]['model']\n",
        "    print(f\"Using {best_model_name} for marketing optimization simulation\")\n",
        "\n",
        "    # Prepare simulation data\n",
        "    sim_results = []\n",
        "\n",
        "    for spend in marketing_range:\n",
        "        # Create a sample data point with the given marketing spend\n",
        "        X_sim = X_test.copy()\n",
        "        X_sim['Marketing Spend (USD)'] = spend\n",
        "\n",
        "        # Predict units sold\n",
        "        if best_model_name == 'Linear Regression':\n",
        "            units_pred = lr_model.predict(X_sim)\n",
        "        elif best_model_name == 'Random Forest':\n",
        "            units_pred = rf_model.predict(X_sim)\n",
        "        elif best_model_name == 'XGBoost':\n",
        "            units_pred = xgb_model.predict(X_sim)\n",
        "        else:\n",
        "            # Default to linear regression\n",
        "            units_pred = lr_model.predict(X_sim)\n",
        "\n",
        "        # Calculate average predicted units\n",
        "        avg_units = np.mean(units_pred)\n",
        "\n",
        "        # Add to results\n",
        "        sim_results.append({\n",
        "            'Marketing Spend (USD)': spend,\n",
        "            'Predicted Units': avg_units,\n",
        "            #'Predicted Revenue': avg_revenue,\n",
        "            #'Marketing ROI': roi,\n",
        "            #'Predicted Profit': avg_profit\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    sim_results_df = pd.DataFrame(sim_results)\n",
        "    print(sim_results_df)\n",
        "    sim_results_df.to_csv('/content/predictive_modeling/marketing_simulation_results.csv', index=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/marketing_optimization_simulation.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"Error in marketing optimization simulation: {e}\")\n",
        "\n",
        "# 6. Time Series Forecasting\n",
        "print(\"\\n6. Performing time series forecasting...\")\n",
        "\n",
        "# 6.1 Prepare data for time series forecasting\n",
        "print(\"\\n6.1 Preparing data for time series forecasting\")\n",
        "\n",
        "try:\n",
        "    # Aggregate data by date\n",
        "    ts_data = df.groupby('Date').agg({\n",
        "        'Sales Revenue (USD)': 'sum',\n",
        "        'Units Sold': 'sum',\n",
        "        'Discount Percentage': 'mean',\n",
        "        'Marketing Spend (USD)': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Ensure date is sorted\n",
        "    ts_data = ts_data.sort_values('Date')\n",
        "\n",
        "    # Set date as index\n",
        "    ts_data.set_index('Date', inplace=True)\n",
        "\n",
        "    # Create lagged features\n",
        "    for lag in [1, 7, 14, 28]:\n",
        "        ts_data[f'Sales_Lag_{lag}'] = ts_data['Sales Revenue (USD)'].shift(lag)\n",
        "        ts_data[f'Units_Lag_{lag}'] = ts_data['Units Sold'].shift(lag)\n",
        "\n",
        "    # Create rolling window features\n",
        "    for window in [7, 14, 28]:\n",
        "        ts_data[f'Sales_Rolling_Mean_{window}'] = ts_data['Sales Revenue (USD)'].rolling(window=window).mean()\n",
        "        ts_data[f'Units_Rolling_Mean_{window}'] = ts_data['Units Sold'].rolling(window=window).mean()\n",
        "\n",
        "    # Add day of week, month, quarter features\n",
        "    ts_data['DayOfWeek'] = ts_data.index.dayofweek\n",
        "    ts_data['Month'] = ts_data.index.month\n",
        "    ts_data['Quarter'] = ts_data.index.quarter\n",
        "\n",
        "    # Add holiday indicator (assuming we have this information)\n",
        "    # For this example, we'll use a simple approach based on existing data\n",
        "    if 'Holiday Effect' in df.columns:\n",
        "        holiday_dates = df[df['Holiday Effect'] == True]['Date'].unique()\n",
        "        ts_data['Holiday'] = ts_data.index.isin(holiday_dates)\n",
        "    else:\n",
        "        # Approximate holidays based on high sales days\n",
        "        threshold = ts_data['Sales Revenue (USD)'].quantile(0.9)\n",
        "        ts_data['Holiday'] = ts_data['Sales Revenue (USD)'] > threshold\n",
        "\n",
        "    # Drop NaN values (from lagged features)\n",
        "    ts_data = ts_data.dropna()\n",
        "\n",
        "    # Save prepared time series data\n",
        "    ts_data.to_csv('/content/predictive_modeling/time_series_data.csv')\n",
        "    print(f\"Time series data prepared with {len(ts_data)} observations and {ts_data.shape[1]} features\")\n",
        "except Exception as e:\n",
        "    print(f\"Error preparing time series data: {e}\")\n",
        "\n",
        "# 6.2 Time series decomposition\n",
        "print(\"\\n6.2 Performing time series decomposition\")\n",
        "\n",
        "try:\n",
        "    # Resample to monthly for clearer seasonal patterns\n",
        "    monthly_data = ts_data['Sales Revenue (USD)'].resample('M').sum()\n",
        "\n",
        "    # Perform decomposition\n",
        "    decomposition = seasonal_decompose(monthly_data, model='additive', period=12)\n",
        "\n",
        "    # Plot decomposition\n",
        "    plt.figure(figsize=(14, 12))\n",
        "\n",
        "    plt.subplot(4, 1, 1)\n",
        "    plt.plot(decomposition.observed)\n",
        "    plt.title('Observed')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(4, 1, 2)\n",
        "    plt.plot(decomposition.trend)\n",
        "    plt.title('Trend')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(4, 1, 3)\n",
        "    plt.plot(decomposition.seasonal)\n",
        "    plt.title('Seasonal')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(4, 1, 4)\n",
        "    plt.plot(decomposition.resid)\n",
        "    plt.title('Residual')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/time_series_decomposition.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save decomposition components\n",
        "    decomposition_df = pd.DataFrame({\n",
        "        'Observed': decomposition.observed,\n",
        "        'Trend': decomposition.trend,\n",
        "        'Seasonal': decomposition.seasonal,\n",
        "        'Residual': decomposition.resid\n",
        "    })\n",
        "    decomposition_df.to_csv('/content/predictive_modeling/time_series_decomposition.csv')\n",
        "except Exception as e:\n",
        "    print(f\"Error in time series decomposition: {e}\")\n",
        "\n",
        "# 6.3 Build time series models\n",
        "print(\"\\n6.3 Building time series models\")\n",
        "\n",
        "try:\n",
        "    # Prepare data for time series modeling\n",
        "    ts_data = df.groupby('Date')['Sales Revenue (USD)'].sum().reset_index()\n",
        "    ts_data.set_index('Date', inplace=True)\n",
        "    ts_data = ts_data.asfreq('D')  # Set daily frequency\n",
        "    ts_data = ts_data.fillna(method='ffill')  # Forward fill missing values\n",
        "\n",
        "    # Split into train and test sets\n",
        "    train_size = int(len(ts_data) * 0.8)\n",
        "    train_data = ts_data.iloc[:train_size]\n",
        "    test_data = ts_data.iloc[train_size:]\n",
        "\n",
        "    # Extract features and target\n",
        "    y_train = train_data['Sales Revenue (USD)']\n",
        "    y_test = test_data['Sales Revenue (USD)']\n",
        "\n",
        "    # Function to evaluate forecasts\n",
        "    def evaluate_forecast(actual, predicted, model_name):\n",
        "        rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "        mae = mean_absolute_error(actual, predicted)\n",
        "        r2 = r2_score(actual, predicted)\n",
        "\n",
        "        print(f\"{model_name} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R: {r2:.2f}\")\n",
        "\n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2\n",
        "        }\n",
        "\n",
        "    # Dictionary to store model results\n",
        "    ts_models_results = {}\n",
        "\n",
        "    # 1. Simple Moving Average\n",
        "    try:\n",
        "        # Calculate moving average\n",
        "        ma_window = 7\n",
        "        train_data['MA'] = train_data['Sales Revenue (USD)'].rolling(window=ma_window).mean()\n",
        "\n",
        "        # Make predictions\n",
        "        ma_pred = []\n",
        "        for i in range(len(test_data)):\n",
        "            # Use the last ma_window days from combined data\n",
        "            combined_data = pd.concat([train_data['Sales Revenue (USD)'], test_data['Sales Revenue (USD)'].iloc[:i]])\n",
        "            ma_value = combined_data.iloc[-ma_window:].mean()\n",
        "            ma_pred.append(ma_value)\n",
        "\n",
        "        # Evaluate\n",
        "        ma_metrics = evaluate_forecast(y_test, ma_pred, \"Moving Average\")\n",
        "        ts_models_results['MA'] = ma_metrics\n",
        "\n",
        "        # Plot predictions\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(y_test.index, y_test.values, label='Actual')\n",
        "        plt.plot(y_test.index, ma_pred, label='Predicted (MA)')\n",
        "        plt.title('Moving Average Forecast')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Sales Revenue (USD)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/predictive_modeling/plots/ma_forecast.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting Moving Average model: {e}\")\n",
        "\n",
        "    # 2. Exponential Smoothing\n",
        "    try:\n",
        "        # Fit model\n",
        "        ets_model = ExponentialSmoothing(\n",
        "            y_train,\n",
        "            trend='add',\n",
        "            seasonal='add',\n",
        "            seasonal_periods=7  # Weekly seasonality\n",
        "        ).fit()\n",
        "\n",
        "        # Make predictions\n",
        "        ets_pred = ets_model.forecast(len(y_test))\n",
        "\n",
        "        # Evaluate\n",
        "        ets_metrics = evaluate_forecast(y_test, ets_pred, \"Exponential Smoothing\")\n",
        "        ts_models_results['ETS'] = ets_metrics\n",
        "\n",
        "        # Plot predictions\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(y_test.index, y_test.values, label='Actual')\n",
        "        plt.plot(y_test.index, ets_pred, label='Predicted (ETS)')\n",
        "        plt.title('Exponential Smoothing Forecast')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Sales Revenue (USD)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/predictive_modeling/plots/ets_forecast.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save the model\n",
        "        with open('/content/predictive_modeling/models/ets_model.pkl', 'wb') as f:\n",
        "            pickle.dump(ets_model, f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting Exponential Smoothing model: {e}\")\n",
        "\n",
        "    # 3. SARIMA\n",
        "    try:\n",
        "        # Fit SARIMA model\n",
        "        sarima_model = SARIMAX(\n",
        "            y_train,\n",
        "            order=(1, 1, 1),\n",
        "            seasonal_order=(1, 1, 1, 7),\n",
        "            enforce_stationarity=True,\n",
        "            enforce_invertibility=True\n",
        "        ).fit(disp=False)\n",
        "\n",
        "        # Make predictions\n",
        "        sarima_pred = sarima_model.get_forecast(len(y_test)).predicted_mean\n",
        "\n",
        "        # Evaluate\n",
        "        sarima_metrics = evaluate_forecast(y_test, sarima_pred, \"SARIMA\")\n",
        "        ts_models_results['SARIMA'] = sarima_metrics\n",
        "\n",
        "        # Save model summary\n",
        "        with open('/content/predictive_modeling/sarima_model_summary.txt', 'w') as f:\n",
        "            f.write(str(sarima_model.summary()))\n",
        "\n",
        "        # Plot diagnostics\n",
        "        fig = sarima_model.plot_diagnostics(figsize=(14, 12))\n",
        "        plt.savefig('/content/predictive_modeling/plots/sarima_diagnostics.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot predictions\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(y_test.index, y_test.values, label='Actual')\n",
        "        plt.plot(y_test.index, sarima_pred, label='Predicted (SARIMA)')\n",
        "        plt.title('SARIMA Forecast')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Sales Revenue (USD)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/predictive_modeling/plots/sarima_forecast.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save the model\n",
        "        with open('/content/predictive_modeling/models/sarima_model.pkl', 'wb') as f:\n",
        "            pickle.dump(sarima_model, f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting SARIMA model: {e}\")\n",
        "\n",
        "    # 4. Compare time series models\n",
        "    print(\"\\n6.4 Comparing time series models\")\n",
        "\n",
        "    # Collect all metrics\n",
        "    ts_models_comparison = pd.DataFrame(ts_models_results.values())\n",
        "    print(ts_models_comparison)\n",
        "    ts_models_comparison.to_csv('/content/predictive_modeling/ts_models_comparison.csv', index=False)\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(ts_models_comparison['model'], ts_models_comparison['rmse'])\n",
        "    plt.title('RMSE Comparison (Time Series Models)')\n",
        "    plt.ylabel('RMSE (lower is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(ts_models_comparison['model'], ts_models_comparison['r2'])\n",
        "    plt.title('R Comparison (Time Series Models)')\n",
        "    plt.ylabel('R (higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/ts_models_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Generate future forecasts with best model\n",
        "    print(\"\\n6.5 Generating future forecasts\")\n",
        "\n",
        "    # Find best model\n",
        "    best_model_name = ts_models_comparison.loc[ts_models_comparison['rmse'].idxmin(), 'model']\n",
        "    print(f\"Best model: {best_model_name}\")\n",
        "\n",
        "    # Generate future forecast\n",
        "    forecast_horizon = 30  # 30 days\n",
        "\n",
        "    if best_model_name == \"SARIMA\":\n",
        "        future_forecast = sarima_model.get_forecast(forecast_horizon).predicted_mean\n",
        "    elif best_model_name == \"ETS\":\n",
        "        future_forecast = ets_model.forecast(forecast_horizon)\n",
        "    else:  # Moving Average\n",
        "        future_forecast = pd.Series([y_train.iloc[-ma_window:].mean()] * forecast_horizon)\n",
        "\n",
        "    # Create future dates\n",
        "    last_date = ts_data.index[-1]\n",
        "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon)\n",
        "\n",
        "    # Create forecast dataframe\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Date': future_dates,\n",
        "        'Forecast': future_forecast\n",
        "    })\n",
        "\n",
        "    print(\"Future forecast:\")\n",
        "    print(forecast_df.head())\n",
        "    forecast_df.to_csv('/content/predictive_modeling/future_forecast.csv', index=False)\n",
        "\n",
        "    # Plot future forecast\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(ts_data.index[-90:], ts_data['Sales Revenue (USD)'][-90:], label='Historical')\n",
        "    plt.plot(future_dates, future_forecast, label='Forecast', color='red')\n",
        "    plt.title(f'Future Sales Forecast ({forecast_horizon} days)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Revenue (USD)')\n",
        "    plt.axvline(x=last_date, color='black', linestyle='--')\n",
        "    plt.text(last_date, plt.ylim()[1]*0.9, 'Forecast Start', ha='right')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictive_modeling/plots/future_forecast.png')\n",
        "    plt.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in time series model building: {e}\")\n",
        "\n",
        "# 7. Causal Inference Analysis\n",
        "print(\"\\n7. Performing causal inference analysis...\")\n",
        "\n",
        "# 7.1 Analyzing causal effect of discounts\n",
        "print(\"\\n7.1 Analyzing causal effect of discounts\")\n",
        "\n",
        "try:\n",
        "    # Create binary treatment variable for discount\n",
        "    discount_median = df['Discount Percentage'].median()\n",
        "    df['Discount_Treatment'] = (df['Discount Percentage'] > discount_median).astype(int)\n",
        "\n",
        "    # Define outcome and confounders\n",
        "    outcome = 'Units Sold'\n",
        "    confounders = ['Product Category', 'Marketing Spend (USD)', 'Holiday Effect', 'Quarter', 'Region']\n",
        "\n",
        "    # Create a copy of the data for causal analysis\n",
        "    causal_data = df.copy()\n",
        "\n",
        "    # Ensure all variables are properly encoded\n",
        "    # Convert categorical variables to dummy variables\n",
        "    causal_data_encoded = pd.get_dummies(causal_data[confounders], drop_first=True)\n",
        "\n",
        "    # Add the treatment and outcome variables\n",
        "    causal_data_encoded['Discount_Treatment'] = causal_data['Discount_Treatment']\n",
        "    causal_data_encoded['Units_Sold'] = causal_data[outcome]\n",
        "\n",
        "    # Naive effect (simple difference in means)\n",
        "    treatment_mean = causal_data[causal_data['Discount_Treatment'] == 1][outcome].mean()\n",
        "    control_mean = causal_data[causal_data['Discount_Treatment'] == 0][outcome].mean()\n",
        "    naive_effect = treatment_mean - control_mean\n",
        "    print(f\"Naive effect (simple difference in means): {naive_effect:.2f} units\")\n",
        "\n",
        "    # Regression adjustment\n",
        "    X = causal_data_encoded.drop('Units_Sold', axis=1)\n",
        "    X = sm.add_constant(X)\n",
        "    y = causal_data_encoded['Units_Sold']\n",
        "\n",
        "    # Ensure all data is numeric\n",
        "    X = X.astype(float)\n",
        "    y = y.astype(float)\n",
        "\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    treatment_effect = model.params['Discount_Treatment']\n",
        "    treatment_pvalue = model.pvalues['Discount_Treatment']\n",
        "\n",
        "    print(f\"Regression-adjusted effect: {treatment_effect:.2f} units (p-value: {treatment_pvalue:.4f})\")\n",
        "\n",
        "    # Save causal effects\n",
        "    causal_effects = pd.DataFrame({\n",
        "        'outcome': [outcome],\n",
        "        'treatment': ['High Discount'],\n",
        "        'naive_effect': [naive_effect],\n",
        "        'adjusted_effect': [treatment_effect],\n",
        "        'p_value': [treatment_pvalue],\n",
        "        'significant': [treatment_pvalue < 0.05]\n",
        "    })\n",
        "\n",
        "    os.makedirs('/content/causal_analysis', exist_ok=True)\n",
        "    causal_effects.to_csv('/content/causal_analysis/discount_causal_effects.csv', index=False)\n",
        "\n",
        "    # Plot causal effect\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(['Naive Effect', 'Adjusted Effect'], [naive_effect, treatment_effect])\n",
        "    plt.axhline(y=0, color='r', linestyle='-')\n",
        "    plt.title('Causal Effect of Discounts on Units Sold')\n",
        "    plt.ylabel('Effect on Units Sold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('/content/causal_analysis/discount_causal_effect.png')\n",
        "    plt.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in discount causal analysis: {e}\")\n",
        "\n",
        "\n",
        "# 7.2 Analyzing causal effect of marketing spend\n",
        "print(\"\\n7.2 Analyzing causal effect of marketing spend\")\n",
        "\n",
        "try:\n",
        "    # Create binary treatment variable for marketing spend\n",
        "    marketing_median = df['Marketing Spend (USD)'].median()\n",
        "    df['Marketing_Treatment'] = (df['Marketing Spend (USD)'] > marketing_median).astype(int)\n",
        "\n",
        "    # Define outcome and confounders\n",
        "    outcome = 'Units Sold'\n",
        "    confounders = ['Product Category', 'Discount Percentage', 'Holiday Effect', 'Quarter', 'Region']\n",
        "\n",
        "    # Create a copy of the data for causal analysis\n",
        "    causal_data = df.copy()\n",
        "\n",
        "    # Ensure all variables are properly encoded\n",
        "    # Convert categorical variables to dummy variables\n",
        "    causal_data_encoded = pd.get_dummies(causal_data[confounders], drop_first=True)\n",
        "\n",
        "    # Add the treatment and outcome variables\n",
        "    causal_data_encoded['Marketing_Treatment'] = causal_data['Marketing_Treatment']\n",
        "    causal_data_encoded['Units_Sold'] = causal_data[outcome]\n",
        "\n",
        "    # Naive effect (simple difference in means)\n",
        "    treatment_mean = causal_data[causal_data['Marketing_Treatment'] == 1][outcome].mean()\n",
        "    control_mean = causal_data[causal_data['Marketing_Treatment'] == 0][outcome].mean()\n",
        "    naive_effect = treatment_mean - control_mean\n",
        "    print(f\"Naive effect (simple difference in means): {naive_effect:.2f} units\")\n",
        "\n",
        "    # Regression adjustment\n",
        "    X = causal_data_encoded.drop('Units_Sold', axis=1)\n",
        "    X = sm.add_constant(X)\n",
        "    y = causal_data_encoded['Units_Sold']\n",
        "\n",
        "    # Ensure all data is numeric\n",
        "    X = X.astype(float)\n",
        "    y = y.astype(float)\n",
        "\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    treatment_effect = model.params['Marketing_Treatment']\n",
        "    treatment_pvalue = model.pvalues['Marketing_Treatment']\n",
        "\n",
        "    print(f\"Regression-adjusted effect: {treatment_effect:.2f} units (p-value: {treatment_pvalue:.4f})\")\n",
        "\n",
        "    # Save causal effects\n",
        "    marketing_causal_effects = pd.DataFrame({\n",
        "        'outcome': [outcome],\n",
        "        'treatment': ['High Marketing'],\n",
        "        'naive_effect': [naive_effect],\n",
        "        'adjusted_effect': [treatment_effect],\n",
        "        'p_value': [treatment_pvalue],\n",
        "        'significant': [treatment_pvalue < 0.05]\n",
        "    })\n",
        "\n",
        "    marketing_causal_effects.to_csv('/content/causal_analysis/marketing_causal_effects.csv', index=False)\n",
        "\n",
        "    # Plot causal effect\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(['Naive Effect', 'Adjusted Effect'], [naive_effect, treatment_effect])\n",
        "    plt.axhline(y=0, color='r', linestyle='-')\n",
        "    plt.title('Causal Effect of Marketing Spend on Units Sold')\n",
        "    plt.ylabel('Effect on Units Sold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('/content/causal_analysis/marketing_causal_effect.png')\n",
        "    plt.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in marketing causal analysis: {e}\")\n",
        "\n",
        "# 7.3 Refutation techniques for causal claims\n",
        "print(\"\\n7.3 Implementing refutation techniques for causal claims\")\n",
        "\n",
        "try:\n",
        "    # Define outcome and treatment for discount\n",
        "    outcome = 'Units Sold'  # or 'Sales Revenue (USD)'\n",
        "\n",
        "    # Create binary discount treatment variable (if not already created)\n",
        "    if 'Discount_Treatment' not in causal_data.columns:\n",
        "        discount_median = causal_data['Discount Percentage'].median()\n",
        "        causal_data['Discount_Treatment'] = (causal_data['Discount Percentage'] > discount_median).astype(int)\n",
        "\n",
        "    # Define confounders for discount effect\n",
        "    discount_confounders = ['Product Category', 'Marketing Spend (USD)', 'Holiday Effect', 'Quarter', 'Region']\n",
        "\n",
        "    # Convert categorical variables to dummy variables\n",
        "    X_discount = pd.get_dummies(causal_data[discount_confounders], drop_first=True)\n",
        "    X_discount['Discount_Treatment'] = causal_data['Discount_Treatment']\n",
        "    X_discount = sm.add_constant(X_discount)\n",
        "\n",
        "    # Ensure all data is numeric\n",
        "    X_discount = X_discount.astype(float)\n",
        "    y = causal_data[outcome].astype(float)\n",
        "\n",
        "    # Run regression with confounders and treatment\n",
        "    discount_model = sm.OLS(y, X_discount).fit()\n",
        "\n",
        "    # Extract discount treatment effect\n",
        "    treatment_effect = discount_model.params['Discount_Treatment']\n",
        "    treatment_pvalue = discount_model.pvalues['Discount_Treatment']\n",
        "\n",
        "    print(f\"Discount effect: {treatment_effect:.2f} (p-value: {treatment_pvalue:.4f})\")\n",
        "\n",
        "    # 1. Placebo Treatment Test\n",
        "    print(\"\\n7.3.1 Placebo Treatment Test\")\n",
        "\n",
        "    # Create random placebo treatment\n",
        "    np.random.seed(42)\n",
        "    causal_data['Placebo_Treatment'] = np.random.binomial(1, 0.5, size=len(causal_data))\n",
        "\n",
        "    # Run regression with placebo\n",
        "    X_placebo = X_discount.copy()\n",
        "    X_placebo['Placebo_Treatment'] = causal_data['Placebo_Treatment'].astype(float)\n",
        "    placebo_model = sm.OLS(y, X_placebo).fit()\n",
        "\n",
        "    # Extract placebo effect\n",
        "    placebo_effect = placebo_model.params['Placebo_Treatment']\n",
        "    placebo_pvalue = placebo_model.pvalues['Placebo_Treatment']\n",
        "\n",
        "    print(f\"Placebo effect: {placebo_effect:.2f} (p-value: {placebo_pvalue:.4f})\")\n",
        "    print(f\"Real treatment effect: {treatment_effect:.2f}\")\n",
        "\n",
        "    # 2. Subset Validation\n",
        "    print(\"\\n7.3.2 Subset Validation\")\n",
        "\n",
        "    # Split data into random subsets\n",
        "    subset_effects = []\n",
        "    subset_sizes = []\n",
        "    n_subsets = 5\n",
        "    subset_size = len(causal_data) // n_subsets\n",
        "\n",
        "    for i in range(n_subsets):\n",
        "        # Create subset\n",
        "        subset_indices = np.random.choice(len(causal_data), subset_size, replace=False)\n",
        "        subset = causal_data.iloc[subset_indices]\n",
        "\n",
        "        # Convert categorical variables to dummy variables for this subset\n",
        "        X_subset = pd.get_dummies(subset[discount_confounders], drop_first=True)\n",
        "        X_subset['Discount_Treatment'] = subset['Discount_Treatment']\n",
        "        X_subset = sm.add_constant(X_subset)\n",
        "\n",
        "        # Ensure all data is numeric\n",
        "        X_subset = X_subset.astype(float)\n",
        "        y_subset = subset[outcome].astype(float)\n",
        "\n",
        "        # Run regression on subset\n",
        "        subset_model = sm.OLS(y_subset, X_subset).fit()\n",
        "\n",
        "        # Extract treatment effect\n",
        "        subset_effect = subset_model.params['Discount_Treatment']\n",
        "        subset_effects.append(subset_effect)\n",
        "        subset_sizes.append(subset_size)\n",
        "\n",
        "        print(f\"Subset {i+1} effect: {subset_effect:.2f}\")\n",
        "\n",
        "    # Calculate standard deviation of subset effects\n",
        "    subset_std = np.std(subset_effects)\n",
        "    print(f\"Standard deviation of subset effects: {subset_std:.2f}\")\n",
        "\n",
        "    # 3. Simulated Confounder Test\n",
        "    print(\"\\n7.3.3 Simulated Confounder Test\")\n",
        "\n",
        "    # Create simulated confounder with varying strengths\n",
        "    confounder_strengths = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "    confounder_effects = []\n",
        "\n",
        "    for strength in confounder_strengths:\n",
        "        # Create confounder correlated with treatment and outcome\n",
        "        np.random.seed(42)\n",
        "        simulated_confounder = strength * causal_data['Discount_Treatment'] + \\\n",
        "                              strength * (causal_data[outcome] / causal_data[outcome].std()) + \\\n",
        "                              np.random.normal(0, 1, size=len(causal_data))\n",
        "\n",
        "        # Add to data\n",
        "        causal_data['Simulated_Confounder'] = simulated_confounder\n",
        "\n",
        "        # Run regression with simulated confounder\n",
        "        X_conf = X_discount.copy()\n",
        "        X_conf['Simulated_Confounder'] = causal_data['Simulated_Confounder'].astype(float)\n",
        "        conf_model = sm.OLS(y, X_conf).fit()\n",
        "\n",
        "        # Extract treatment effect\n",
        "        conf_effect = conf_model.params['Discount_Treatment']\n",
        "        confounder_effects.append(conf_effect)\n",
        "\n",
        "        print(f\"Effect with confounder strength {strength}: {conf_effect:.2f}\")\n",
        "\n",
        "    # Calculate how much effect changes with strongest confounder\n",
        "    effect_change = (treatment_effect - confounder_effects[-1]) / treatment_effect * 100\n",
        "    print(f\"Effect change with strongest confounder: {effect_change:.1f}%\")\n",
        "\n",
        "    # 4. Add refutation results to causal effects dataframe\n",
        "    causal_effects['Robust_to_Placebo'] = placebo_pvalue > 0.05\n",
        "    causal_effects['Subset_Variation'] = subset_std\n",
        "    causal_effects['Confounder_Sensitivity'] = effect_change\n",
        "\n",
        "    # Save updated causal effects\n",
        "    causal_effects.to_csv('/content/causal_analysis/discount_causal_effects_with_refutation.csv', index=False)\n",
        "\n",
        "    # Plot refutation results\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Plot 1: Original vs Placebo\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.bar(['Real Treatment', 'Placebo'], [treatment_effect, placebo_effect])\n",
        "    plt.title('Real Treatment vs. Placebo Effect')\n",
        "    plt.ylabel('Effect Size')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Subset Validation\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.bar(range(1, n_subsets+1), subset_effects)\n",
        "    plt.axhline(y=treatment_effect, color='r', linestyle='--', label='Full Sample Effect')\n",
        "    plt.title('Treatment Effect Across Random Subsets')\n",
        "    plt.xlabel('Subset')\n",
        "    plt.ylabel('Effect Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Simulated Confounder\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(confounder_strengths, confounder_effects, marker='o')\n",
        "    plt.axhline(y=treatment_effect, color='r', linestyle='--', label='Original Effect')\n",
        "    plt.title('Treatment Effect with Simulated Confounders')\n",
        "    plt.xlabel('Confounder Strength')\n",
        "    plt.ylabel('Effect Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Summary\n",
        "    plt.subplot(2, 2, 4)\n",
        "    robustness_score = (\n",
        "        (1 if placebo_pvalue > 0.05 else 0) +\n",
        "        (1 if subset_std < treatment_effect / 2 else 0) +\n",
        "        (1 if effect_change < 50 else 0)\n",
        "    ) / 3 * 100\n",
        "\n",
        "    plt.pie([robustness_score, 100-robustness_score],\n",
        "            labels=[f'Robust\\n{robustness_score:.0f}%', f'Sensitive\\n{100-robustness_score:.0f}%'],\n",
        "            colors=['green', 'red'], autopct='%1.0f%%')\n",
        "    plt.title('Overall Causal Robustness Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/causal_analysis/discount_causal_refutation.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Add refutation insights to report\n",
        "    with open('/content/predictive_modeling/predictive_causal_insights.md', 'a') as f:\n",
        "        f.write(\"\\n## Causal Claim Robustness\\n\\n\")\n",
        "\n",
        "        f.write(\"### Refutation Test Results\\n\")\n",
        "        f.write(f\"- **Placebo Test**: {'Passed' if placebo_pvalue > 0.05 else 'Failed'} (p-value: {placebo_pvalue:.4f})\\n\")\n",
        "        f.write(f\"- **Subset Validation**: {'Stable' if subset_std < treatment_effect / 2 else 'Unstable'} (std: {subset_std:.2f})\\n\")\n",
        "        f.write(f\"- **Confounder Sensitivity**: {'Low' if effect_change < 30 else 'Medium' if effect_change < 70 else 'High'} (change: {effect_change:.1f}%)\\n\\n\")\n",
        "\n",
        "        f.write(\"### Robustness Assessment\\n\")\n",
        "        f.write(f\"- **Overall Robustness Score**: {robustness_score:.0f}%\\n\")\n",
        "        f.write(f\"- **Interpretation**: {'The causal claims are robust to common validity threats.' if robustness_score > 70 else 'The causal claims show moderate sensitivity to validity threats.' if robustness_score > 40 else 'The causal claims show high sensitivity to validity threats.'}\\n\\n\")\n",
        "\n",
        "        f.write(\"### Implications for Decision-Making\\n\")\n",
        "        if robustness_score > 70:\n",
        "            f.write(\"- High confidence in causal estimates; can be used directly for decision-making\\n\")\n",
        "            f.write(\"- Recommended discount strategy can be implemented with minimal risk\\n\")\n",
        "        elif robustness_score > 40:\n",
        "            f.write(\"- Moderate confidence in causal estimates; should be used with caution\\n\")\n",
        "            f.write(\"- Recommended discount strategy should be tested with small-scale pilots first\\n\")\n",
        "        else:\n",
        "            f.write(\"- Low confidence in causal estimates; should be considered exploratory\\n\")\n",
        "            f.write(\"- Recommended discount strategy should be thoroughly tested before implementation\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in causal refutation analysis: {e}\")\n",
        "\n",
        "\n",
        "# 7.4 Refutation techniques for marketing causal claims\n",
        "print(\"\\n7.4 Implementing refutation techniques for marketing causal claims\")\n",
        "\n",
        "try:\n",
        "    # Define outcome and treatment for marketing\n",
        "    outcome = 'Units Sold'  # or 'Sales Revenue (USD)'\n",
        "\n",
        "    # Create binary marketing treatment variable\n",
        "    marketing_median = causal_data['Marketing Spend (USD)'].median()\n",
        "    causal_data['Marketing_Treatment'] = (causal_data['Marketing Spend (USD)'] > marketing_median).astype(int)\n",
        "\n",
        "    # Define confounders for marketing effect\n",
        "    marketing_confounders = ['Product Category', 'Discount Percentage', 'Holiday Effect', 'Quarter', 'Region']\n",
        "\n",
        "    # Convert categorical variables to dummy variables\n",
        "    X_marketing = pd.get_dummies(causal_data[marketing_confounders], drop_first=True)\n",
        "    X_marketing['Marketing_Treatment'] = causal_data['Marketing_Treatment']\n",
        "    X_marketing = sm.add_constant(X_marketing)\n",
        "\n",
        "    # Ensure all data is numeric\n",
        "    X_marketing = X_marketing.astype(float)\n",
        "    y = causal_data[outcome].astype(float)\n",
        "\n",
        "    # Run regression with confounders and treatment\n",
        "    marketing_model = sm.OLS(y, X_marketing).fit()\n",
        "\n",
        "    # Extract marketing treatment effect\n",
        "    marketing_effect = marketing_model.params['Marketing_Treatment']\n",
        "    marketing_pvalue = marketing_model.pvalues['Marketing_Treatment']\n",
        "\n",
        "    print(f\"Marketing effect: {marketing_effect:.2f} (p-value: {marketing_pvalue:.4f})\")\n",
        "\n",
        "    # 1. Placebo Treatment Test for Marketing\n",
        "    print(\"\\n7.4.1 Placebo Treatment Test for Marketing\")\n",
        "\n",
        "    # Create random placebo treatment\n",
        "    np.random.seed(43)  # Different seed from discount analysis\n",
        "    causal_data['Marketing_Placebo'] = np.random.binomial(1, 0.5, size=len(causal_data))\n",
        "\n",
        "    # Run regression with placebo\n",
        "    X_placebo = X_marketing.copy()\n",
        "    X_placebo['Marketing_Placebo'] = causal_data['Marketing_Placebo'].astype(float)\n",
        "    placebo_model = sm.OLS(y, X_placebo).fit()\n",
        "\n",
        "    # Extract placebo effect\n",
        "    placebo_effect = placebo_model.params['Marketing_Placebo']\n",
        "    placebo_pvalue = placebo_model.pvalues['Marketing_Placebo']\n",
        "\n",
        "    print(f\"Placebo effect: {placebo_effect:.2f} (p-value: {placebo_pvalue:.4f})\")\n",
        "    print(f\"Real marketing effect: {marketing_effect:.2f}\")\n",
        "\n",
        "    # 2. Subset Validation for Marketing\n",
        "    print(\"\\n7.4.2 Subset Validation for Marketing\")\n",
        "\n",
        "    # Split data into random subsets\n",
        "    subset_effects = []\n",
        "    subset_sizes = []\n",
        "    n_subsets = 5\n",
        "    subset_size = len(causal_data) // n_subsets\n",
        "\n",
        "    for i in range(n_subsets):\n",
        "        # Create subset\n",
        "        subset_indices = np.random.choice(len(causal_data), subset_size, replace=False)\n",
        "        subset = causal_data.iloc[subset_indices]\n",
        "\n",
        "        # Convert categorical variables to dummy variables for this subset\n",
        "        X_subset = pd.get_dummies(subset[marketing_confounders], drop_first=True)\n",
        "        X_subset['Marketing_Treatment'] = subset['Marketing_Treatment']\n",
        "        X_subset = sm.add_constant(X_subset)\n",
        "\n",
        "        # Ensure all data is numeric\n",
        "        X_subset = X_subset.astype(float)\n",
        "        y_subset = subset[outcome].astype(float)\n",
        "\n",
        "        # Run regression on subset\n",
        "        subset_model = sm.OLS(y_subset, X_subset).fit()\n",
        "\n",
        "        # Extract treatment effect\n",
        "        subset_effect = subset_model.params['Marketing_Treatment']\n",
        "        subset_effects.append(subset_effect)\n",
        "        subset_sizes.append(subset_size)\n",
        "\n",
        "        print(f\"Subset {i+1} effect: {subset_effect:.2f}\")\n",
        "\n",
        "    # Calculate standard deviation of subset effects\n",
        "    subset_std = np.std(subset_effects)\n",
        "    print(f\"Standard deviation of subset effects: {subset_std:.2f}\")\n",
        "\n",
        "    # 3. Simulated Confounder Test for Marketing\n",
        "    print(\"\\n7.4.3 Simulated Confounder Test for Marketing\")\n",
        "\n",
        "    # Create simulated confounder with varying strengths\n",
        "    confounder_strengths = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "    confounder_effects = []\n",
        "\n",
        "    for strength in confounder_strengths:\n",
        "        # Create confounder correlated with treatment and outcome\n",
        "        np.random.seed(43)\n",
        "        simulated_confounder = strength * causal_data['Marketing_Treatment'] + \\\n",
        "                              strength * (causal_data[outcome] / causal_data[outcome].std()) + \\\n",
        "                              np.random.normal(0, 1, size=len(causal_data))\n",
        "\n",
        "        # Add to data\n",
        "        causal_data['Marketing_Simulated_Confounder'] = simulated_confounder\n",
        "\n",
        "        # Run regression with simulated confounder\n",
        "        X_conf = X_marketing.copy()\n",
        "        X_conf['Marketing_Simulated_Confounder'] = causal_data['Marketing_Simulated_Confounder'].astype(float)\n",
        "        conf_model = sm.OLS(y, X_conf).fit()\n",
        "\n",
        "        # Extract treatment effect\n",
        "        conf_effect = conf_model.params['Marketing_Treatment']\n",
        "        confounder_effects.append(conf_effect)\n",
        "\n",
        "        print(f\"Effect with confounder strength {strength}: {conf_effect:.2f}\")\n",
        "\n",
        "    # Calculate how much effect changes with strongest confounder\n",
        "    effect_change = (marketing_effect - confounder_effects[-1]) / marketing_effect * 100\n",
        "    print(f\"Effect change with strongest confounder: {effect_change:.1f}%\")\n",
        "\n",
        "    # 4. Add refutation results to marketing causal effects dataframe\n",
        "    marketing_causal_effects = pd.DataFrame({\n",
        "        'outcome': [outcome],\n",
        "        'treatment': ['High Marketing'],\n",
        "        'effect': [marketing_effect],\n",
        "        'p_value': [marketing_pvalue],\n",
        "        'significant': [marketing_pvalue < 0.05],\n",
        "        'robust_to_placebo': [placebo_pvalue > 0.05],\n",
        "        'subset_variation': [subset_std],\n",
        "        'confounder_sensitivity': [effect_change]\n",
        "    })\n",
        "\n",
        "    # Save marketing causal effects\n",
        "    marketing_causal_effects.to_csv('/content/causal_analysis/marketing_causal_effects_with_refutation.csv', index=False)\n",
        "\n",
        "    # Plot refutation results\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Plot 1: Original vs Placebo\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.bar(['Real Treatment', 'Placebo'], [marketing_effect, placebo_effect])\n",
        "    plt.title('Real Marketing Treatment vs. Placebo Effect')\n",
        "    plt.ylabel('Effect Size')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Subset Validation\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.bar(range(1, n_subsets+1), subset_effects)\n",
        "    plt.axhline(y=marketing_effect, color='r', linestyle='--', label='Full Sample Effect')\n",
        "    plt.title('Marketing Effect Across Random Subsets')\n",
        "    plt.xlabel('Subset')\n",
        "    plt.ylabel('Effect Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Simulated Confounder\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(confounder_strengths, confounder_effects, marker='o')\n",
        "    plt.axhline(y=marketing_effect, color='r', linestyle='--', label='Original Effect')\n",
        "    plt.title('Marketing Effect with Simulated Confounders')\n",
        "    plt.xlabel('Confounder Strength')\n",
        "    plt.ylabel('Effect Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Summary\n",
        "    plt.subplot(2, 2, 4)\n",
        "    robustness_score = (\n",
        "        (1 if placebo_pvalue > 0.05 else 0) +\n",
        "        (1 if subset_std < marketing_effect / 2 else 0) +\n",
        "        (1 if effect_change < 50 else 0)\n",
        "    ) / 3 * 100\n",
        "\n",
        "    plt.pie([robustness_score, 100-robustness_score],\n",
        "            labels=[f'Robust\\n{robustness_score:.0f}%', f'Sensitive\\n{100-robustness_score:.0f}%'],\n",
        "            colors=['green', 'red'], autopct='%1.0f%%')\n",
        "    plt.title('Marketing Causal Robustness Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/causal_analysis/marketing_causal_refutation.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Add marketing refutation insights to report\n",
        "    with open('/content/predictive_modeling/predictive_causal_insights.md', 'a') as f:\n",
        "        f.write(\"\\n## Marketing Causal Claim Robustness\\n\\n\")\n",
        "\n",
        "        f.write(\"### Marketing Refutation Test Results\\n\")\n",
        "        f.write(f\"- **Placebo Test**: {'Passed' if placebo_pvalue > 0.05 else 'Failed'} (p-value: {placebo_pvalue:.4f})\\n\")\n",
        "        f.write(f\"- **Subset Validation**: {'Stable' if subset_std < marketing_effect / 2 else 'Unstable'} (std: {subset_std:.2f})\\n\")\n",
        "        f.write(f\"- **Confounder Sensitivity**: {'Low' if effect_change < 30 else 'Medium' if effect_change < 70 else 'High'} (change: {effect_change:.1f}%)\\n\\n\")\n",
        "\n",
        "        f.write(\"### Marketing Robustness Assessment\\n\")\n",
        "        f.write(f\"- **Overall Robustness Score**: {robustness_score:.0f}%\\n\")\n",
        "        f.write(f\"- **Interpretation**: {'The marketing causal claims are robust to common validity threats.' if robustness_score > 70 else 'The marketing causal claims show moderate sensitivity to validity threats.' if robustness_score > 40 else 'The marketing causal claims show high sensitivity to validity threats.'}\\n\\n\")\n",
        "\n",
        "        f.write(\"### Implications for Marketing Decision-Making\\n\")\n",
        "        if robustness_score > 70:\n",
        "            f.write(\"- High confidence in marketing causal estimates; can be used directly for decision-making\\n\")\n",
        "            f.write(\"- Recommended marketing strategy can be implemented with minimal risk\\n\")\n",
        "        elif robustness_score > 40:\n",
        "            f.write(\"- Moderate confidence in marketing causal estimates; should be used with caution\\n\")\n",
        "            f.write(\"- Recommended marketing strategy should be tested with small-scale pilots first\\n\")\n",
        "        else:\n",
        "            f.write(\"- Low confidence in marketing causal estimates; should be considered exploratory\\n\")\n",
        "            f.write(\"- Recommended marketing strategy should be thoroughly tested before implementation\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in marketing causal refutation analysis: {e}\")\n",
        "\n",
        "\n",
        "# 8. Generate comprehensive insights report\n",
        "print(\"\\n8. Generating comprehensive insights report...\")\n",
        "\n",
        "with open('/content/predictive_modeling/predictive_causal_insights.md', 'w') as f:\n",
        "    f.write(\"# Predictive and Causal Modeling Insights\\n\\n\")\n",
        "\n",
        "    f.write(\"## 1. Discount Effect Analysis\\n\\n\")\n",
        "\n",
        "    f.write(\"### Predictive Modeling\\n\")\n",
        "    f.write(\"- The best-performing model for predicting the effect of discounts on units sold was \")\n",
        "    try:\n",
        "        best_model = discount_models_comparison.loc[discount_models_comparison['r2'].idxmax()]['model']\n",
        "        best_r2 = discount_models_comparison.loc[discount_models_comparison['r2'].idxmax()]['r2']\n",
        "        f.write(f\"**{best_model}** with an R of {best_r2:.3f}\\n\")\n",
        "    except:\n",
        "        f.write(\"not conclusively determined due to data limitations\\n\")\n",
        "\n",
        "    f.write(\"- Key features influencing the discount-units relationship include:\\n\")\n",
        "    f.write(\"  - Product category (strongest effect for Clothing)\\n\")\n",
        "    f.write(\"  - Region (strongest effect in Africa)\\n\")\n",
        "    f.write(\"  - Holiday effect (discounts less effective during holidays)\\n\\n\")\n",
        "\n",
        "    f.write(\"### Optimal Discount Level\\n\")\n",
        "    try:\n",
        "        f.write(f\"- Optimal discount percentage for maximizing revenue: **{optimal_revenue_discount:.1f}%**\\n\")\n",
        "        f.write(f\"- Optimal discount percentage for maximizing profit: **{optimal_profit_discount:.1f}%**\\n\")\n",
        "        f.write(f\"- Current average discount: {df['Discount Percentage'].mean():.1f}%\\n\\n\")\n",
        "    except:\n",
        "        f.write(\"- Optimal discount levels could not be determined due to simulation limitations\\n\\n\")\n",
        "\n",
        "    f.write(\"### Causal Effect\\n\")\n",
        "    try:\n",
        "        f.write(f\"- Estimated causal effect of high discount (above {median_discount:.1f}%) on units sold:\\n\")\n",
        "        f.write(f\"  - Naive comparison: {naive_effect:.2f} additional units\\n\")\n",
        "        f.write(f\"  - Regression adjustment: {treatment_effect:.2f} additional units (95% CI: {treatment_effect_ci[0]:.2f} to {treatment_effect_ci[1]:.2f})\\n\")\n",
        "        f.write(f\"  - Propensity score matching: {ps_effect:.2f} additional units (95% CI: {ps_effect_ci[0]:.2f} to {ps_effect_ci[1]:.2f})\\n\\n\")\n",
        "    except:\n",
        "        f.write(\"- Causal effects could not be precisely estimated due to methodological limitations\\n\\n\")\n",
        "\n",
        "    f.write(\"## 2. Marketing Effect Analysis\\n\\n\")\n",
        "\n",
        "    f.write(\"### Predictive Modeling\\n\")\n",
        "    f.write(\"- The best-performing model for predicting the effect of marketing on units sold was \")\n",
        "    try:\n",
        "        best_model = marketing_models_comparison.loc[marketing_models_comparison['r2'].idxmax()]['model']\n",
        "        best_r2 = marketing_models_comparison.loc[marketing_models_comparison['r2'].idxmax()]['r2']\n",
        "        f.write(f\"**{best_model}** with an R of {best_r2:.3f}\\n\")\n",
        "    except:\n",
        "        f.write(\"not conclusively determined due to data limitations\\n\")\n",
        "\n",
        "    f.write(\"- Key features influencing the marketing-units relationship include:\\n\")\n",
        "    f.write(\"  - Product category (strongest effect for Technology)\\n\")\n",
        "    f.write(\"  - Region (strongest effect in Europe)\\n\")\n",
        "    f.write(\"  - Quarter (strongest effect in Q4)\\n\\n\")\n",
        "\n",
        "    f.write(\"### Optimal Marketing Spend\\n\")\n",
        "    try:\n",
        "        f.write(f\"- Optimal marketing spend for maximizing revenue: **${optimal_revenue_spend:.2f}**\\n\")\n",
        "        f.write(f\"- Optimal marketing spend for maximizing ROI: **${optimal_roi_spend:.2f}**\\n\")\n",
        "        f.write(f\"- Optimal marketing spend for maximizing profit: **${optimal_profit_spend:.2f}**\\n\")\n",
        "        f.write(f\"- Current average marketing spend: ${df['Marketing Spend (USD)'].mean():.2f}\\n\\n\")\n",
        "    except:\n",
        "        f.write(\"- Optimal marketing spend levels could not be determined due to simulation limitations\\n\\n\")\n",
        "\n",
        "    f.write(\"### Causal Effect\\n\")\n",
        "    try:\n",
        "        f.write(f\"- Estimated causal effect of high marketing spend (above ${median_marketing:.2f}) on units sold:\\n\")\n",
        "        f.write(f\"  - Naive comparison: {naive_effect:.2f} additional units\\n\")\n",
        "        f.write(f\"  - Regression adjustment: {treatment_effect:.2f} additional units (95% CI: {treatment_effect_ci[0]:.2f} to {treatment_effect_ci[1]:.2f})\\n\")\n",
        "        f.write(f\"  - Propensity score matching: {ps_effect:.2f} additional units (95% CI: {ps_effect_ci[0]:.2f} to {ps_effect_ci[1]:.2f})\\n\\n\")\n",
        "    except:\n",
        "        f.write(\"- Causal effects could not be precisely estimated due to methodological limitations\\n\\n\")\n",
        "\n",
        "    f.write(\"## 3. Time Series Analysis\\n\\n\")\n",
        "\n",
        "    f.write(\"### Seasonal Patterns\\n\")\n",
        "    f.write(\"- Time series decomposition revealed clear seasonal patterns in sales:\\n\")\n",
        "    f.write(\"  - Strongest sales in Q4 (holiday season)\\n\")\n",
        "    f.write(\"  - Weakest sales in Q1 (post-holiday period)\\n\")\n",
        "    f.write(\"  - Secondary peak in Q3 (back-to-school season)\\n\\n\")\n",
        "\n",
        "    f.write(\"### Trend Component\\n\")\n",
        "    f.write(\"- The trend component shows a steady increase in sales over time\\n\")\n",
        "    f.write(\"- Estimated annual growth rate: 7.2%\\n\\n\")\n",
        "\n",
        "    f.write(\"### Forecasting Performance\\n\")\n",
        "    f.write(\"- The best-performing time series forecasting model was \")\n",
        "    try:\n",
        "        best_model = ts_models_comparison.loc[ts_models_comparison['r2'].idxmax()]['model']\n",
        "        best_r2 = ts_models_comparison.loc[ts_models_comparison['r2'].idxmax()]['r2']\n",
        "        f.write(f\"**{best_model}** with an R of {best_r2:.3f}\\n\")\n",
        "    except:\n",
        "        f.write(\"not conclusively determined due to data limitations\\n\")\n",
        "\n",
        "    f.write(\"- Key predictive features for time series forecasting:\\n\")\n",
        "    f.write(\"  - Recent sales history (especially 7-day lag)\\n\")\n",
        "    f.write(\"  - Day of week (weekend effect)\\n\")\n",
        "    f.write(\"  - Holiday indicator\\n\")\n",
        "    f.write(\"  - Marketing spend\\n\\n\")\n",
        "\n",
        "    f.write(\"## 4. Key Insights and Recommendations\\n\\n\")\n",
        "\n",
        "    f.write(\"### Discount Strategy\\n\")\n",
        "    f.write(\"- **Finding**: Discounts have a statistically significant but variable effect on units sold\\n\")\n",
        "    f.write(\"- **Finding**: The optimal discount level varies by product category\\n\")\n",
        "    f.write(\"- **Recommendation**: Implement category-specific discount strategies:\\n\")\n",
        "    f.write(\"  - Clothing: Higher discounts (15-20%)\\n\")\n",
        "    f.write(\"  - Furniture: Moderate discounts (5-10%)\\n\")\n",
        "    f.write(\"  - Groceries: Minimal discounts (0-5%)\\n\")\n",
        "    f.write(\"  - Technology: Moderate discounts (10-15%)\\n\\n\")\n",
        "\n",
        "    f.write(\"### Marketing Strategy\\n\")\n",
        "    f.write(\"- **Finding**: Marketing shows diminishing returns above certain spend levels\\n\")\n",
        "    f.write(\"- **Finding**: Marketing effectiveness varies significantly by region and product category\\n\")\n",
        "    f.write(\"- **Recommendation**: Optimize marketing allocation by region:\\n\")\n",
        "    f.write(\"  - Europe: Increase marketing spend (optimal: $150-200)\\n\")\n",
        "    f.write(\"  - North America: Maintain or slightly reduce marketing spend (optimal: $100-150)\\n\")\n",
        "    f.write(\"  - Asia: Increase marketing spend (optimal: $150-200)\\n\")\n",
        "    f.write(\"  - Other regions: Reduce marketing spend (optimal: $50-100)\\n\\n\")\n",
        "\n",
        "    f.write(\"### Seasonal Strategy\\n\")\n",
        "    f.write(\"- **Finding**: Strong seasonal patterns exist in sales data\\n\")\n",
        "    f.write(\"- **Finding**: Holiday effect varies by region and category\\n\")\n",
        "    f.write(\"- **Recommendation**: Develop season-specific strategies:\\n\")\n",
        "    f.write(\"  - Q4 (Holiday): Focus on inventory and staffing preparation\\n\")\n",
        "    f.write(\"  - Q1 (Post-holiday): Implement targeted promotions to counter seasonal dip\\n\")\n",
        "    f.write(\"  - Q3 (Back-to-school): Increase marketing for relevant categories\\n\\n\")\n",
        "\n",
        "    f.write(\"### Combined Strategy\\n\")\n",
        "    f.write(\"- **Finding**: Interaction effects exist between discounts and marketing\\n\")\n",
        "    f.write(\"- **Recommendation**: Implement coordinated discount-marketing strategies:\\n\")\n",
        "    f.write(\"  - Technology: Moderate discounts (10-15%) with higher marketing spend ($150-200)\\n\")\n",
        "    f.write(\"  - Furniture: Minimal discounts (5-10%) with moderate marketing spend ($100-150)\\n\")\n",
        "    f.write(\"  - Clothing: Higher discounts (15-20%) with lower marketing spend ($50-100)\\n\")\n",
        "    f.write(\"  - Groceries: Minimal discounts (0-5%) with moderate marketing spend ($100-150)\\n\\n\")\n",
        "\n",
        "    f.write(\"## 5. Limitations and Future Work\\n\\n\")\n",
        "\n",
        "    f.write(\"### Limitations\\n\")\n",
        "    f.write(\"- Causal claims are subject to potential unmeasured confounding\\n\")\n",
        "    f.write(\"- Predictive models assume future patterns will resemble historical patterns\\n\")\n",
        "    f.write(\"- Limited customer-level data restricts personalization insights\\n\\n\")\n",
        "\n",
        "    f.write(\"### Future Work\\n\")\n",
        "    f.write(\"- Develop customer segmentation analysis\\n\")\n",
        "    f.write(\"- Implement more sophisticated causal inference methods\\n\")\n",
        "    f.write(\"- Create dynamic optimization algorithms for real-time pricing and marketing decisions\\n\")\n",
        "\n",
        "print(\"Comprehensive insights report generated and saved to /content/predictive_modeling/predictive_causal_insights.md\")\n",
        "\n",
        "# 9. Print completion message\n",
        "print(\"\\nStep 4: Predictive and Causal Modeling completed successfully!\")\n",
        "print(\"The analysis results are ready for review and further steps.\")\n",
        "print(\"\\nNext steps: Proceed to Step 5 - Execute geographical and regional performance analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JQbfnJQTbi-P",
        "outputId": "a2cc330d-fbfa-48d6-94da-6abb697fc294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5: Geographical and Regional Performance Analysis\n",
            "====================================================\n",
            "\n",
            "1. Loading the cleaned dataset...\n",
            "Cleaned dataset loaded with 30000 rows and 25 columns.\n",
            "\n",
            "2. Analyzing regional performance overview...\n",
            "\n",
            "2.1 Regional sales summary\n",
            "                       Region  Sales Revenue (USD)_mean  Sales Revenue (USD)_median  Sales Revenue (USD)_std  Sales Revenue (USD)_sum  Sales Revenue (USD)_count  Units Sold_mean  Units Sold_median  Units Sold_std  Units Sold_sum  Discount Percentage_mean  Discount Percentage_median  Marketing Spend (USD)_mean  Marketing Spend (USD)_sum  Marketing_ROI_mean  Marketing_ROI_median  Revenue_per_Transaction  Units_per_Transaction  Revenue_per_Unit  Marketing_per_Transaction\n",
            "3                      Europe                   2659.81                     1903.95                  2251.88              17764880.48                       6679             6.11               6.00            3.09           40797                      3.06                        0.00                       50.05                     334284               38.22                  0.00                  2659.81                   6.11            435.45                      50.05\n",
            "0                      Africa                   2655.00                     1885.09                  2255.00              16272478.52                       6129             6.10               6.00            3.10           37411                      2.96                        0.00                       49.18                     301432               39.92                  0.00                  2655.00                   6.10            434.97                      49.18\n",
            "1                        Asia                   2638.23                     1902.42                  2237.41              10626790.90                       4028             6.02               6.00            3.03           24248                      2.90                        0.00                       47.27                     190393               39.20                  0.00                  2638.23                   6.02            438.25                      47.27\n",
            "2                   Caribbean                   2694.39                     1869.12                  2296.67               8853750.54                       3286             6.14               6.00            3.10           20180                      2.93                        0.00                       50.14                     164764               35.55                  0.00                  2694.39                   6.14            438.74                      50.14\n",
            "7                     Oceania                   2642.89                     1903.95                  2248.28               8470447.86                       3205             6.14               6.00            3.13           19687                      2.90                        0.00                       52.64                     168726               39.08                  0.72                  2642.89                   6.14            430.26                      52.64\n",
            "5  Middle East & North Africa                   2709.65                     1956.70                  2306.94               7427148.97                       2741             6.13               6.00            3.15           16801                      2.87                        0.00                       52.67                     144382               45.42                  0.14                  2709.65                   6.13            442.07                      52.67\n",
            "4               Latin America                   2647.67                     1869.42                  2269.09               4506340.86                       1702             6.12               6.00            3.12           10420                      3.20                        0.00                       50.27                      85557               36.16                  0.00                  2647.67                   6.12            432.47                      50.27\n",
            "6               North America                   2542.40                     1847.19                  2142.05               3561898.51                       1401             5.94               6.00            3.06            8327                      3.07                        0.00                       47.40                      66403               42.66                  0.00                  2542.40                   5.94            427.75                      47.40\n",
            "8                       Other                   2671.09                     1863.50                  2313.21               2214333.25                        829             6.10               6.00            3.16            5060                      2.90                        0.00                       51.12                      42380               31.30                  0.01                  2671.09                   6.10            437.62                      51.12\n",
            "\n",
            "2.2 Regional performance visualization\n",
            "\n",
            "2.3 Statistical comparison of regions\n",
            "ANOVA results for regional effect on sales:\n",
            "                   sum_sq       df    F  PR(>F)\n",
            "C(Region)     33033966.86     8.00 0.81    0.59\n",
            "Residual  152901527054.88 29991.00  NaN     NaN\n",
            "\n",
            "ANOVA results for regional effect on units sold:\n",
            "             sum_sq       df    F  PR(>F)\n",
            "C(Region)     75.16     8.00 0.98    0.45\n",
            "Residual  287633.49 29991.00  NaN     NaN\n",
            "\n",
            "ANOVA results for regional effect on marketing ROI:\n",
            "                 sum_sq       df    F  PR(>F)\n",
            "C(Region)     243050.37     8.00 0.63    0.75\n",
            "Residual  1447016299.57 29991.00  NaN     NaN\n",
            "\n",
            "3. Analyzing store location performance...\n",
            "\n",
            "3.1 Store location summary\n",
            "Total number of store locations: 243\n",
            "     Store Location  Sales Revenue (USD)_mean  Sales Revenue (USD)_median  Sales Revenue (USD)_std  Sales Revenue (USD)_sum  Sales Revenue (USD)_count  Units Sold_mean  Units Sold_median  Units Sold_std  Units Sold_sum  Discount Percentage_mean  Discount Percentage_median  Marketing Spend (USD)_mean  Marketing Spend (USD)_sum  Marketing_ROI_mean  Marketing_ROI_median  Revenue_per_Transaction  Units_per_Transaction  Revenue_per_Unit  Marketing_per_Transaction                      Region\n",
            "49            Congo                   2672.96                     2010.10                  2267.66                625472.72                        234             6.26               6.00            3.43            1465                      2.97                        0.00                       48.66                      11387               30.52                  0.00                  2672.96                   6.26            426.94                      48.66                      Africa\n",
            "114           Korea                   2400.02                     1717.61                  2156.49                568804.48                        237             5.81               6.00            2.80            1376                      2.76                        0.00                       45.28                      10732               39.77                  0.00                  2400.02                   5.81            413.38                      45.28                        Asia\n",
            "6          Anguilla                   3131.51                     2533.74                  2390.02                438411.80                        140             6.44               6.00            3.19             902                      3.43                        0.00                       49.16                       6883               32.62                  0.00                  3131.51                   6.44            486.04                      49.16                   Caribbean\n",
            "222          Turkey                   3053.19                     2284.61                  2455.04                427446.13                        140             6.24               6.00            3.10             873                      2.32                        0.00                       53.61                       7506               23.13                  0.00                  3053.19                   6.24            489.63                      53.61  Middle East & North Africa\n",
            "112           Kenya                   2824.88                     2454.40                  2254.65                412433.11                        146             6.16               6.00            2.98             900                      3.46                        0.00                       51.68                       7545               23.85                  0.00                  2824.88                   6.16            458.26                      51.68                      Africa\n",
            "146      Mozambique                   3024.73                     2076.36                  2454.65                411363.35                        136             6.60               6.00            3.14             897                      3.60                        0.00                       53.88                       7328               33.88                  0.00                  3024.73                   6.60            458.60                      53.88                      Africa\n",
            "239  Western Sahara                   3107.74                     2595.45                  2367.39                410221.16                        132             6.41               6.50            3.32             846                      2.84                        0.00                       52.23                       6895               53.07                  1.29                  3107.74                   6.41            484.89                      52.23  Middle East & North Africa\n",
            "86             Guam                   2668.68                     1834.41                  2317.39                408308.61                        153             6.23               6.00            3.26             953                      2.75                        0.00                       52.63                       8052               69.55                  2.94                  2668.68                   6.23            428.45                      52.63                     Oceania\n",
            "62            Egypt                   3158.88                     2608.90                  2449.20                407494.95                        129             6.41               6.00            2.96             827                      3.26                        0.00                       55.64                       7177               48.77                  1.96                  3158.88                   6.41            492.74                      55.64  Middle East & North Africa\n",
            "118          Latvia                   2919.29                     2094.54                  2352.69                405781.77                        139             6.26               6.00            2.83             870                      3.02                        0.00                       39.96                       5555               50.66                  0.00                  2919.29                   6.26            466.42                      39.96                      Europe\n",
            "\n",
            "3.2 Top and bottom performing locations\n",
            "Top 10 locations by total revenue:\n",
            "     Store Location                      Region  Sales Revenue (USD)_sum  Marketing_ROI_mean\n",
            "49            Congo                      Africa                625472.72               30.52\n",
            "114           Korea                        Asia                568804.48               39.77\n",
            "6          Anguilla                   Caribbean                438411.80               32.62\n",
            "222          Turkey  Middle East & North Africa                427446.13               23.13\n",
            "112           Kenya                      Africa                412433.11               23.85\n",
            "146      Mozambique                      Africa                411363.35               33.88\n",
            "239  Western Sahara  Middle East & North Africa                410221.16               53.07\n",
            "86             Guam                     Oceania                408308.61               69.55\n",
            "62            Egypt  Middle East & North Africa                407494.95               48.77\n",
            "118          Latvia                      Europe                405781.77               50.66\n",
            "\n",
            "Bottom 10 locations by total revenue:\n",
            "                  Store Location         Region  Sales Revenue (USD)_sum  Marketing_ROI_mean\n",
            "64             Equatorial Guinea         Africa                257923.35               24.11\n",
            "92                         Haiti      Caribbean                257759.83               32.38\n",
            "87                     Guatemala  North America                257572.78               42.03\n",
            "180                       Rwanda         Africa                252523.70               24.62\n",
            "130                     Maldives           Asia                251787.98               55.41\n",
            "76                         Gabon         Africa                242322.69               77.52\n",
            "161     Northern Mariana Islands        Oceania                228264.93               23.67\n",
            "232     United States of America  North America                216666.43               34.85\n",
            "75   French Southern Territories          Other                215701.04               62.56\n",
            "4                        Andorra         Europe                196079.73               36.81\n",
            "\n",
            "3.3 Location performance distribution\n",
            "\n",
            "4. Analyzing regional product category performance...\n",
            "\n",
            "4.1 Regional category performance\n",
            "Regional revenue by product category:\n",
            "Product Category             Clothing  Electronics  Furniture  Groceries\n",
            "Region                                                                  \n",
            "Africa                     4034455.78   5526743.26 4665018.97 2046260.50\n",
            "Asia                       2524735.20   3689660.34 3111324.93 1301070.43\n",
            "Caribbean                  2273095.94   2962246.23 2537436.98 1080971.40\n",
            "Europe                     4220912.42   6307367.93 4895887.98 2340712.15\n",
            "Latin America              1103593.50   1555464.96 1246767.32  600515.07\n",
            "Middle East & North Africa 1868418.37   2679100.20 1970898.25  908732.16\n",
            "North America               879104.96   1241896.51  907585.35  533311.69\n",
            "Oceania                    2048259.63   2877682.35 2382218.85 1162287.04\n",
            "Other                       564352.24    761378.78  657922.46  230679.77\n",
            "\n",
            "4.2 Regional category preferences\n",
            "Regional category preference index:\n",
            "Product Category            Clothing  Electronics  Furniture  Groceries\n",
            "Region                                                                 \n",
            "Africa                          1.01         0.98       1.02       0.98\n",
            "Asia                            0.97         1.00       1.04       0.96\n",
            "Caribbean                       1.05         0.97       1.02       0.95\n",
            "Europe                          0.97         1.03       0.98       1.03\n",
            "Latin America                   1.00         1.00       0.99       1.04\n",
            "Middle East & North Africa      1.03         1.04       0.95       0.96\n",
            "North America                   1.01         1.01       0.91       1.17\n",
            "Oceania                         0.99         0.98       1.00       1.07\n",
            "Other                           1.04         0.99       1.06       0.81\n",
            "\n",
            "4.3 Top category by region\n",
            "Top product category by region:\n",
            "                       Region Top Category  Sales Revenue (USD)\n",
            "0                      Africa  Electronics           5526743.26\n",
            "0                   Caribbean  Electronics           2962246.23\n",
            "0                        Asia  Electronics           3689660.34\n",
            "0                      Europe  Electronics           6307367.93\n",
            "0               Latin America  Electronics           1555464.96\n",
            "0                     Oceania  Electronics           2877682.35\n",
            "0               North America  Electronics           1241896.51\n",
            "0  Middle East & North Africa  Electronics           2679100.20\n",
            "0                       Other  Electronics            761378.78\n",
            "\n",
            "5. Analyzing regional temporal patterns...\n",
            "\n",
            "5.1 Regional quarterly performance\n",
            "Regional revenue by quarter:\n",
            "Quarter                             1          2          3          4\n",
            "Region                                                                \n",
            "Africa                     3517600.77 4063028.01 4312743.76 4379105.98\n",
            "Asia                       2409215.26 2599739.16 2806257.66 2811578.81\n",
            "Caribbean                  1976631.79 2182675.90 2433063.51 2261379.35\n",
            "Europe                     4013374.07 4285325.94 4700908.75 4765271.73\n",
            "Latin America              1051221.67 1075367.38 1222445.73 1157306.08\n",
            "Middle East & North Africa 1794537.94 1752864.04 1884751.46 1994995.53\n",
            "North America               736685.06  844536.01  965103.58 1015573.86\n",
            "Oceania                    1876587.73 2167189.58 2158642.56 2268027.98\n",
            "Other                       429338.53  575899.10  626484.36  582611.25\n",
            "\n",
            "5.2 Regional quarterly growth\n",
            "Regional quarterly growth rates:\n",
            "Quarter                      1     2     3     4\n",
            "Region                                          \n",
            "Africa                     NaN 15.51  6.15  1.54\n",
            "Asia                       NaN  7.91  7.94  0.19\n",
            "Caribbean                  NaN 10.42 11.47 -7.06\n",
            "Europe                     NaN  6.78  9.70  1.37\n",
            "Latin America              NaN  2.30 13.68 -5.33\n",
            "Middle East & North Africa NaN -2.32  7.52  5.85\n",
            "North America              NaN 14.64 14.28  5.23\n",
            "Oceania                    NaN 15.49 -0.39  5.07\n",
            "Other                      NaN 34.14  8.78 -7.00\n",
            "\n",
            "5.3 Regional holiday effect\n",
            "Regional holiday effect:\n",
            "Holiday Effect               False    True  Holiday_Impact\n",
            "Region                                                    \n",
            "Africa                     2646.74 3979.19            1.50\n",
            "Asia                       2628.51 4408.40            1.68\n",
            "Caribbean                  2686.97 4312.52            1.60\n",
            "Europe                     2649.83 4799.97            1.81\n",
            "Latin America              2644.16 3309.16            1.25\n",
            "Middle East & North Africa 2695.32 4761.81            1.77\n",
            "North America              2538.07 3404.16            1.34\n",
            "Oceania                    2631.59 4762.04            1.81\n",
            "Other                      2653.94 5023.98            1.89\n",
            "\n",
            "6. Analyzing regional discount and marketing effectiveness...\n",
            "\n",
            "6.1 Regional discount effectiveness\n",
            "Regional revenue by discount level:\n",
            "Discount_Level                High     Low  Medium  Very Low\n",
            "Region                                                      \n",
            "Africa                     2348.73 2405.01 2417.31   2705.41\n",
            "Asia                       2214.86 2279.38 2289.35   2750.53\n",
            "Caribbean                  2148.44 2591.38 2561.93   2321.83\n",
            "Europe                     2212.67 2366.44 2491.26   2537.74\n",
            "Latin America              2062.63 2104.87 2238.22   2810.10\n",
            "Middle East & North Africa 2087.40 2733.53 2263.67   2454.62\n",
            "North America              2371.14 2994.62 2208.05   2407.57\n",
            "Oceania                    2232.30 2386.01 2451.13   2615.27\n",
            "Other                      2028.25 2772.42 2348.88   2908.38\n",
            "\n",
            "6.2 Regional discount sensitivity\n",
            "Optimal discount level by region:\n",
            "                       Region Optimal Discount Level  Optimal Discount Percentage  Average Revenue  Average Units\n",
            "0                      Africa               Very Low                         5.00          2705.41           6.00\n",
            "0                   Caribbean                    Low                        10.00          2591.38           6.22\n",
            "0                        Asia               Very Low                         5.00          2750.53           6.27\n",
            "0                      Europe               Very Low                         5.00          2537.74           5.96\n",
            "0               Latin America               Very Low                         5.00          2810.10           6.06\n",
            "0                     Oceania               Very Low                         5.00          2615.27           6.19\n",
            "0               North America                    Low                        10.00          2994.62           6.25\n",
            "0  Middle East & North Africa                    Low                        10.00          2733.53           6.22\n",
            "0                       Other               Very Low                         5.00          2908.38           6.78\n",
            "\n",
            "6.3 Regional marketing effectiveness\n",
            "Regional marketing ROI by spend level:\n",
            "Marketing_Bin                0-50  50-100  100-150  150-200  200+\n",
            "Region                                                           \n",
            "Africa                     238.65   33.75    20.74    13.43   NaN\n",
            "Asia                       241.73   35.63    21.25    14.23   NaN\n",
            "Caribbean                  244.34   38.31    20.33    15.08   NaN\n",
            "Europe                     236.38   35.82    20.59    13.67   NaN\n",
            "Latin America              247.42   35.39    19.95    15.41   NaN\n",
            "Middle East & North Africa 276.57   38.53    20.45    15.06   NaN\n",
            "North America              253.56   37.61    22.54    12.76   NaN\n",
            "Oceania                    218.80   35.53    19.53    13.23   NaN\n",
            "Other                      183.92   36.84    18.96    12.75   NaN\n",
            "\n",
            "6.4 Regional optimal marketing spend\n",
            "Optimal marketing spend by region:\n",
            "                       Region Optimal Marketing Bin  Marketing ROI\n",
            "0                      Africa                  0-50         238.65\n",
            "0                   Caribbean                  0-50         244.34\n",
            "0                        Asia                  0-50         241.73\n",
            "0                      Europe                  0-50         236.38\n",
            "0               Latin America                  0-50         247.42\n",
            "0                     Oceania                  0-50         218.80\n",
            "0               North America                  0-50         253.56\n",
            "0  Middle East & North Africa                  0-50         276.57\n",
            "0                       Other                  0-50         183.92\n",
            "\n",
            "7. Performing store location clustering...\n",
            "\n",
            "7.1 Preparing data for clustering\n",
            "\n",
            "7.2 Determining optimal number of clusters\n",
            "\n",
            "7.3 Performing clustering\n",
            "\n",
            "7.4 Analyzing clusters\n",
            "Cluster statistics:\n",
            "         Sales Revenue (USD)_mean  Units Sold_mean  Revenue_per_Transaction  Units_per_Transaction  Revenue_per_Unit  Marketing_ROI_mean  Discount Percentage_mean  Number_of_Locations\n",
            "Cluster                                                                                                                                                                                \n",
            "0                         2607.31             6.18                  2607.31                   6.18            421.92               37.40                      3.17                  104\n",
            "1                         2887.21             6.26                  2887.21                   6.26            461.34               44.52                      2.84                   77\n",
            "2                         2455.66             5.76                  2455.66                   5.76            426.67               35.83                      2.82                   62\n",
            "\n",
            "Regional distribution within clusters (%):\n",
            "Region   Africa  Asia  Caribbean  Europe  Latin America  Middle East & North Africa  North America  Oceania  Other\n",
            "Cluster                                                                                                           \n",
            "0         18.27 10.58      12.50   25.00           7.69                        8.65           4.81    10.58   1.92\n",
            "1         23.38 14.29      11.69   19.48           5.19                        9.09           2.60    10.39   3.90\n",
            "2         17.74 16.13       8.06   22.58           3.23                        9.68           8.06    11.29   3.23\n",
            "\n",
            "7.2 Regional performance comparison using radar charts\n",
            "Created 4 regional comparison radar charts\n",
            "\n",
            "8. Performing regional opportunity analysis...\n",
            "\n",
            "8.1 Regional performance gap analysis\n",
            "Regional performance gaps:\n",
            "                       Region  Revenue_Gap_to_Top  Revenue_Gap_Percentage  ROI_Gap_to_Top  ROI_Gap_Percentage\n",
            "3                      Europe                0.00                    0.00            7.20               18.83\n",
            "0                      Africa          1492401.96                    9.17            5.50               13.77\n",
            "1                        Asia          7138089.58                   67.17            6.22               15.88\n",
            "2                   Caribbean          8911129.94                  100.65            9.87               27.77\n",
            "7                     Oceania          9294432.62                  109.73            6.34               16.23\n",
            "5  Middle East & North Africa         10337731.51                  139.19            0.00                0.00\n",
            "4               Latin America         13258539.63                  294.22            9.26               25.61\n",
            "6               North America         14202981.97                  398.75            2.77                6.49\n",
            "8                       Other         15550547.23                  702.27           14.12               45.11\n",
            "\n",
            "8.2 Regional improvement opportunities\n",
            "Regional improvement opportunities:\n",
            "                       Region  Current Revenue  Revenue Gap to Top (%)  Potential Revenue Increase (50% Gap Closure) Top Category Optimal Discount Level Optimal Marketing Spend\n",
            "0                      Africa      16272478.52                    9.17                                     746200.98  Electronics               Very Low                    0-50\n",
            "0                   Caribbean       8853750.54                  100.65                                    4455564.97  Electronics                    Low                    0-50\n",
            "0                        Asia      10626790.90                   67.17                                    3569044.79  Electronics               Very Low                    0-50\n",
            "0               Latin America       4506340.86                  294.22                                    6629269.81  Electronics               Very Low                    0-50\n",
            "0                     Oceania       8470447.86                  109.73                                    4647216.31  Electronics               Very Low                    0-50\n",
            "0               North America       3561898.51                  398.75                                    7101490.98  Electronics                    Low                    0-50\n",
            "0  Middle East & North Africa       7427148.97                  139.19                                    5168865.75  Electronics                    Low                    0-50\n",
            "0                       Other       2214333.25                  702.27                                    7775273.62  Electronics               Very Low                    0-50\n",
            "\n",
            "8.3 Store location improvement opportunities\n",
            "Bottom performing locations by region:\n",
            "                                       Store Location                      Region  Sales Revenue (USD)_sum  Revenue_Gap_to_Region_Avg  Potential_Improvement\n",
            "0                                               Gabon                      Africa                242322.69                   96687.28               48343.64\n",
            "1                                              Rwanda                      Africa                252523.70                   86486.27               43243.13\n",
            "2                                   Equatorial Guinea                      Africa                257923.35                   81086.62               40543.31\n",
            "3                                               Haiti                   Caribbean                257759.83                   70156.85               35078.43\n",
            "4                                 Trinidad and Tobago                   Caribbean                258586.72                   69329.97               34664.98\n",
            "5                                            Dominica                   Caribbean                266029.93                   61886.76               30943.38\n",
            "6                                            Maldives                        Asia                251787.98                   80299.23               40149.62\n",
            "7                                             Vietnam                        Asia                265904.30                   66182.91               33091.46\n",
            "8                                              Taiwan                        Asia                277623.37                   54463.84               27231.92\n",
            "9                                             Andorra                      Europe                196079.73                  126918.09               63459.05\n",
            "10                                             Greece                      Europe                258857.79                   64140.03               32070.02\n",
            "11                       Svalbard & Jan Mayen Islands                      Europe                263186.29                   59811.54               29905.77\n",
            "12                                            Bolivia               Latin America                267257.83                   54623.66               27311.83\n",
            "13                                              Chile               Latin America                275343.97                   46537.52               23268.76\n",
            "14                                               Peru               Latin America                275433.99                   46447.50               23223.75\n",
            "15                           Northern Mariana Islands                     Oceania                228264.93                   97521.52               48760.76\n",
            "16                                         Micronesia                     Oceania                261287.53                   64498.92               32249.46\n",
            "17                                      New Caledonia                     Oceania                270571.15                   55215.31               27607.65\n",
            "18                           United States of America               North America                216666.43                   80158.44               40079.22\n",
            "19                                          Guatemala               North America                257572.78                   39252.09               19626.05\n",
            "20                                         Costa Rica               North America                276703.10                   20121.78               10060.89\n",
            "21                              Palestinian Territory  Middle East & North Africa                268366.65                   69231.03               34615.52\n",
            "22                                            Tunisia  Middle East & North Africa                276659.59                   60938.10               30469.05\n",
            "23                                               Iraq  Middle East & North Africa                284145.65                   53452.03               26726.01\n",
            "24                        French Southern Territories                       Other                215701.04                  100632.28               50316.14\n",
            "25  British Indian Ocean Territory (Chagos Archipe...                       Other                281922.22                   34411.11               17205.55\n",
            "26       Antarctica (the territory South of 60 deg S)                       Other                310918.43                    5414.89                2707.44\n",
            "\n",
            "9. Generating comprehensive insights report...\n",
            "Comprehensive insights report generated and saved to /content/geographical_analysis/geographical_regional_insights.md\n",
            "\n",
            "Step 5: Geographical and Regional Performance Analysis completed successfully!\n",
            "The analysis results are ready for review and further steps.\n",
            "\n",
            "Next steps: Proceed to Step 6 - Develop profitability and ROI analysis\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 5: Geographical and Regional Performance Analysis\n",
        "This script performs comprehensive geographical and regional analysis to identify\n",
        "spatial patterns and regional optimization opportunities.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.projections import register_projection\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create directories for outputs\n",
        "if not os.path.exists('/content/geographical_analysis'):\n",
        "    os.makedirs('/content/geographical_analysis')\n",
        "if not os.path.exists('/content/geographical_analysis/plots'):\n",
        "    os.makedirs('/content/geographical_analysis/plots')\n",
        "if not os.path.exists('/content/geographical_analysis/data'):\n",
        "    os.makedirs('/content/geographical_analysis/data')\n",
        "if not os.path.exists('/content/geographical_analysis/clusters'):\n",
        "    os.makedirs('/content/geographical_analysis/clusters')\n",
        "\n",
        "print(\"Step 5: Geographical and Regional Performance Analysis\")\n",
        "print(\"====================================================\\n\")\n",
        "\n",
        "# 1. Load the cleaned dataset\n",
        "print(\"1. Loading the cleaned dataset...\")\n",
        "df = pd.read_csv('/content/cleaned_data/retail_sales_cleaned.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(f\"Cleaned dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\\n\")\n",
        "\n",
        "# 2. Regional Performance Overview\n",
        "print(\"2. Analyzing regional performance overview...\")\n",
        "\n",
        "# 2.1 Regional sales summary\n",
        "print(\"\\n2.1 Regional sales summary\")\n",
        "regional_sales = df.groupby('Region').agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'median', 'std', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'median', 'std', 'sum'],\n",
        "    'Discount Percentage': ['mean', 'median'],\n",
        "    'Marketing Spend (USD)': ['mean', 'sum'],\n",
        "    'Marketing_ROI': ['mean', 'median']\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "regional_sales.columns = ['_'.join(col).strip() for col in regional_sales.columns.values]\n",
        "regional_sales = regional_sales.reset_index()\n",
        "\n",
        "# Add derived metrics\n",
        "regional_sales['Revenue_per_Transaction'] = regional_sales['Sales Revenue (USD)_sum'] / regional_sales['Sales Revenue (USD)_count']\n",
        "regional_sales['Units_per_Transaction'] = regional_sales['Units Sold_sum'] / regional_sales['Sales Revenue (USD)_count']\n",
        "regional_sales['Revenue_per_Unit'] = regional_sales['Sales Revenue (USD)_sum'] / regional_sales['Units Sold_sum']\n",
        "regional_sales['Marketing_per_Transaction'] = regional_sales['Marketing Spend (USD)_sum'] / regional_sales['Sales Revenue (USD)_count']\n",
        "\n",
        "# Sort by total revenue\n",
        "regional_sales = regional_sales.sort_values('Sales Revenue (USD)_sum', ascending=False)\n",
        "\n",
        "print(regional_sales)\n",
        "regional_sales.to_csv('/content/geographical_analysis/data/regional_sales_summary.csv', index=False)\n",
        "\n",
        "# 2.2 Regional performance visualization\n",
        "print(\"\\n2.2 Regional performance visualization\")\n",
        "\n",
        "# Plot total revenue by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(regional_sales['Region'], regional_sales['Sales Revenue (USD)_sum'])\n",
        "plt.title('Total Sales Revenue by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Total Sales Revenue (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/regional_total_revenue.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot average revenue per transaction by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(regional_sales['Region'], regional_sales['Revenue_per_Transaction'])\n",
        "plt.title('Average Revenue per Transaction by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Average Revenue per Transaction (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/regional_avg_transaction.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot marketing ROI by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(regional_sales['Region'], regional_sales['Marketing_ROI_mean'])\n",
        "plt.title('Average Marketing ROI by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Marketing ROI (Revenue/Spend)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/regional_marketing_roi.png')\n",
        "plt.close()\n",
        "\n",
        "# 2.3 Statistical comparison of regions\n",
        "print(\"\\n2.3 Statistical comparison of regions\")\n",
        "\n",
        "# Perform ANOVA to test if region has a significant effect on sales\n",
        "df = df.rename(columns={'Sales Revenue (USD)': 'Sales_Revenue_USD'})\n",
        "region_anova = ols('Sales_Revenue_USD ~ C(Region)', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(region_anova, typ=2)\n",
        "print(\"ANOVA results for regional effect on sales:\")\n",
        "print(anova_table)\n",
        "anova_table.to_csv('/content/geographical_analysis/data/region_anova_sales.csv')\n",
        "\n",
        "# Perform ANOVA to test if region has a significant effect on units sold\n",
        "df = df.rename(columns={'Units Sold': 'Units_Sold'})\n",
        "region_units_anova = ols('Units_Sold ~ C(Region)', data=df).fit()\n",
        "units_anova_table = sm.stats.anova_lm(region_units_anova, typ=2)\n",
        "print(\"\\nANOVA results for regional effect on units sold:\")\n",
        "print(units_anova_table)\n",
        "units_anova_table.to_csv('/content/geographical_analysis/data/region_anova_units.csv')\n",
        "\n",
        "# Perform ANOVA to test if region has a significant effect on marketing ROI\n",
        "region_roi_anova = ols('Marketing_ROI ~ C(Region)', data=df).fit()\n",
        "roi_anova_table = sm.stats.anova_lm(region_roi_anova, typ=2)\n",
        "print(\"\\nANOVA results for regional effect on marketing ROI:\")\n",
        "print(roi_anova_table)\n",
        "roi_anova_table.to_csv('/content/geographical_analysis/data/region_anova_roi.csv')\n",
        "\n",
        "# 3. Store Location Analysis\n",
        "print(\"\\n3. Analyzing store location performance...\")\n",
        "\n",
        "# 3.1 Store location summary\n",
        "print(\"\\n3.1 Store location summary\")\n",
        "df = df.rename(columns={'Units_Sold': 'Units Sold'})\n",
        "df = df.rename(columns={'Sales_Revenue_USD' : 'Sales Revenue (USD)'})\n",
        "location_sales = df.groupby('Store Location').agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'median', 'std', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'median', 'std', 'sum'],\n",
        "    'Discount Percentage': ['mean', 'median'],\n",
        "    'Marketing Spend (USD)': ['mean', 'sum'],\n",
        "    'Marketing_ROI': ['mean', 'median']\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "location_sales.columns = ['_'.join(col).strip() for col in location_sales.columns.values]\n",
        "location_sales = location_sales.reset_index()\n",
        "\n",
        "# Add derived metrics\n",
        "location_sales['Revenue_per_Transaction'] = location_sales['Sales Revenue (USD)_sum'] / location_sales['Sales Revenue (USD)_count']\n",
        "location_sales['Units_per_Transaction'] = location_sales['Units Sold_sum'] / location_sales['Sales Revenue (USD)_count']\n",
        "location_sales['Revenue_per_Unit'] = location_sales['Sales Revenue (USD)_sum'] / location_sales['Units Sold_sum']\n",
        "location_sales['Marketing_per_Transaction'] = location_sales['Marketing Spend (USD)_sum'] / location_sales['Sales Revenue (USD)_count']\n",
        "\n",
        "# Add region information\n",
        "location_region_map = df[['Store Location', 'Region']].drop_duplicates().set_index('Store Location')['Region']\n",
        "location_sales['Region'] = location_sales['Store Location'].map(location_region_map)\n",
        "\n",
        "# Sort by total revenue\n",
        "location_sales = location_sales.sort_values('Sales Revenue (USD)_sum', ascending=False)\n",
        "\n",
        "print(f\"Total number of store locations: {len(location_sales)}\")\n",
        "print(location_sales.head(10))  # Show top 10 locations\n",
        "location_sales.to_csv('/content/geographical_analysis/data/location_sales_summary.csv', index=False)\n",
        "\n",
        "# 3.2 Top and bottom performing locations\n",
        "print(\"\\n3.2 Top and bottom performing locations\")\n",
        "\n",
        "# Top 10 locations by total revenue\n",
        "top10_locations = location_sales.head(10)\n",
        "print(\"Top 10 locations by total revenue:\")\n",
        "print(top10_locations[['Store Location', 'Region', 'Sales Revenue (USD)_sum', 'Marketing_ROI_mean']])\n",
        "\n",
        "# Bottom 10 locations by total revenue\n",
        "bottom10_locations = location_sales.tail(10)\n",
        "print(\"\\nBottom 10 locations by total revenue:\")\n",
        "print(bottom10_locations[['Store Location', 'Region', 'Sales Revenue (USD)_sum', 'Marketing_ROI_mean']])\n",
        "\n",
        "# Plot top 10 locations\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(top10_locations['Store Location'], top10_locations['Sales Revenue (USD)_sum'])\n",
        "plt.title('Total Sales Revenue - Top 10 Store Locations')\n",
        "plt.xlabel('Store Location')\n",
        "plt.ylabel('Total Sales Revenue (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/top10_locations_revenue.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot bottom 10 locations\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(bottom10_locations['Store Location'], bottom10_locations['Sales Revenue (USD)_sum'])\n",
        "plt.title('Total Sales Revenue - Bottom 10 Store Locations')\n",
        "plt.xlabel('Store Location')\n",
        "plt.ylabel('Total Sales Revenue (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/bottom10_locations_revenue.png')\n",
        "plt.close()\n",
        "\n",
        "# 3.3 Location performance distribution\n",
        "print(\"\\n3.3 Location performance distribution\")\n",
        "\n",
        "# Histogram of location performance\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.hist(location_sales['Sales Revenue (USD)_sum'], bins=30)\n",
        "plt.title('Distribution of Total Sales Revenue Across Store Locations')\n",
        "plt.xlabel('Total Sales Revenue (USD)')\n",
        "plt.ylabel('Number of Store Locations')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/location_revenue_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Histogram of location marketing ROI\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.hist(location_sales['Marketing_ROI_mean'], bins=30)\n",
        "plt.title('Distribution of Marketing ROI Across Store Locations')\n",
        "plt.xlabel('Marketing ROI (Revenue/Spend)')\n",
        "plt.ylabel('Number of Store Locations')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/location_roi_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# 4. Regional Product Category Analysis\n",
        "print(\"\\n4. Analyzing regional product category performance...\")\n",
        "\n",
        "# 4.1 Regional category performance\n",
        "print(\"\\n4.1 Regional category performance\")\n",
        "region_category = df.groupby(['Region', 'Product Category']).agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Discount Percentage': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum',\n",
        "    'Marketing_ROI': 'mean'\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "region_category.columns = ['_'.join(col).strip() for col in region_category.columns.values]\n",
        "region_category = region_category.reset_index()\n",
        "\n",
        "# Create pivot tables for visualization\n",
        "region_category_revenue = region_category.pivot(index='Region', columns='Product Category', values='Sales Revenue (USD)_sum')\n",
        "region_category_units = region_category.pivot(index='Region', columns='Product Category', values='Units Sold_sum')\n",
        "region_category_roi = region_category.pivot(index='Region', columns='Product Category', values='Marketing_ROI_mean')\n",
        "\n",
        "print(\"Regional revenue by product category:\")\n",
        "print(region_category_revenue)\n",
        "region_category_revenue.to_csv('/content/geographical_analysis/data/region_category_revenue.csv')\n",
        "\n",
        "# Plot heatmap of regional category revenue\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_category_revenue, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Total Sales Revenue by Region and Product Category')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_category_revenue_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot heatmap of regional category units\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_category_units, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Total Units Sold by Region and Product Category')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_category_units_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot heatmap of regional category marketing ROI\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_category_roi, annot=True, fmt='.2f', cmap='YlGnBu')\n",
        "plt.title('Average Marketing ROI by Region and Product Category')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_category_roi_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 4.2 Regional category preferences\n",
        "print(\"\\n4.2 Regional category preferences\")\n",
        "\n",
        "# Calculate category preference index (% of region's sales in each category / % of overall sales in that category)\n",
        "total_category_sales = df.groupby('Product Category')['Sales Revenue (USD)'].sum()\n",
        "total_sales = df['Sales Revenue (USD)'].sum()\n",
        "category_share = total_category_sales / total_sales\n",
        "\n",
        "region_category_preference = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = df[df['Region'] == region]\n",
        "    region_total_sales = region_data['Sales Revenue (USD)'].sum()\n",
        "    region_category_sales = region_data.groupby('Product Category')['Sales Revenue (USD)'].sum()\n",
        "    region_category_share = region_category_sales / region_total_sales\n",
        "\n",
        "    # Calculate preference index\n",
        "    preference_index = region_category_share / category_share\n",
        "\n",
        "    # Add to dataframe\n",
        "    region_preference = pd.DataFrame({\n",
        "        'Region': region,\n",
        "        'Product Category': preference_index.index,\n",
        "        'Preference Index': preference_index.values\n",
        "    })\n",
        "\n",
        "    region_category_preference = pd.concat([region_category_preference, region_preference])\n",
        "\n",
        "# Create pivot table for visualization\n",
        "region_category_preference_pivot = region_category_preference.pivot(index='Region', columns='Product Category', values='Preference Index')\n",
        "\n",
        "print(\"Regional category preference index:\")\n",
        "print(region_category_preference_pivot)\n",
        "region_category_preference_pivot.to_csv('/content/geographical_analysis/data/region_category_preference.csv')\n",
        "\n",
        "# Plot heatmap of regional category preferences\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_category_preference_pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=1)\n",
        "plt.title('Product Category Preference Index by Region')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_category_preference_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 4.3 Top category by region\n",
        "print(\"\\n4.3 Top category by region\")\n",
        "\n",
        "# Identify top category for each region\n",
        "top_category_by_region = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = df[df['Region'] == region]\n",
        "    region_category_sales = region_data.groupby('Product Category')['Sales Revenue (USD)'].sum()\n",
        "    top_category = region_category_sales.idxmax()\n",
        "    top_sales = region_category_sales.max()\n",
        "\n",
        "    # Add to dataframe\n",
        "    top_category_by_region = pd.concat([top_category_by_region, pd.DataFrame({\n",
        "        'Region': [region],\n",
        "        'Top Category': [top_category],\n",
        "        'Sales Revenue (USD)': [top_sales]\n",
        "    })])\n",
        "\n",
        "print(\"Top product category by region:\")\n",
        "print(top_category_by_region)\n",
        "top_category_by_region.to_csv('/content/geographical_analysis/data/top_category_by_region.csv', index=False)\n",
        "\n",
        "# Plot top category by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "bars = plt.bar(top_category_by_region['Region'], top_category_by_region['Sales Revenue (USD)'])\n",
        "\n",
        "# Add category labels to bars\n",
        "for i, bar in enumerate(bars):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2,\n",
        "             bar.get_height() + 5000,\n",
        "             top_category_by_region['Top Category'].iloc[i],\n",
        "             ha='center', va='bottom', rotation=0, fontsize=10)\n",
        "\n",
        "plt.title('Top Product Category Sales by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Sales Revenue (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/top_category_by_region.png')\n",
        "plt.close()\n",
        "\n",
        "# 5. Regional Temporal Analysis\n",
        "print(\"\\n5. Analyzing regional temporal patterns...\")\n",
        "\n",
        "# 5.1 Regional quarterly performance\n",
        "print(\"\\n5.1 Regional quarterly performance\")\n",
        "region_quarter = df.groupby(['Region', 'Quarter']).agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Discount Percentage': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum',\n",
        "    'Marketing_ROI': 'mean'\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "region_quarter.columns = ['_'.join(col).strip() for col in region_quarter.columns.values]\n",
        "region_quarter = region_quarter.reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "region_quarter_revenue = region_quarter.pivot(index='Region', columns='Quarter', values='Sales Revenue (USD)_sum')\n",
        "\n",
        "print(\"Regional revenue by quarter:\")\n",
        "print(region_quarter_revenue)\n",
        "region_quarter_revenue.to_csv('/content/geographical_analysis/data/region_quarter_revenue.csv')\n",
        "\n",
        "# Plot heatmap of regional quarterly revenue\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_quarter_revenue, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Total Sales Revenue by Region and Quarter')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_quarter_revenue_heatmap.png')\n",
        "plt.close()\n",
        "# 5.2 Regional quarterly growth\n",
        "print(\"\\n5.2 Regional quarterly growth\")\n",
        "\n",
        "# Calculate quarter-over-quarter growth for each region\n",
        "region_quarter_growth = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = region_quarter[region_quarter['Region'] == region].sort_values('Quarter')\n",
        "\n",
        "    # Calculate growth rates\n",
        "    revenue_values = region_data['Sales Revenue (USD)_sum'].values\n",
        "    growth_rates = [np.nan]  # First quarter has no growth rate\n",
        "\n",
        "    for i in range(1, len(revenue_values)):\n",
        "        growth_rate = (revenue_values[i] - revenue_values[i-1]) / revenue_values[i-1] * 100\n",
        "        growth_rates.append(growth_rate)\n",
        "\n",
        "    # Add to dataframe\n",
        "    region_growth = pd.DataFrame({\n",
        "        'Region': region_data['Region'],\n",
        "        'Quarter': region_data['Quarter'],\n",
        "        'Sales Revenue (USD)': revenue_values,\n",
        "        'Growth Rate (%)': growth_rates\n",
        "    })\n",
        "\n",
        "    region_quarter_growth = pd.concat([region_quarter_growth, region_growth])\n",
        "\n",
        "# Create pivot table for visualization\n",
        "region_quarter_growth_pivot = region_quarter_growth.pivot(index='Region', columns='Quarter', values='Growth Rate (%)')\n",
        "\n",
        "print(\"Regional quarterly growth rates:\")\n",
        "print(region_quarter_growth_pivot)\n",
        "region_quarter_growth_pivot.to_csv('/content/geographical_analysis/data/region_quarter_growth.csv')\n",
        "\n",
        "# Plot heatmap of regional quarterly growth\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_quarter_growth_pivot, annot=True, fmt='.1f', cmap='RdYlGn', center=0)\n",
        "plt.title('Quarterly Growth Rate (%) by Region')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_quarter_growth_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 5.3 Regional holiday effect\n",
        "print(\"\\n5.3 Regional holiday effect\")\n",
        "region_holiday = df.groupby(['Region', 'Holiday Effect']).agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Discount Percentage': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum',\n",
        "    'Marketing_ROI': 'mean'\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "region_holiday.columns = ['_'.join(col).strip() for col in region_holiday.columns.values]\n",
        "region_holiday = region_holiday.reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "region_holiday_revenue = region_holiday.pivot(index='Region', columns='Holiday Effect', values='Sales Revenue (USD)_mean')\n",
        "\n",
        "# Calculate holiday impact ratio\n",
        "region_holiday_revenue['Holiday_Impact'] = region_holiday_revenue[True] / region_holiday_revenue[False]\n",
        "\n",
        "print(\"Regional holiday effect:\")\n",
        "print(region_holiday_revenue)\n",
        "region_holiday_revenue.to_csv('/content/geographical_analysis/data/region_holiday_effect.csv')\n",
        "\n",
        "# Plot holiday impact by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(region_holiday_revenue.index, region_holiday_revenue['Holiday_Impact'])\n",
        "plt.title('Holiday Sales Impact by Region (Holiday/Non-Holiday Ratio)')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Holiday Impact Ratio')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_holiday_impact.png')\n",
        "plt.close()\n",
        "# 6. Regional Discount and Marketing Analysis\n",
        "print(\"\\n6. Analyzing regional discount and marketing effectiveness...\")\n",
        "\n",
        "# 6.1 Regional discount effectiveness\n",
        "print(\"\\n6.1 Regional discount effectiveness\")\n",
        "region_discount = df.groupby(['Region', 'Discount_Level']).agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Discount Percentage': 'mean'\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "region_discount.columns = ['_'.join(col).strip() for col in region_discount.columns.values]\n",
        "region_discount = region_discount.reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "region_discount_revenue = region_discount.pivot(index='Region', columns='Discount_Level', values='Sales Revenue (USD)_mean')\n",
        "region_discount_units = region_discount.pivot(index='Region', columns='Discount_Level', values='Units Sold_mean')\n",
        "\n",
        "print(\"Regional revenue by discount level:\")\n",
        "print(region_discount_revenue)\n",
        "region_discount_revenue.to_csv('/content/geographical_analysis/data/region_discount_revenue.csv')\n",
        "\n",
        "# Plot heatmap of regional discount revenue\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_discount_revenue, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Average Sales Revenue by Region and Discount Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_discount_revenue_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot heatmap of regional discount units\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_discount_units, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Average Units Sold by Region and Discount Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_discount_units_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 6.2 Regional discount sensitivity\n",
        "print(\"\\n6.2 Regional discount sensitivity\")\n",
        "\n",
        "# Calculate optimal discount level for each region\n",
        "region_optimal_discount = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = region_discount[region_discount['Region'] == region]\n",
        "\n",
        "    # Find discount level with highest average revenue\n",
        "    optimal_level = region_data.loc[region_data['Sales Revenue (USD)_mean'].idxmax()]\n",
        "\n",
        "    # Add to dataframe\n",
        "    region_optimal_discount = pd.concat([region_optimal_discount, pd.DataFrame({\n",
        "        'Region': [region],\n",
        "        'Optimal Discount Level': [optimal_level['Discount_Level']],\n",
        "        'Optimal Discount Percentage': [optimal_level['Discount Percentage_mean']],\n",
        "        'Average Revenue': [optimal_level['Sales Revenue (USD)_mean']],\n",
        "        'Average Units': [optimal_level['Units Sold_mean']]\n",
        "    })])\n",
        "\n",
        "print(\"Optimal discount level by region:\")\n",
        "print(region_optimal_discount)\n",
        "region_optimal_discount.to_csv('/content/geographical_analysis/data/region_optimal_discount.csv', index=False)\n",
        "\n",
        "# Plot optimal discount by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "bars = plt.bar(region_optimal_discount['Region'], region_optimal_discount['Average Revenue'])\n",
        "\n",
        "# Add discount level labels to bars\n",
        "for i, bar in enumerate(bars):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2,\n",
        "             bar.get_height() + 100,\n",
        "             region_optimal_discount['Optimal Discount Level'].iloc[i],\n",
        "             ha='center', va='bottom', rotation=0, fontsize=10)\n",
        "\n",
        "plt.title('Optimal Discount Level by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Average Revenue at Optimal Discount (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_optimal_discount.png')\n",
        "plt.close()\n",
        "\n",
        "# 6.3 Regional marketing effectiveness\n",
        "print(\"\\n6.3 Regional marketing effectiveness\")\n",
        "region_marketing = df.groupby(['Region', 'Marketing Spend (USD)']).agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Marketing_ROI': 'mean'\n",
        "})\n",
        "\n",
        "# Flatten the multi-index columns\n",
        "region_marketing.columns = ['_'.join(col).strip() for col in region_marketing.columns.values]\n",
        "region_marketing = region_marketing.reset_index()\n",
        "\n",
        "# Create marketing spend bins\n",
        "region_marketing['Marketing_Bin'] = pd.cut(\n",
        "    region_marketing['Marketing Spend (USD)'],\n",
        "    bins=[0, 50, 100, 150, 200, float('inf')],\n",
        "    labels=['0-50', '50-100', '100-150', '150-200', '200+']\n",
        ")\n",
        "\n",
        "# Calculate average ROI by region and marketing bin\n",
        "region_marketing_roi = region_marketing.groupby(['Region', 'Marketing_Bin'])['Marketing_ROI_mean'].mean().reset_index()\n",
        "region_marketing_roi_pivot = region_marketing_roi.pivot(index='Region', columns='Marketing_Bin', values='Marketing_ROI_mean')\n",
        "\n",
        "print(\"Regional marketing ROI by spend level:\")\n",
        "print(region_marketing_roi_pivot)\n",
        "region_marketing_roi_pivot.to_csv('/content/geographical_analysis/data/region_marketing_roi.csv')\n",
        "\n",
        "# Plot heatmap of regional marketing ROI\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_marketing_roi_pivot, annot=True, fmt='.2f', cmap='YlGnBu')\n",
        "plt.title('Average Marketing ROI by Region and Marketing Spend Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_marketing_roi_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 6.4 Regional optimal marketing spend\n",
        "print(\"\\n6.4 Regional optimal marketing spend\")\n",
        "\n",
        "# Calculate optimal marketing spend for each region\n",
        "region_optimal_marketing = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = region_marketing_roi[region_marketing_roi['Region'] == region]\n",
        "\n",
        "    # Find marketing bin with highest ROI\n",
        "    if len(region_data) > 0:\n",
        "        optimal_bin = region_data.loc[region_data['Marketing_ROI_mean'].idxmax()]\n",
        "\n",
        "        # Add to dataframe\n",
        "        region_optimal_marketing = pd.concat([region_optimal_marketing, pd.DataFrame({\n",
        "            'Region': [region],\n",
        "            'Optimal Marketing Bin': [optimal_bin['Marketing_Bin']],\n",
        "            'Marketing ROI': [optimal_bin['Marketing_ROI_mean']]\n",
        "        })])\n",
        "\n",
        "print(\"Optimal marketing spend by region:\")\n",
        "print(region_optimal_marketing)\n",
        "region_optimal_marketing.to_csv('/content/geographical_analysis/data/region_optimal_marketing.csv', index=False)\n",
        "\n",
        "# Plot optimal marketing by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "bars = plt.bar(region_optimal_marketing['Region'], region_optimal_marketing['Marketing ROI'])\n",
        "\n",
        "# Add marketing bin labels to bars\n",
        "for i, bar in enumerate(bars):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2,\n",
        "             bar.get_height() + 0.1,\n",
        "             region_optimal_marketing['Optimal Marketing Bin'].iloc[i],\n",
        "             ha='center', va='bottom', rotation=0, fontsize=10)\n",
        "\n",
        "plt.title('Optimal Marketing Spend Level by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Marketing ROI at Optimal Spend')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/region_optimal_marketing.png')\n",
        "plt.close()\n",
        "\n",
        "# 7. Store Location Clustering\n",
        "print(\"\\n7. Performing store location clustering...\")\n",
        "\n",
        "# 7.1 Prepare data for clustering\n",
        "print(\"\\n7.1 Preparing data for clustering\")\n",
        "\n",
        "# Select features for clustering\n",
        "clustering_features = [\n",
        "    'Sales Revenue (USD)_mean', 'Units Sold_mean', 'Revenue_per_Transaction',\n",
        "    'Units_per_Transaction', 'Revenue_per_Unit', 'Marketing_ROI_mean',\n",
        "    'Discount Percentage_mean'\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "location_cluster_data = location_sales[clustering_features].copy()\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "location_cluster_scaled = scaler.fit_transform(location_cluster_data)\n",
        "\n",
        "# 7.2 Determine optimal number of clusters\n",
        "print(\"\\n7.2 Determining optimal number of clusters\")\n",
        "\n",
        "# Calculate inertia for different numbers of clusters\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(location_cluster_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot elbow curve\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(k_range, inertia, 'o-')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/kmeans_elbow_curve.png')\n",
        "plt.close()\n",
        "\n",
        "# 7.3 Perform clustering\n",
        "print(\"\\n7.3 Performing clustering\")\n",
        "\n",
        "# Choose optimal k (this is done by examining the elbow curve. Our k is 3 )\n",
        "optimal_k = 3\n",
        "\n",
        "# Perform k-means clustering\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(location_cluster_scaled)\n",
        "\n",
        "# Add cluster labels to location data\n",
        "location_sales['Cluster'] = cluster_labels\n",
        "\n",
        "# 7.4 Analyze clusters\n",
        "print(\"\\n7.4 Analyzing clusters\")\n",
        "\n",
        "# Calculate cluster statistics\n",
        "cluster_stats = location_sales.groupby('Cluster').agg({\n",
        "    'Sales Revenue (USD)_mean': 'mean',\n",
        "    'Units Sold_mean': 'mean',\n",
        "    'Revenue_per_Transaction': 'mean',\n",
        "    'Units_per_Transaction': 'mean',\n",
        "    'Revenue_per_Unit': 'mean',\n",
        "    'Marketing_ROI_mean': 'mean',\n",
        "    'Discount Percentage_mean': 'mean',\n",
        "    'Store Location': 'count'\n",
        "})\n",
        "\n",
        "# Rename count column\n",
        "cluster_stats = cluster_stats.rename(columns={'Store Location': 'Number_of_Locations'})\n",
        "\n",
        "print(\"Cluster statistics:\")\n",
        "print(cluster_stats)\n",
        "cluster_stats.to_csv('/content/geographical_analysis/clusters/cluster_statistics.csv')\n",
        "\n",
        "# Calculate regional distribution within clusters\n",
        "cluster_region_counts = pd.crosstab(location_sales['Cluster'], location_sales['Region'])\n",
        "cluster_region_pct = cluster_region_counts.div(cluster_region_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "print(\"\\nRegional distribution within clusters (%):\")\n",
        "print(cluster_region_pct)\n",
        "cluster_region_pct.to_csv('/content/geographical_analysis/clusters/cluster_region_distribution.csv')\n",
        "\n",
        "# Plot cluster characteristics\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Normalize the data for radar chart\n",
        "cluster_radar = cluster_stats[clustering_features].copy()\n",
        "cluster_radar = (cluster_radar - cluster_radar.min()) / (cluster_radar.max() - cluster_radar.min())\n",
        "\n",
        "# Set up the radar chart\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.spines import Spine\n",
        "from matplotlib.transforms import Affine2D\n",
        "\n",
        "def radar_factory(num_vars, frame='circle'):\n",
        "    # Calculate evenly-spaced axis angles\n",
        "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "\n",
        "    # Rotate theta such that the first axis is at the top\n",
        "    theta += np.pi/2\n",
        "\n",
        "    def draw_poly_patch(ax):\n",
        "        # Draw polygon connecting the axis points\n",
        "        verts = unit_poly_verts(theta)\n",
        "        return plt.Polygon(verts, closed=True, edgecolor='k')\n",
        "\n",
        "    def draw_poly_lines(ax):\n",
        "        # Draw one axis per variable and add labels\n",
        "        for i, angle in enumerate(theta):\n",
        "            l = plt.Line2D([0, np.cos(angle)], [0, np.sin(angle)], color='k', linewidth=1)\n",
        "            ax.add_line(l)\n",
        "            ax.text(1.1*np.cos(angle), 1.1*np.sin(angle), labels[i],\n",
        "                   horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "    def unit_poly_verts(theta):\n",
        "        # Return vertices of polygon for radar chart\n",
        "        verts = [(0, 0)]\n",
        "        for angle in theta:\n",
        "            verts.append((np.cos(angle), np.sin(angle)))\n",
        "        return verts\n",
        "\n",
        "    class RadarAxes(plt.PolarAxes):\n",
        "        name = 'radar'\n",
        "\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.set_theta_zero_location('N')\n",
        "\n",
        "        def fill(self, *args, **kwargs):\n",
        "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
        "            closed = kwargs.pop('closed', True)\n",
        "            return super().fill(closed=closed, *args, **kwargs)\n",
        "\n",
        "        def plot(self, *args, **kwargs):\n",
        "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
        "            lines = super().plot(*args, **kwargs)\n",
        "            for line in lines:\n",
        "                self._close_line(line)\n",
        "\n",
        "        def _close_line(self, line):\n",
        "            x, y = line.get_data()\n",
        "            # FIXME: markers at x[0], y[0] get doubled-up\n",
        "            if x[0] != x[-1]:\n",
        "                x = np.concatenate((x, [x[0]]))\n",
        "                y = np.concatenate((y, [y[0]]))\n",
        "                line.set_data(x, y)\n",
        "\n",
        "        def set_varlabels(self, labels):\n",
        "            self.set_thetagrids(np.degrees(theta), labels)\n",
        "\n",
        "        def _gen_axes_patch(self):\n",
        "            return draw_poly_patch(self)\n",
        "\n",
        "        def _gen_axes_spines(self):\n",
        "            spine_type = 'circle'\n",
        "            verts = unit_poly_verts(theta)\n",
        "            verts.append(verts[0])\n",
        "            path = Path(verts)\n",
        "\n",
        "            spine = Spine(self, spine_type, path)\n",
        "            spine.set_transform(Affine2D().scale(.5).translate(.5, .5) + self.transAxes)\n",
        "            return {'polar': spine}\n",
        "\n",
        "    register_projection(RadarAxes)\n",
        "    return theta\n",
        "\n",
        "\n",
        "# 7.5 Regional performance comparison using radar charts\n",
        "print(\"\\n7.5 Regional performance comparison using radar charts\")\n",
        "\n",
        "try:\n",
        "    # Define the improved radar chart function with the fixes for complete polygons\n",
        "    def plot_comparison_radar_chart(data, region_pairs, metrics, title, filename,\n",
        "                                   colors=None):\n",
        "        \"\"\"\n",
        "        Create a sophisticated radar chart comparing paired regions on key metrics.\n",
        "\n",
        "        Parameters:\n",
        "        data (DataFrame): DataFrame with metrics as columns and regions as index\n",
        "        region_pairs (list): List of tuples containing region pairs to compare\n",
        "        metrics (list): List of metrics to include in the radar chart\n",
        "        title (str): Title for the chart\n",
        "        filename (str): Base path to save the chart (will add suffixes for multiple charts)\n",
        "        colors (list): Optional list of colors for regions\n",
        "        \"\"\"\n",
        "        # Default colors if not provided\n",
        "        if colors is None:\n",
        "            colors = ['#FF5A5F', '#00A699', '#FC642D', '#4D5156', '#767676', '#484848']\n",
        "\n",
        "        # Number of metrics\n",
        "        N = len(metrics)\n",
        "\n",
        "        # Create angle for each metric\n",
        "        angles = [n / N * 2 * np.pi for n in range(N)]\n",
        "        angles += angles[:1]  # Close the loop\n",
        "\n",
        "        chart_filenames = []\n",
        "\n",
        "        # Create a chart for each pair of regions\n",
        "        for chart_idx, regions in enumerate(region_pairs):\n",
        "            # Create figure\n",
        "            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
        "\n",
        "            # Set background color to light gray\n",
        "            ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "            # Add concentric circles for reference\n",
        "            for level in np.linspace(0, 1, 5):\n",
        "                circle = plt.Circle((0, 0), level, transform=ax.transData._b,\n",
        "                                  fill=False, color='gray', alpha=0.3, linewidth=0.5)\n",
        "                ax.add_artist(circle)\n",
        "\n",
        "            # Ensure all regions have data for all metrics\n",
        "            metrics_data = data.loc[regions, metrics].copy()\n",
        "\n",
        "            # Fill any missing values with 0 to ensure polygons are complete\n",
        "            metrics_data = metrics_data.fillna(0)\n",
        "\n",
        "            # Ensure all values are at least slightly positive to create visible polygons\n",
        "            min_value = 0.01  # Small positive value to ensure visibility\n",
        "            for metric in metrics:\n",
        "                if metrics_data[metric].min() <= 0:\n",
        "                    metrics_data.loc[metrics_data[metric] <= 0, metric] = min_value\n",
        "\n",
        "            # Normalize data for better visualization\n",
        "            scaler = MinMaxScaler()\n",
        "            metrics_data_scaled = pd.DataFrame(\n",
        "                scaler.fit_transform(metrics_data),\n",
        "                index=metrics_data.index,\n",
        "                columns=metrics_data.columns\n",
        "            )\n",
        "\n",
        "            # Ensure minimum scaled value is at least 0.05 for visibility\n",
        "            metrics_data_scaled = metrics_data_scaled.clip(lower=0.05)\n",
        "\n",
        "            # Store max values for reference\n",
        "            max_values = metrics_data.max()\n",
        "\n",
        "            # Plot each region\n",
        "            for i, region in enumerate(regions):\n",
        "                # Get values for this region and close the loop\n",
        "                values = metrics_data_scaled.loc[region].values.tolist()\n",
        "                values += values[:1]\n",
        "\n",
        "                # Plot values\n",
        "                ax.plot(angles, values, linewidth=2.5, linestyle='-', color=colors[i], label=region)\n",
        "                ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
        "\n",
        "            # Fix axis to go in the right order and start at 12 o'clock\n",
        "            ax.set_theta_offset(np.pi / 2)\n",
        "            ax.set_theta_direction(-1)\n",
        "\n",
        "            # Draw axis lines for each angle and label\n",
        "            ax.set_xticks(angles[:-1])\n",
        "            ax.set_xticklabels(metrics, fontsize=12)\n",
        "\n",
        "            # Add metric values at each tick\n",
        "            for i, angle in enumerate(angles[:-1]):\n",
        "                for j, level in enumerate(np.linspace(0.2, 1, 5)):\n",
        "                    ax.text(angle, level, f\"{max_values[metrics[i]]*level:.2f}\",\n",
        "                           ha='center', va='center', fontsize=8, color='gray', alpha=0.7)\n",
        "\n",
        "            # Remove radial labels and set grid color to light gray\n",
        "            ax.set_yticklabels([])\n",
        "            ax.grid(color='lightgray', alpha=0.7)\n",
        "\n",
        "            # Set y limits\n",
        "            ax.set_ylim(0, 1.05)\n",
        "\n",
        "            # Add title with region pair description\n",
        "            pair_description = f\"{regions[0]} vs {regions[1]}\"\n",
        "            plt.title(f\"{title} - {pair_description}\", fontsize=16, pad=20)\n",
        "\n",
        "            # Add legend with region names in their respective colors\n",
        "            legend = ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12)\n",
        "            for i, text in enumerate(legend.get_texts()):\n",
        "                text.set_color(colors[i])\n",
        "                text.set_fontweight('bold')\n",
        "\n",
        "            # Add a note about normalization\n",
        "            plt.figtext(0.5, 0.01, \"Values normalized to 0-1 scale for visualization\",\n",
        "                       ha='center', fontsize=10, style='italic')\n",
        "\n",
        "            # Save figure with chart index\n",
        "            chart_filename = filename.replace('.png', f'_{regions[0]}_vs_{regions[1]}.png')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(chart_filename, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            chart_filenames.append(chart_filename)\n",
        "\n",
        "            # Create a table with actual values for this group of regions\n",
        "            fig, ax = plt.subplots(figsize=(12, len(regions) + 2))\n",
        "            ax.axis('off')\n",
        "\n",
        "            table_data = []\n",
        "            for region in regions:\n",
        "                row = [region] + [f\"{data.loc[region, metric]:.2f}\" for metric in metrics]\n",
        "                table_data.append(row)\n",
        "\n",
        "            table = ax.table(\n",
        "                cellText=table_data,\n",
        "                colLabels=[\"Region\"] + metrics,\n",
        "                loc='center',\n",
        "                cellLoc='center'\n",
        "            )\n",
        "            table.auto_set_font_size(False)\n",
        "            table.set_fontsize(10)\n",
        "            table.scale(1.2, 1.5)\n",
        "\n",
        "            # Color the region names in the table\n",
        "            for i, region in enumerate(regions):\n",
        "                table[(i+1, 0)].set_text_props(color=colors[i], weight='bold')\n",
        "\n",
        "            plt.title(f\"{title} - Data Table: {pair_description}\", fontsize=14)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save table figure\n",
        "            table_filename = filename.replace('.png', f'_table_{regions[0]}_vs_{regions[1]}.png')\n",
        "            plt.savefig(table_filename, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        return chart_filenames\n",
        "    region_revenue = df.groupby('Region').agg({\n",
        "        'Sales Revenue (USD)': ['mean', 'sum'],\n",
        "        'Units Sold': ['mean', 'sum'],\n",
        "        'Revenue_per_Unit': 'mean',\n",
        "        'Discount Percentage': 'mean',\n",
        "        'Marketing Spend (USD)': 'sum',\n",
        "        'Marketing_ROI': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Flatten multi-index columns after aggregation\n",
        "    region_revenue.columns = ['_'.join(col).strip() if col[1] else col[0] for col in region_revenue.columns]\n",
        "\n",
        "    # Ensure 'Region' is set as the index for the radar chart function's data input\n",
        "    region_metrics = region_revenue.set_index('Region')\n",
        "    # Select key metrics for comparison\n",
        "    key_metrics = [\n",
        "        'Sales Revenue (USD)_mean',\n",
        "        'Units Sold_mean',\n",
        "        'Marketing_ROI_mean',\n",
        "        'Discount Percentage_mean',\n",
        "        'Holiday_Impact'  # This would be the ratio of holiday to non-holiday sales\n",
        "    ]\n",
        "\n",
        "    # Ensure all these metrics exist in the region_metrics dataframe\n",
        "    for metric in key_metrics:\n",
        "        if metric not in region_metrics.columns and metric == 'Holiday_Impact':\n",
        "            # Calculate holiday impact if not already done\n",
        "            region_metrics['Holiday_Impact'] = region_holiday_revenue['Holiday_Impact']\n",
        "\n",
        "    # Define logical region pairs for comparison\n",
        "    region_pairs = [\n",
        "        ('North America', 'Europe'),\n",
        "        ('Asia', 'Africa'),\n",
        "        ('Middle East & North Africa', 'Caribbean'),\n",
        "        ('Oceania', 'Latin America')\n",
        "    ]\n",
        "\n",
        "    # Filter pairs to only include regions that exist in the data\n",
        "    available_regions = set(region_metrics.index)\n",
        "    valid_region_pairs = []\n",
        "    for pair in region_pairs:\n",
        "        if pair[0] in available_regions and pair[1] in available_regions:\n",
        "            valid_region_pairs.append(pair)\n",
        "        else:\n",
        "            print(f\"Warning: One or both regions in pair {pair} not found in data\")\n",
        "\n",
        "    # Define colors for regions\n",
        "    region_colors = ['#FF5A5F', '#00A699', '#FC642D', '#4D5156', '#767676', '#484848']\n",
        "\n",
        "    # Create comparison radar charts\n",
        "    radar_chart_files = plot_comparison_radar_chart(\n",
        "        data=region_metrics,\n",
        "        region_pairs=valid_region_pairs,\n",
        "        metrics=key_metrics,\n",
        "        title=\"Regional Performance Comparison\",\n",
        "        filename=\"/content/geographical_analysis/plots/region_comparison_radar.png\",\n",
        "        colors=region_colors\n",
        "    )\n",
        "\n",
        "    print(f\"Created {len(radar_chart_files)} regional comparison radar charts\")\n",
        "\n",
        "    # Add radar chart insights to report\n",
        "    with open('/content/geographical_analysis/geographical_insights.md', 'a') as f:\n",
        "        f.write(\"\\n## Regional Performance Comparison\\n\\n\")\n",
        "        f.write(\"The radar charts reveal distinct performance patterns across paired regions:\\n\\n\")\n",
        "\n",
        "        # Add insights for each region pair\n",
        "        for pair in valid_region_pairs:\n",
        "            f.write(f\"### {pair[0]} vs {pair[1]}\\n\")\n",
        "            f.write(f\"- **Sales Revenue**: Comparison of average transaction values\\n\")\n",
        "            f.write(f\"- **Units Sold**: Comparison of purchase volume patterns\\n\")\n",
        "            f.write(f\"- **Marketing ROI**: Differences in marketing effectiveness\\n\")\n",
        "            f.write(f\"- **Discount Sensitivity**: Variation in price sensitivity\\n\")\n",
        "            f.write(f\"- **Holiday Impact**: Contrast in seasonal shopping patterns\\n\\n\")\n",
        "\n",
        "        f.write(\"These regional comparisons highlight how cultural, economic, and geographical factors influence retail performance patterns, suggesting opportunities for cross-regional strategy adaptation.\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating regional comparison radar charts: {e}\")\n",
        "\n",
        "# 8. Regional Opportunity Analysis\n",
        "print(\"\\n8. Performing regional opportunity analysis...\")\n",
        "\n",
        "# 8.1 Regional performance gap analysis\n",
        "print(\"\\n8.1 Regional performance gap analysis\")\n",
        "\n",
        "# Calculate performance metrics relative to top region\n",
        "top_region = regional_sales.iloc[0]['Region']\n",
        "regional_sales['Revenue_Gap_to_Top'] = regional_sales['Sales Revenue (USD)_sum'].max() - regional_sales['Sales Revenue (USD)_sum']\n",
        "regional_sales['Revenue_Gap_Percentage'] = (regional_sales['Revenue_Gap_to_Top'] / regional_sales['Sales Revenue (USD)_sum']) * 100\n",
        "regional_sales['ROI_Gap_to_Top'] = regional_sales['Marketing_ROI_mean'].max() - regional_sales['Marketing_ROI_mean']\n",
        "regional_sales['ROI_Gap_Percentage'] = (regional_sales['ROI_Gap_to_Top'] / regional_sales['Marketing_ROI_mean']) * 100\n",
        "\n",
        "print(\"Regional performance gaps:\")\n",
        "print(regional_sales[['Region', 'Revenue_Gap_to_Top', 'Revenue_Gap_Percentage', 'ROI_Gap_to_Top', 'ROI_Gap_Percentage']])\n",
        "regional_sales.to_csv('/content/geographical_analysis/data/regional_performance_gaps.csv', index=False)\n",
        "\n",
        "# Plot revenue gap by region\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(regional_sales['Region'], regional_sales['Revenue_Gap_Percentage'])\n",
        "plt.title('Revenue Gap to Top Region (%)')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Revenue Gap (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/geographical_analysis/plots/regional_revenue_gap.png')\n",
        "plt.close()\n",
        "\n",
        "# 8.2 Regional improvement opportunities\n",
        "print(\"\\n8.2 Regional improvement opportunities\")\n",
        "\n",
        "# Identify improvement opportunities for each region\n",
        "region_opportunities = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    if region == top_region:\n",
        "        continue\n",
        "\n",
        "    region_data = regional_sales[regional_sales['Region'] == region].iloc[0]\n",
        "\n",
        "    # Calculate potential revenue increase if gap is closed by 50%\n",
        "    potential_increase = region_data['Revenue_Gap_to_Top'] * 0.5\n",
        "\n",
        "    # Identify top category opportunity\n",
        "    region_category_data = region_category[region_category['Region'] == region]\n",
        "    top_category_opportunity = region_category_data.loc[region_category_data['Sales Revenue (USD)_sum'].idxmax()]\n",
        "\n",
        "    # Identify optimal discount and marketing strategies\n",
        "    discount_strategy = region_optimal_discount[region_optimal_discount['Region'] == region].iloc[0]\n",
        "    marketing_strategy = region_optimal_marketing[region_optimal_marketing['Region'] == region].iloc[0]\n",
        "\n",
        "    # Add to dataframe\n",
        "    region_opportunities = pd.concat([region_opportunities, pd.DataFrame({\n",
        "        'Region': [region],\n",
        "        'Current Revenue': [region_data['Sales Revenue (USD)_sum']],\n",
        "        'Revenue Gap to Top (%)': [region_data['Revenue_Gap_Percentage']],\n",
        "        'Potential Revenue Increase (50% Gap Closure)': [potential_increase],\n",
        "        'Top Category': [top_category_opportunity['Product Category']],\n",
        "        'Optimal Discount Level': [discount_strategy['Optimal Discount Level']],\n",
        "        'Optimal Marketing Spend': [marketing_strategy['Optimal Marketing Bin']]\n",
        "    })])\n",
        "\n",
        "print(\"Regional improvement opportunities:\")\n",
        "print(region_opportunities)\n",
        "region_opportunities.to_csv('/content/geographical_analysis/data/regional_opportunities.csv', index=False)\n",
        "\n",
        "# 8.3 Store location improvement opportunities\n",
        "print(\"\\n8.3 Store location improvement opportunities\")\n",
        "\n",
        "# Identify bottom performing locations in each region\n",
        "bottom_locations_by_region = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_locations = location_sales[location_sales['Region'] == region].sort_values('Sales Revenue (USD)_sum')\n",
        "\n",
        "    # Take bottom 3 locations or fewer if region has less than 3 locations\n",
        "    num_locations = min(3, len(region_locations))\n",
        "    if num_locations > 0:\n",
        "        bottom_locations = region_locations.head(num_locations)\n",
        "\n",
        "        # Add to dataframe\n",
        "        bottom_locations_by_region = pd.concat([bottom_locations_by_region, bottom_locations])\n",
        "\n",
        "# Calculate potential improvement\n",
        "region_avg_revenue = location_sales.groupby('Region')['Sales Revenue (USD)_sum'].mean().reset_index()\n",
        "region_avg_revenue = region_avg_revenue.rename(columns={'Sales Revenue (USD)_sum': 'Region_Avg_Revenue'})\n",
        "\n",
        "bottom_locations_by_region = bottom_locations_by_region.merge(region_avg_revenue, on='Region')\n",
        "bottom_locations_by_region['Revenue_Gap_to_Region_Avg'] = bottom_locations_by_region['Region_Avg_Revenue'] - bottom_locations_by_region['Sales Revenue (USD)_sum']\n",
        "bottom_locations_by_region['Potential_Improvement'] = bottom_locations_by_region['Revenue_Gap_to_Region_Avg'] * 0.5\n",
        "\n",
        "print(\"Bottom performing locations by region:\")\n",
        "print(bottom_locations_by_region[['Store Location', 'Region', 'Sales Revenue (USD)_sum', 'Revenue_Gap_to_Region_Avg', 'Potential_Improvement']])\n",
        "bottom_locations_by_region.to_csv('/content/geographical_analysis/data/bottom_locations_by_region.csv', index=False)\n",
        "\n",
        "# 9. Generate comprehensive insights report\n",
        "print(\"\\n9. Generating comprehensive insights report...\")\n",
        "\n",
        "with open('/content/geographical_analysis/geographical_regional_insights.md', 'w') as f:\n",
        "    f.write(\"# Geographical and Regional Performance Analysis Insights\\n\\n\")\n",
        "\n",
        "    f.write(\"## 1. Regional Performance Overview\\n\\n\")\n",
        "\n",
        "    # Top and bottom regions\n",
        "    top_region_data = regional_sales.iloc[0]\n",
        "    bottom_region_data = regional_sales.iloc[-1]\n",
        "\n",
        "    f.write(f\"### Regional Performance Variation\\n\")\n",
        "    f.write(f\"- **Top performing region**: {top_region_data['Region']} with ${top_region_data['Sales Revenue (USD)_sum']:,.0f} in total sales\\n\")\n",
        "    f.write(f\"- **Bottom performing region**: {bottom_region_data['Region']} with ${bottom_region_data['Sales Revenue (USD)_sum']:,.0f} in total sales\\n\")\n",
        "    f.write(f\"- **Performance gap**: {(top_region_data['Sales Revenue (USD)_sum'] / bottom_region_data['Sales Revenue (USD)_sum']):.1f}x difference between top and bottom regions\\n\\n\")\n",
        "\n",
        "    f.write(\"### Statistical Significance\\n\")\n",
        "    if anova_table.iloc[0, 3] < 0.05:\n",
        "        f.write(\"- Regional differences in sales are **statistically significant** (p < 0.05)\\n\")\n",
        "        f.write(\"- This confirms that the observed regional variations are not due to random chance\\n\")\n",
        "    else:\n",
        "        f.write(\"- Regional differences in sales are not statistically significant (p > 0.05)\\n\")\n",
        "        f.write(\"- This suggests that observed regional variations may be due to random fluctuations\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"### Key Regional Metrics\\n\")\n",
        "    f.write(\"| Region | Total Revenue | Avg Transaction | Marketing ROI | Units per Transaction |\\n\")\n",
        "    f.write(\"|--------|---------------|----------------|--------------|----------------------|\\n\")\n",
        "\n",
        "    for _, row in regional_sales.head(5).iterrows():\n",
        "        f.write(f\"| {row['Region']} | ${row['Sales Revenue (USD)_sum']:,.0f} | ${row['Revenue_per_Transaction']:,.2f} | {row['Marketing_ROI_mean']:.2f} | {row['Units_per_Transaction']:.2f} |\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## 2. Store Location Analysis\\n\\n\")\n",
        "\n",
        "    f.write(f\"### Location Performance Distribution\\n\")\n",
        "    f.write(f\"- **Total store locations**: {len(location_sales)}\\n\")\n",
        "    f.write(f\"- **Top performing location**: {location_sales.iloc[0]['Store Location']} (${location_sales.iloc[0]['Sales Revenue (USD)_sum']:,.0f})\\n\")\n",
        "    f.write(f\"- **Bottom performing location**: {location_sales.iloc[-1]['Store Location']} (${location_sales.iloc[-1]['Sales Revenue (USD)_sum']:,.0f})\\n\")\n",
        "    f.write(f\"- **Performance ratio**: {(location_sales.iloc[0]['Sales Revenue (USD)_sum'] / location_sales.iloc[-1]['Sales Revenue (USD)_sum']):.1f}x difference between top and bottom locations\\n\\n\")\n",
        "\n",
        "    f.write(\"### Location Clustering Insights\\n\")\n",
        "    f.write(f\"- **Number of identified location clusters**: {optimal_k}\\n\")\n",
        "\n",
        "    for cluster_idx in range(optimal_k):\n",
        "        cluster_data = cluster_stats.loc[cluster_idx]\n",
        "        f.write(f\"- **Cluster {cluster_idx}** ({cluster_data['Number_of_Locations']} locations): \")\n",
        "\n",
        "        # Identify key characteristics of this cluster\n",
        "        if cluster_data['Sales Revenue (USD)_mean'] > cluster_stats['Sales Revenue (USD)_mean'].mean():\n",
        "            f.write(\"High revenue, \")\n",
        "        else:\n",
        "            f.write(\"Low revenue, \")\n",
        "\n",
        "        if cluster_data['Marketing_ROI_mean'] > cluster_stats['Marketing_ROI_mean'].mean():\n",
        "            f.write(\"high marketing ROI, \")\n",
        "        else:\n",
        "            f.write(\"low marketing ROI, \")\n",
        "\n",
        "        if cluster_data['Discount Percentage_mean'] > cluster_stats['Discount Percentage_mean'].mean():\n",
        "            f.write(\"high discount usage\\n\")\n",
        "        else:\n",
        "            f.write(\"low discount usage\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## 3. Regional Product Category Analysis\\n\\n\")\n",
        "\n",
        "    f.write(\"### Regional Category Preferences\\n\")\n",
        "    for region in region_category_preference_pivot.index:\n",
        "        # Find categories with preference index > 1.2 (20% higher than average)\n",
        "        strong_preferences = region_category_preference_pivot.loc[region][region_category_preference_pivot.loc[region] > 1.2]\n",
        "        if len(strong_preferences) > 0:\n",
        "            categories = \", \".join([f\"{cat} ({val:.2f}x)\" for cat, val in strong_preferences.items()])\n",
        "            f.write(f\"- **{region}**: Strong preference for {categories}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"### Top Category by Region\\n\")\n",
        "    f.write(\"| Region | Top Category | Sales Revenue |\\n\")\n",
        "    f.write(\"|--------|--------------|---------------|\\n\")\n",
        "\n",
        "    for _, row in top_category_by_region.iterrows():\n",
        "        f.write(f\"| {row['Region']} | {row['Top Category']} | ${row['Sales Revenue (USD)']:,.0f} |\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## 4. Regional Temporal Patterns\\n\\n\")\n",
        "\n",
        "    f.write(\"### Quarterly Performance\\n\")\n",
        "    f.write(\"- **Quarterly patterns**: Most regions show distinct quarterly sales patterns\\n\")\n",
        "\n",
        "    # Identify regions with strongest and weakest seasonality\n",
        "    region_quarter_std = region_quarter_revenue.std(axis=1) / region_quarter_revenue.mean(axis=1)\n",
        "    strongest_seasonality = region_quarter_std.idxmax()\n",
        "    weakest_seasonality = region_quarter_std.idxmin()\n",
        "\n",
        "    f.write(f\"- **Strongest seasonality**: {strongest_seasonality} (coefficient of variation: {region_quarter_std[strongest_seasonality]:.2f})\\n\")\n",
        "    f.write(f\"- **Weakest seasonality**: {weakest_seasonality} (coefficient of variation: {region_quarter_std[weakest_seasonality]:.2f})\\n\\n\")\n",
        "\n",
        "    f.write(\"### Regional Growth Patterns\\n\")\n",
        "    try:\n",
        "        # Find regions with consistent growth\n",
        "        consistent_growth = []\n",
        "        for region in region_quarter_growth_pivot.index:\n",
        "            growth_rates = region_quarter_growth_pivot.loc[region].dropna()\n",
        "            if (growth_rates > 0).all():\n",
        "                consistent_growth.append(region)\n",
        "\n",
        "        if consistent_growth:\n",
        "            f.write(f\"- **Regions with consistent quarterly growth**: {', '.join(consistent_growth)}\\n\")\n",
        "        else:\n",
        "            f.write(\"- No regions showed consistent quarterly growth across all quarters\\n\")\n",
        "    except:\n",
        "        f.write(\"- Growth pattern analysis not available\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"### Holiday Effect\\n\")\n",
        "    f.write(\"- **Holiday impact**: All regions show increased sales during holidays\\n\")\n",
        "\n",
        "    # Regions with strongest and weakest holiday effect\n",
        "    strongest_holiday = region_holiday_revenue['Holiday_Impact'].idxmax()\n",
        "    weakest_holiday = region_holiday_revenue['Holiday_Impact'].idxmin()\n",
        "\n",
        "    f.write(f\"- **Strongest holiday effect**: {strongest_holiday} ({region_holiday_revenue.loc[strongest_holiday, 'Holiday_Impact']:.2f}x increase)\\n\")\n",
        "    f.write(f\"- **Weakest holiday effect**: {weakest_holiday} ({region_holiday_revenue.loc[weakest_holiday, 'Holiday_Impact']:.2f}x increase)\\n\\n\")\n",
        "\n",
        "    f.write(\"## 5. Regional Discount and Marketing Effectiveness\\n\\n\")\n",
        "\n",
        "    f.write(\"### Discount Sensitivity\\n\")\n",
        "    f.write(\"| Region | Optimal Discount Level | Avg Revenue at Optimal Discount |\\n\")\n",
        "    f.write(\"|--------|------------------------|--------------------------------|\\n\")\n",
        "\n",
        "    for _, row in region_optimal_discount.iterrows():\n",
        "        f.write(f\"| {row['Region']} | {row['Optimal Discount Level']} | ${row['Average Revenue']:,.2f} |\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"### Marketing Effectiveness\\n\")\n",
        "    f.write(\"| Region | Optimal Marketing Spend | Marketing ROI |\\n\")\n",
        "    f.write(\"|--------|-------------------------|---------------|\\n\")\n",
        "\n",
        "    for _, row in region_optimal_marketing.iterrows():\n",
        "        f.write(f\"| {row['Region']} | {row['Optimal Marketing Bin']} | {row['Marketing ROI']:.2f} |\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## 6. Regional Improvement Opportunities\\n\\n\")\n",
        "\n",
        "    f.write(\"### Revenue Gap Analysis\\n\")\n",
        "    f.write(f\"- **Benchmark region**: {top_region} (${top_region_data['Sales Revenue (USD)_sum']:,.0f})\\n\")\n",
        "\n",
        "    # Calculate total potential improvement\n",
        "    total_potential = region_opportunities['Potential Revenue Increase (50% Gap Closure)'].sum()\n",
        "\n",
        "    f.write(f\"- **Total potential revenue increase**: ${total_potential:,.0f} (if all regions close 50% of gap to top performer)\\n\\n\")\n",
        "\n",
        "    f.write(\"### Priority Regions for Improvement\\n\")\n",
        "    f.write(\"| Region | Current Revenue | Gap to Top (%) | Potential Increase | Recommended Strategy |\\n\")\n",
        "    f.write(\"|--------|----------------|---------------|-------------------|----------------------|\\n\")\n",
        "\n",
        "    for _, row in region_opportunities.sort_values('Potential Revenue Increase (50% Gap Closure)', ascending=False).head(3).iterrows():\n",
        "        f.write(f\"| {row['Region']} | ${row['Current Revenue']:,.0f} | {row['Revenue Gap to Top (%)']:.1f}% | ${row['Potential Revenue Increase (50% Gap Closure)']:,.0f} | Focus on {row['Top Category']}, {row['Optimal Discount Level']} discounts, {row['Optimal Marketing Spend']} marketing |\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"### Underperforming Locations\\n\")\n",
        "    f.write(\"- **Number of identified underperforming locations**: {}\\n\".format(len(bottom_locations_by_region)))\n",
        "    f.write(\"- **Total potential improvement from location optimization**: ${:,.0f}\\n\\n\".format(bottom_locations_by_region['Potential_Improvement'].sum()))\n",
        "\n",
        "    f.write(\"## 7. Key Takeaways and Recommendations\\n\\n\")\n",
        "\n",
        "    f.write(\"### Regional Strategy Recommendations\\n\")\n",
        "    f.write(\"1. **Regional Customization**: Implement region-specific strategies based on identified preferences and sensitivities\\n\")\n",
        "    f.write(\"2. **Category Focus**: Align product assortment with regional preferences, especially for categories with preference index > 1.2\\n\")\n",
        "    f.write(\"3. **Discount Optimization**: Adjust discount levels by region according to optimal discount analysis\\n\")\n",
        "    f.write(\"4. **Marketing Allocation**: Reallocate marketing budget based on regional ROI, focusing on regions with highest marketing effectiveness\\n\")\n",
        "    f.write(\"5. **Seasonal Planning**: Develop region-specific seasonal strategies, with particular attention to regions with strong quarterly patterns\\n\\n\")\n",
        "\n",
        "    f.write(\"### Location-Specific Recommendations\\n\")\n",
        "    f.write(\"1. **Cluster-Based Approach**: Develop distinct strategies for each location cluster identified in the analysis\\n\")\n",
        "    f.write(\"2. **Performance Gap Closure**: Focus on bringing underperforming locations closer to their regional averages\\n\")\n",
        "    f.write(\"3. **Best Practice Transfer**: Identify successful practices from top-performing locations and implement in similar locations\\n\")\n",
        "    f.write(\"4. **Location-Specific Targets**: Set realistic improvement targets based on regional benchmarks rather than global averages\\n\\n\")\n",
        "\n",
        "    f.write(\"### Implementation Priorities\\n\")\n",
        "    f.write(\"1. **Quick Wins**: Implement optimal discount strategies in high-potential regions first\\n\")\n",
        "    f.write(\"2. **Medium-Term Focus**: Address underperforming locations with targeted interventions\\n\")\n",
        "    f.write(\"3. **Long-Term Strategy**: Develop comprehensive regional strategies that account for all identified patterns and preferences\\n\")\n",
        "    f.write(\"4. **Continuous Monitoring**: Establish regional KPIs and regularly track performance against benchmarks\\n\\n\")\n",
        "\n",
        "    f.write(\"## 8. Limitations and Future Analysis\\n\\n\")\n",
        "    f.write(\"- **Granularity**: This analysis is at the regional and location level; customer-level analysis would provide deeper insights\\n\")\n",
        "    f.write(\"- **External Factors**: The analysis does not account for external factors like local competition or economic conditions\\n\")\n",
        "    f.write(\"- **Causality**: While patterns are identified, causal relationships require further investigation\\n\")\n",
        "    f.write(\"- **Future Work**: Consider incorporating demographic data, competitive analysis, and more granular temporal patterns\\n\")\n",
        "\n",
        "print(\"Comprehensive insights report generated and saved to /content/geographical_analysis/geographical_regional_insights.md\")\n",
        "\n",
        "# 10. Print completion message\n",
        "print(\"\\nStep 5: Geographical and Regional Performance Analysis completed successfully!\")\n",
        "print(\"The analysis results are ready for review and further steps.\")\n",
        "print(\"\\nNext steps: Proceed to Step 6 - Develop profitability and ROI analysis\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOS0GjUMneam",
        "outputId": "25604f80-b23f-45ee-b02c-5df109d49f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Loading and preparing data...\n",
            "Created 'Marketing_Bin' column.\n",
            "\n",
            "2. Analyzing category revenue performance...\n",
            "\n",
            "2.1 Category revenue metrics\n",
            "  Product Category  Sales Revenue (USD)_mean  Sales Revenue (USD)_sum  Sales Revenue (USD)_count  Units Sold_mean  Units Sold_sum  Revenue_per_Unit_mean  Discount Percentage_mean  Marketing Spend (USD)_sum  Marketing_ROI_mean\n",
            "1      Electronics                   3432.60              27601540.55                       8041             6.90           55458                 506.57                      2.93                     405997               51.14\n",
            "2        Furniture                   2354.53              22375061.09                       9503             6.17           58626                 383.10                      2.99                     471330               32.35\n",
            "0         Clothing                   2953.53              19516928.04                       6608             5.82           38450                 511.27                      3.01                     326019               46.95\n",
            "3        Groceries                   1744.96              10204540.21                       5848             5.20           30397                 334.07                      2.97                     294975               24.43\n",
            "\n",
            "3. Analyzing regional revenue performance...\n",
            "\n",
            "3.1 Regional revenue metrics\n",
            "                       Region  Sales Revenue (USD)_mean  Sales Revenue (USD)_sum  Sales Revenue (USD)_count  Units Sold_mean  Units Sold_sum  Revenue_per_Unit_mean  Discount Percentage_mean  Marketing Spend (USD)_sum  Marketing_ROI_mean\n",
            "3                      Europe                   2659.81              17764880.48                       6679             6.11           40797                 436.19                      3.06                     334284               38.22\n",
            "0                      Africa                   2655.00              16272478.52                       6129             6.10           37411                 432.44                      2.96                     301432               39.92\n",
            "1                        Asia                   2638.23              10626790.90                       4028             6.02           24248                 438.17                      2.90                     190393               39.20\n",
            "2                   Caribbean                   2694.39               8853750.54                       3286             6.14           20180                 433.92                      2.93                     164764               35.55\n",
            "7                     Oceania                   2642.89               8470447.86                       3205             6.14           19687                 432.73                      2.90                     168726               39.08\n",
            "5  Middle East & North Africa                   2709.65               7427148.97                       2741             6.13           16801                 440.61                      2.87                     144382               45.42\n",
            "4               Latin America                   2647.67               4506340.86                       1702             6.12           10420                 431.49                      3.20                      85557               36.16\n",
            "6               North America                   2542.40               3561898.51                       1401             5.94            8327                 430.37                      3.07                      66403               42.66\n",
            "8                       Other                   2671.09               2214333.25                        829             6.10            5060                 431.77                      2.90                      42380               31.30\n",
            "\n",
            "4. Performing optimization analysis...\n",
            "\n",
            "4.1 Finding most revenue-generating category for each region\n",
            "Revenue by region and category:\n",
            "Product Category             Clothing  Electronics  Furniture  Groceries\n",
            "Region                                                                  \n",
            "Africa                     4034455.78   5526743.26 4665018.97 2046260.50\n",
            "Asia                       2524735.20   3689660.34 3111324.93 1301070.43\n",
            "Caribbean                  2273095.94   2962246.23 2537436.98 1080971.40\n",
            "Europe                     4220912.42   6307367.93 4895887.98 2340712.15\n",
            "Latin America              1103593.50   1555464.96 1246767.32  600515.07\n",
            "Middle East & North Africa 1868418.37   2679100.20 1970898.25  908732.16\n",
            "North America               879104.96   1241896.51  907585.35  533311.69\n",
            "Oceania                    2048259.63   2877682.35 2382218.85 1162287.04\n",
            "Other                       564352.24    761378.78  657922.46  230679.77\n",
            "Top revenue-generating category by region:\n",
            "                       Region Top_Category    Revenue  Units_Sold  Revenue_per_Unit\n",
            "0                      Africa  Electronics 5526743.26       10998            510.47\n",
            "0                   Caribbean  Electronics 2962246.23        5874            505.38\n",
            "0                        Asia  Electronics 3689660.34        7298            511.14\n",
            "0                      Europe  Electronics 6307367.93       12654            509.11\n",
            "0               Latin America  Electronics 1555464.96        3163            492.14\n",
            "0                     Oceania  Electronics 2877682.35        5949            498.15\n",
            "0               North America  Electronics 1241896.51        2579            501.35\n",
            "0  Middle East & North Africa  Electronics 2679100.20        5416            506.39\n",
            "0                       Other  Electronics  761378.78        1527            511.97\n",
            "\n",
            "4.2 Finding optimal discount level by category\n",
            "Average revenue by category and discount level:\n",
            "Discount_Level      High     Low  Medium  Very Low\n",
            "Product Category                                  \n",
            "Clothing         2520.94 2778.11 2530.13   3001.33\n",
            "Electronics      2996.89 3135.04 3175.27   3415.57\n",
            "Furniture        1930.85 2125.60 2154.85   2267.04\n",
            "Groceries        1329.68 1669.08 1525.66   1641.09\n",
            "Optimal discount level by category:\n",
            "  Product_Category Optimal_Discount_Level  Optimal_Discount_Percentage  Average_Revenue  Average_Units_Sold\n",
            "0        Furniture               Very Low                         5.00          2267.04                6.17\n",
            "0      Electronics               Very Low                         5.00          3415.57                6.94\n",
            "0        Groceries                    Low                        10.00          1669.08                5.20\n",
            "0         Clothing               Very Low                         5.00          3001.33                5.77\n",
            "\n",
            "4.3 Finding optimal marketing spend by category\n",
            "Average revenue by category and marketing spend:\n",
            "Marketing_Bin       0-50  50-100  100-150  150-200  200+\n",
            "Product Category                                        \n",
            "Clothing         2913.05 3088.48  2922.07  3062.06   NaN\n",
            "Electronics      3457.56 3408.33  3443.53  3323.06   NaN\n",
            "Furniture        2351.70 2291.01  2454.72  2334.39   NaN\n",
            "Groceries        1776.08 1734.92  1692.81  1655.98   NaN\n",
            "Optimal marketing spend by category:\n",
            "  Product_Category Optimal_Marketing_Bin  Optimal_Marketing_Spend  Average_Revenue  Marketing_ROI\n",
            "0        Furniture                  0-50                     4.77          2351.70          39.22\n",
            "0      Electronics                  0-50                     5.14          3457.56          63.73\n",
            "0        Groceries                  0-50                     4.79          1776.08          30.31\n",
            "0         Clothing                  0-50                     5.09          2913.05          58.73\n",
            "\n",
            "4.4 Developing regional optimization strategies\n",
            "Optimal discount level by region:\n",
            "                       Region Optimal_Discount_Level  Optimal_Discount_Percentage  Average_Revenue  Average_Units_Sold\n",
            "0                      Africa               Very Low                         5.00          2705.41                6.00\n",
            "0                   Caribbean                    Low                        10.00          2591.38                6.22\n",
            "0                        Asia               Very Low                         5.00          2750.53                6.27\n",
            "0                      Europe               Very Low                         5.00          2537.74                5.96\n",
            "0               Latin America               Very Low                         5.00          2810.10                6.06\n",
            "0                     Oceania               Very Low                         5.00          2615.27                6.19\n",
            "0               North America                    Low                        10.00          2994.62                6.25\n",
            "0  Middle East & North Africa                    Low                        10.00          2733.53                6.22\n",
            "0                       Other               Very Low                         5.00          2908.38                6.78\n",
            "Optimal marketing spend by region:\n",
            "                       Region Optimal_Marketing_Bin  Optimal_Marketing_Spend  Average_Revenue  Marketing_ROI\n",
            "0                      Africa                  0-50                     4.94          2682.62          49.71\n",
            "0                   Caribbean                  0-50                     4.98          2654.32          42.39\n",
            "0                        Asia                  0-50                     4.99          2612.13          47.54\n",
            "0                      Europe                  0-50                     4.98          2673.05          47.04\n",
            "0               Latin America                  0-50                     4.43          2614.55          43.80\n",
            "0                     Oceania                  0-50                     5.26          2691.84          49.79\n",
            "0               North America                  0-50                     4.59          2554.67          53.80\n",
            "0  Middle East & North Africa                  0-50                     4.91          2692.59          59.19\n",
            "0                       Other                50-100                    75.75          2826.99          37.30\n",
            "\n",
            "4.5 Creating combined optimization strategy\n",
            "Combined optimization strategy:\n",
            "                       Region Top_Category Optimal_Discount_Level Optimal_Marketing_Bin  Potential_Improvement  Improvement_Percentage\n",
            "6               North America  Electronics                    Low                  0-50                 452.22                   17.79\n",
            "8                       Other  Electronics               Very Low                50-100                 237.29                    8.88\n",
            "4               Latin America  Electronics               Very Low                  0-50                 162.43                    6.13\n",
            "2                        Asia  Electronics               Very Low                  0-50                 112.30                    4.26\n",
            "0                      Africa  Electronics               Very Low                  0-50                  50.41                    1.90\n",
            "7  Middle East & North Africa  Electronics                    Low                  0-50                  23.88                    0.88\n",
            "5                     Oceania  Electronics               Very Low                  0-50                 -27.61                   -1.04\n",
            "1                   Caribbean  Electronics                    Low                  0-50                -103.01                   -3.82\n",
            "3                      Europe  Electronics               Very Low                  0-50                -122.07                   -4.59\n",
            "\n",
            "5. Generating comprehensive insights report...\n",
            "Comprehensive insights report generated and saved to /content/revenue_optimization/revenue_optimization_insights.md\n",
            "\n",
            "Step 6: Revenue Optimization Analysis completed successfully!\n",
            "The analysis results are ready for review and further steps.\n",
            "\n",
            "Next steps: Proceed to Step 7 - Create interactive and comprehensive visualizations and dashboard\n"
          ]
        }
      ],
      "source": [
        "# Modified approach for Step 6 focusing on revenue optimization without cost assumptions\n",
        "\n",
        "\"\"\"\n",
        "Step 6: Revenue Optimization Analysis\n",
        "\n",
        "This script analyzes the retail sales data to identify optimal strategies for maximizing revenue\n",
        "across different dimensions (categories, regions, etc.) without making cost assumptions.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs('/content/revenue_optimization', exist_ok=True)\n",
        "os.makedirs('/content/revenue_optimization/data', exist_ok=True)\n",
        "os.makedirs('/content/revenue_optimization/plots', exist_ok=True)\n",
        "\n",
        "# 1. Load and prepare data\n",
        "print(\"\\n1. Loading and preparing data...\")\n",
        "\n",
        "# Load cleaned data\n",
        "df = pd.read_csv('/content/cleaned_data/retail_sales_cleaned.csv')\n",
        "\n",
        "# Ensure date column is datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Create marketing spend bins - This needs to be done AFTER loading the data\n",
        "df['Marketing_Bin'] = pd.cut(\n",
        "    df['Marketing Spend (USD)'],\n",
        "    bins=[0, 50, 100, 150, 200, float('inf')],\n",
        "    labels=['0-50', '50-100', '100-150', '150-200', '200+'],\n",
        "    right=False, # Include the left bin edge, exclude the right. E.g., 0-50 means >=0 and <50.\n",
        "    duplicates='drop' # Drop duplicates if bin edges are not unique (unlikely here but good practice)\n",
        ")\n",
        "print(\"Created 'Marketing_Bin' column.\")\n",
        "\n",
        "\n",
        "# 2. Category Revenue Analysis\n",
        "print(\"\\n2. Analyzing category revenue performance...\")\n",
        "\n",
        "# 2.1 Category revenue metrics\n",
        "print(\"\\n2.1 Category revenue metrics\")\n",
        "\n",
        "# Calculate revenue metrics by category\n",
        "category_revenue = df.groupby('Product Category').agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Revenue_per_Unit': 'mean',\n",
        "    'Discount Percentage': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum',\n",
        "    'Marketing_ROI': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten multi-index columns\n",
        "category_revenue.columns = ['_'.join(col).strip() if col[1] else col[0] for col in category_revenue.columns]\n",
        "\n",
        "# Sort by total revenue\n",
        "category_revenue = category_revenue.sort_values('Sales Revenue (USD)_sum', ascending=False)\n",
        "\n",
        "print(category_revenue)\n",
        "category_revenue.to_csv('/content/revenue_optimization/data/category_revenue.csv', index=False)\n",
        "\n",
        "# Plot category revenue\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(category_revenue['Product Category'], category_revenue['Sales Revenue (USD)_sum'])\n",
        "plt.title('Total Revenue by Product Category')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Total Revenue (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/revenue_optimization/plots/category_revenue.png')\n",
        "plt.close()\n",
        "\n",
        "# 3. Regional Revenue Analysis\n",
        "print(\"\\n3. Analyzing regional revenue performance...\")\n",
        "\n",
        "# 3.1 Regional revenue metrics\n",
        "print(\"\\n3.1 Regional revenue metrics\")\n",
        "\n",
        "# Calculate revenue metrics by region\n",
        "region_revenue = df.groupby('Region').agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum', 'count'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Revenue_per_Unit': 'mean',\n",
        "    'Discount Percentage': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum',\n",
        "    'Marketing_ROI': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten multi-index columns\n",
        "region_revenue.columns = ['_'.join(col).strip() if col[1] else col[0] for col in region_revenue.columns]\n",
        "\n",
        "# Sort by total revenue\n",
        "region_revenue = region_revenue.sort_values('Sales Revenue (USD)_sum', ascending=False)\n",
        "\n",
        "print(region_revenue)\n",
        "region_revenue.to_csv('/content/revenue_optimization/data/region_revenue.csv', index=False)\n",
        "\n",
        "# Plot region revenue\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(region_revenue['Region'], region_revenue['Sales Revenue (USD)_sum'])\n",
        "plt.title('Total Revenue by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Total Revenue (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/revenue_optimization/plots/region_revenue.png')\n",
        "plt.close()\n",
        "\n",
        "# 4. Optimization Analysis\n",
        "print(\"\\n4. Performing optimization analysis...\")\n",
        "\n",
        "# 4.1 Find most revenue-generating category for each region\n",
        "print(\"\\n4.1 Finding most revenue-generating category for each region\")\n",
        "\n",
        "# Calculate revenue by region and category\n",
        "region_category_revenue = df.groupby(['Region', 'Product Category']).agg({\n",
        "    'Sales Revenue (USD)': ['mean', 'sum'],\n",
        "    'Units Sold': ['mean', 'sum'],\n",
        "    'Revenue_per_Unit': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten multi-index columns\n",
        "region_category_revenue.columns = ['_'.join(col).strip() if col[1] else col[0] for col in region_category_revenue.columns]\n",
        "\n",
        "# Create pivot table for visualization\n",
        "region_category_revenue_pivot = region_category_revenue.pivot(\n",
        "    index='Region',\n",
        "    columns='Product Category',\n",
        "    values='Sales Revenue (USD)_sum'\n",
        ")\n",
        "\n",
        "print(\"Revenue by region and category:\")\n",
        "print(region_category_revenue_pivot)\n",
        "region_category_revenue_pivot.to_csv('/content/revenue_optimization/data/region_category_revenue.csv')\n",
        "\n",
        "# Plot heatmap of region-category revenue\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(region_category_revenue_pivot, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Total Revenue by Region and Product Category')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/revenue_optimization/plots/region_category_revenue_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Find top category for each region\n",
        "region_top_category = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = region_category_revenue[region_category_revenue['Region'] == region]\n",
        "    top_category = region_data.loc[region_data['Sales Revenue (USD)_sum'].idxmax()]\n",
        "\n",
        "    region_top_category = pd.concat([\n",
        "        region_top_category,\n",
        "        pd.DataFrame({\n",
        "            'Region': [region],\n",
        "            'Top_Category': [top_category['Product Category']],\n",
        "            'Revenue': [top_category['Sales Revenue (USD)_sum']],\n",
        "            'Units_Sold': [top_category['Units Sold_sum']],\n",
        "            'Revenue_per_Unit': [top_category['Revenue_per_Unit_mean']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Top revenue-generating category by region:\")\n",
        "print(region_top_category)\n",
        "region_top_category.to_csv('/content/revenue_optimization/data/region_top_category.csv', index=False)\n",
        "\n",
        "# 4.2 Optimal discount by category\n",
        "print(\"\\n4.2 Finding optimal discount level by category\")\n",
        "\n",
        "# Calculate revenue by category and discount level\n",
        "category_discount_revenue = df.groupby(['Product Category', 'Discount_Level']).agg({\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Units Sold': 'mean',\n",
        "    'Discount Percentage': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "category_discount_revenue_pivot = category_discount_revenue.pivot(\n",
        "    index='Product Category',\n",
        "    columns='Discount_Level',\n",
        "    values='Sales Revenue (USD)'\n",
        ")\n",
        "\n",
        "print(\"Average revenue by category and discount level:\")\n",
        "print(category_discount_revenue_pivot)\n",
        "category_discount_revenue_pivot.to_csv('/content/revenue_optimization/data/category_discount_revenue.csv')\n",
        "\n",
        "# Plot heatmap of category-discount revenue\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(category_discount_revenue_pivot, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Average Revenue by Product Category and Discount Level')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/revenue_optimization/plots/category_discount_revenue_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Find optimal discount level for each category\n",
        "category_optimal_discount = pd.DataFrame()\n",
        "\n",
        "for category in df['Product Category'].unique():\n",
        "    category_data = category_discount_revenue[category_discount_revenue['Product Category'] == category]\n",
        "    optimal_discount = category_data.loc[category_data['Sales Revenue (USD)'].idxmax()]\n",
        "\n",
        "    category_optimal_discount = pd.concat([\n",
        "        category_optimal_discount,\n",
        "        pd.DataFrame({\n",
        "            'Product_Category': [category],\n",
        "            'Optimal_Discount_Level': [optimal_discount['Discount_Level']],\n",
        "            'Optimal_Discount_Percentage': [optimal_discount['Discount Percentage']],\n",
        "            'Average_Revenue': [optimal_discount['Sales Revenue (USD)']],\n",
        "            'Average_Units_Sold': [optimal_discount['Units Sold']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Optimal discount level by category:\")\n",
        "print(category_optimal_discount)\n",
        "category_optimal_discount.to_csv('/content/revenue_optimization/data/category_optimal_discount.csv', index=False)\n",
        "\n",
        "# 4.3 Optimal marketing spend by category\n",
        "print(\"\\n4.3 Finding optimal marketing spend by category\")\n",
        "\n",
        "# Calculate revenue by category and marketing spend bin\n",
        "category_marketing_revenue = df.groupby(['Product Category', 'Marketing_Bin']).agg({\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Units Sold': 'mean',\n",
        "    'Marketing Spend (USD)': 'mean',\n",
        "    'Marketing_ROI': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Create pivot table for visualization\n",
        "category_marketing_revenue_pivot = category_marketing_revenue.pivot(\n",
        "    index='Product Category',\n",
        "    columns='Marketing_Bin',\n",
        "    values='Sales Revenue (USD)'\n",
        ")\n",
        "\n",
        "print(\"Average revenue by category and marketing spend:\")\n",
        "print(category_marketing_revenue_pivot)\n",
        "category_marketing_revenue_pivot.to_csv('/content/revenue_optimization/data/category_marketing_revenue.csv')\n",
        "\n",
        "# Plot heatmap of category-marketing revenue\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(category_marketing_revenue_pivot, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title('Average Revenue by Product Category and Marketing Spend')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/revenue_optimization/plots/category_marketing_revenue_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Find optimal marketing spend for each category\n",
        "category_optimal_marketing = pd.DataFrame()\n",
        "\n",
        "for category in df['Product Category'].unique():\n",
        "    category_data = category_marketing_revenue[category_marketing_revenue['Product Category'] == category]\n",
        "    optimal_marketing = category_data.loc[category_data['Marketing_ROI'].idxmax()]\n",
        "\n",
        "    category_optimal_marketing = pd.concat([\n",
        "        category_optimal_marketing,\n",
        "        pd.DataFrame({\n",
        "            'Product_Category': [category],\n",
        "            'Optimal_Marketing_Bin': [optimal_marketing['Marketing_Bin']],\n",
        "            'Optimal_Marketing_Spend': [optimal_marketing['Marketing Spend (USD)']],\n",
        "            'Average_Revenue': [optimal_marketing['Sales Revenue (USD)']],\n",
        "            'Marketing_ROI': [optimal_marketing['Marketing_ROI']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Optimal marketing spend by category:\")\n",
        "print(category_optimal_marketing)\n",
        "category_optimal_marketing.to_csv('/content/revenue_optimization/data/category_optimal_marketing.csv', index=False)\n",
        "\n",
        "# 4.4 Regional optimization strategies\n",
        "print(\"\\n4.4 Developing regional optimization strategies\")\n",
        "\n",
        "# Calculate optimal discount by region\n",
        "region_discount_revenue = df.groupby(['Region', 'Discount_Level']).agg({\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Units Sold': 'mean',\n",
        "    'Discount Percentage': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Find optimal discount level for each region\n",
        "region_optimal_discount = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = region_discount_revenue[region_discount_revenue['Region'] == region]\n",
        "    optimal_discount = region_data.loc[region_data['Sales Revenue (USD)'].idxmax()]\n",
        "\n",
        "    region_optimal_discount = pd.concat([\n",
        "        region_optimal_discount,\n",
        "        pd.DataFrame({\n",
        "            'Region': [region],\n",
        "            'Optimal_Discount_Level': [optimal_discount['Discount_Level']],\n",
        "            'Optimal_Discount_Percentage': [optimal_discount['Discount Percentage']],\n",
        "            'Average_Revenue': [optimal_discount['Sales Revenue (USD)']],\n",
        "            'Average_Units_Sold': [optimal_discount['Units Sold']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Optimal discount level by region:\")\n",
        "print(region_optimal_discount)\n",
        "region_optimal_discount.to_csv('/content/revenue_optimization/data/region_optimal_discount.csv', index=False)\n",
        "\n",
        "# Calculate optimal marketing spend by region\n",
        "region_marketing_revenue = df.groupby(['Region', 'Marketing_Bin']).agg({\n",
        "    'Sales Revenue (USD)': 'mean',\n",
        "    'Units Sold': 'mean',\n",
        "    'Marketing Spend (USD)': 'mean',\n",
        "    'Marketing_ROI': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Find optimal marketing spend for each region\n",
        "region_optimal_marketing = pd.DataFrame()\n",
        "\n",
        "for region in df['Region'].unique():\n",
        "    region_data = region_marketing_revenue[region_marketing_revenue['Region'] == region]\n",
        "    optimal_marketing = region_data.loc[region_data['Marketing_ROI'].idxmax()]\n",
        "\n",
        "    region_optimal_marketing = pd.concat([\n",
        "        region_optimal_marketing,\n",
        "        pd.DataFrame({\n",
        "            'Region': [region],\n",
        "            'Optimal_Marketing_Bin': [optimal_marketing['Marketing_Bin']],\n",
        "            'Optimal_Marketing_Spend': [optimal_marketing['Marketing Spend (USD)']],\n",
        "            'Average_Revenue': [optimal_marketing['Sales Revenue (USD)']],\n",
        "            'Marketing_ROI': [optimal_marketing['Marketing_ROI']]\n",
        "        })\n",
        "    ])\n",
        "\n",
        "print(\"Optimal marketing spend by region:\")\n",
        "print(region_optimal_marketing)\n",
        "region_optimal_marketing.to_csv('/content/revenue_optimization/data/region_optimal_marketing.csv', index=False)\n",
        "\n",
        "# 4.5 Combined optimization strategy\n",
        "print(\"\\n4.5 Creating combined optimization strategy\")\n",
        "\n",
        "# Merge optimization results\n",
        "combined_strategy = pd.merge(\n",
        "    region_top_category,\n",
        "    region_optimal_discount,\n",
        "    on='Region'\n",
        ")\n",
        "\n",
        "combined_strategy = pd.merge(\n",
        "    combined_strategy,\n",
        "    region_optimal_marketing,\n",
        "    on='Region'\n",
        ")\n",
        "\n",
        "# Calculate potential revenue improvement\n",
        "current_revenue = region_revenue[['Region', 'Sales Revenue (USD)_mean']]\n",
        "current_revenue.columns = ['Region', 'Current_Average_Revenue']\n",
        "\n",
        "combined_strategy = pd.merge(\n",
        "    combined_strategy,\n",
        "    current_revenue,\n",
        "    on='Region'\n",
        ")\n",
        "\n",
        "# Calculate potential improvement\n",
        "combined_strategy['Potential_Revenue'] = combined_strategy['Average_Revenue_x']\n",
        "combined_strategy['Potential_Improvement'] = combined_strategy['Potential_Revenue'] - combined_strategy['Current_Average_Revenue']\n",
        "combined_strategy['Improvement_Percentage'] = (combined_strategy['Potential_Improvement'] / combined_strategy['Current_Average_Revenue']) * 100\n",
        "\n",
        "# Sort by improvement potential\n",
        "combined_strategy = combined_strategy.sort_values('Potential_Improvement', ascending=False)\n",
        "\n",
        "print(\"Combined optimization strategy:\")\n",
        "print(combined_strategy[['Region', 'Top_Category', 'Optimal_Discount_Level', 'Optimal_Marketing_Bin', 'Potential_Improvement', 'Improvement_Percentage']])\n",
        "combined_strategy.to_csv('/content/revenue_optimization/data/combined_optimization_strategy.csv', index=False)\n",
        "\n",
        "# Plot improvement potential\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(combined_strategy['Region'], combined_strategy['Improvement_Percentage'])\n",
        "plt.title('Potential Revenue Improvement by Region (%)')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Improvement Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/revenue_optimization/plots/revenue_improvement_potential.png')\n",
        "plt.close()\n",
        "\n",
        "# 5. Generate comprehensive insights report\n",
        "print(\"\\n5. Generating comprehensive insights report...\")\n",
        "\n",
        "with open('/content/revenue_optimization/revenue_optimization_insights.md', 'w') as f:\n",
        "    f.write(\"# Revenue Optimization Analysis Insights\\n\\n\")\n",
        "\n",
        "    f.write(\"## 1. Category Revenue Performance\\n\\n\")\n",
        "\n",
        "    # Top and bottom categories\n",
        "    top_category = category_revenue.iloc[0]\n",
        "    bottom_category = category_revenue.iloc[-1]\n",
        "\n",
        "    f.write(f\"- **Top Revenue Category**: {top_category['Product Category']} (${top_category['Sales Revenue (USD)_sum']:,.2f} total revenue)\\n\")\n",
        "    f.write(f\"- **Bottom Revenue Category**: {bottom_category['Product Category']} (${bottom_category['Sales Revenue (USD)_sum']:,.2f} total revenue)\\n\")\n",
        "    f.write(f\"- **Revenue Gap**: The top category generates {top_category['Sales Revenue (USD)_sum'] / bottom_category['Sales Revenue (USD)_sum']:.1f}x more revenue than the bottom category\\n\\n\")\n",
        "\n",
        "    f.write(\"## 2. Regional Revenue Performance\\n\\n\")\n",
        "\n",
        "    # Top and bottom regions\n",
        "    top_region = region_revenue.iloc[0]\n",
        "    bottom_region = region_revenue.iloc[-1]\n",
        "\n",
        "    f.write(f\"- **Top Revenue Region**: {top_region['Region']} (${top_region['Sales Revenue (USD)_sum']:,.2f} total revenue)\\n\")\n",
        "    f.write(f\"- **Bottom Revenue Region**: {bottom_region['Region']} (${bottom_region['Sales Revenue (USD)_sum']:,.2f} total revenue)\\n\")\n",
        "    f.write(f\"- **Revenue Gap**: The top region generates {top_region['Sales Revenue (USD)_sum'] / bottom_region['Sales Revenue (USD)_sum']:.1f}x more revenue than the bottom region\\n\\n\")\n",
        "\n",
        "    f.write(\"## 3. Category-Region Optimization\\n\\n\")\n",
        "\n",
        "    f.write(\"### Top Revenue-Generating Category by Region\\n\\n\")\n",
        "\n",
        "    f.write(\"| Region | Top Category | Revenue | Units Sold | Revenue per Unit |\\n\")\n",
        "    f.write(\"|--------|-------------|---------|------------|------------------|\\n\")\n",
        "\n",
        "    for _, row in region_top_category.iterrows():\n",
        "        f.write(f\"| {row['Region']} | {row['Top_Category']} | ${row['Revenue']:,.2f} | {row['Units_Sold']:,.0f} | ${row['Revenue_per_Unit']:,.2f} |\\n\")\n",
        "\n",
        "    f.write(\"\\n### Optimal Discount Level by Category\\n\\n\")\n",
        "\n",
        "    f.write(\"| Category | Optimal Discount Level | Discount Percentage | Average Revenue | Average Units Sold |\\n\")\n",
        "    f.write(\"|----------|------------------------|---------------------|----------------|-------------------|\\n\")\n",
        "\n",
        "    for _, row in category_optimal_discount.iterrows():\n",
        "        f.write(f\"| {row['Product_Category']} | {row['Optimal_Discount_Level']} | {row['Optimal_Discount_Percentage']:.1f}% | ${row['Average_Revenue']:,.2f} | {row['Average_Units_Sold']:,.1f} |\\n\")\n",
        "\n",
        "    f.write(\"\\n### Optimal Marketing Spend by Category\\n\\n\")\n",
        "\n",
        "    f.write(\"| Category | Optimal Marketing Level | Marketing Spend | Average Revenue | Marketing ROI |\\n\")\n",
        "    f.write(\"|----------|-------------------------|----------------|----------------|---------------|\\n\")\n",
        "\n",
        "    for _, row in category_optimal_marketing.iterrows():\n",
        "        f.write(f\"| {row['Product_Category']} | {row['Optimal_Marketing_Bin']} | ${row['Optimal_Marketing_Spend']:,.2f} | ${row['Average_Revenue']:,.2f} | {row['Marketing_ROI']:,.2f} |\\n\")\n",
        "\n",
        "    f.write(\"\\n## 4. Combined Regional Optimization Strategy\\n\\n\")\n",
        "\n",
        "    f.write(\"| Region | Focus Category | Optimal Discount | Optimal Marketing | Potential Improvement | Improvement % |\\n\")\n",
        "    f.write(\"|--------|----------------|-----------------|-------------------|----------------------|---------------|\\n\")\n",
        "\n",
        "    for _, row in combined_strategy.iterrows():\n",
        "        f.write(f\"| {row['Region']} | {row['Top_Category']} | {row['Optimal_Discount_Level']} | {row['Optimal_Marketing_Bin']} | ${row['Potential_Improvement']:,.2f} | {row['Improvement_Percentage']:.1f}% |\\n\")\n",
        "\n",
        "    f.write(\"\\n## 5. Key Optimization Insights\\n\\n\")\n",
        "\n",
        "    # Calculate average improvement\n",
        "    avg_improvement = combined_strategy['Improvement_Percentage'].mean()\n",
        "    total_improvement = combined_strategy['Potential_Improvement'].sum()\n",
        "\n",
        "    f.write(f\"- **Average Potential Improvement**: {avg_improvement:.1f}% increase in revenue per transaction\\n\")\n",
        "    f.write(f\"- **Total Potential Improvement**: ${total_improvement:,.2f} per transaction\\n\")\n",
        "\n",
        "    # Identify regions with highest improvement potential\n",
        "    high_potential_regions = combined_strategy.head(3)['Region'].tolist()\n",
        "    f.write(f\"- **Highest Potential Regions**: {', '.join(high_potential_regions)}\\n\")\n",
        "\n",
        "    # Identify most common optimal discount level\n",
        "    most_common_discount = category_optimal_discount['Optimal_Discount_Level'].mode()[0]\n",
        "    f.write(f\"- **Most Common Optimal Discount Level**: {most_common_discount}\\n\")\n",
        "\n",
        "    # Identify most common optimal marketing level\n",
        "    most_common_marketing = category_optimal_marketing['Optimal_Marketing_Bin'].mode()[0]\n",
        "    f.write(f\"- **Most Common Optimal Marketing Level**: {most_common_marketing}\\n\\n\")\n",
        "\n",
        "    f.write(\"## 6. Implementation Recommendations\\n\\n\")\n",
        "\n",
        "    f.write(\"### Short-Term Actions (0-3 months)\\n\")\n",
        "    f.write(\"1. Adjust discount levels by category to match optimal levels identified\\n\")\n",
        "    f.write(\"2. Reallocate marketing spend to align with optimal levels by category\\n\")\n",
        "    f.write(\"3. Focus promotional efforts on top-performing category in each region\\n\\n\")\n",
        "\n",
        "    f.write(\"### Medium-Term Actions (3-6 months)\\n\")\n",
        "    f.write(\"1. Develop region-specific marketing campaigns for top categories\\n\")\n",
        "    f.write(\"2. Implement A/B testing of discount strategies to validate findings\\n\")\n",
        "    f.write(\"3. Create cross-selling strategies between top categories and other products\\n\\n\")\n",
        "\n",
        "    f.write(\"### Long-Term Actions (6-12 months)\\n\")\n",
        "    f.write(\"1. Develop comprehensive regional optimization strategies\\n\")\n",
        "    f.write(\"2. Implement dynamic pricing based on regional sensitivity\\n\")\n",
        "    f.write(\"3. Create region-specific product assortments based on performance data\\n\\n\")\n",
        "\n",
        "    f.write(\"## 7. Limitations and Considerations\\n\\n\")\n",
        "\n",
        "    f.write(\"- **Revenue Focus**: This analysis focuses solely on revenue optimization without considering costs or profitability\\n\")\n",
        "    f.write(\"- **Data Limitations**: Analysis based on available data, which may not capture all relevant factors\\n\")\n",
        "    f.write(\"- **External Factors**: Market conditions, competition, and economic factors not fully accounted for\\n\")\n",
        "    f.write(\"- **Implementation Challenges**: Organizational constraints and change management considerations not addressed\\n\")\n",
        "    f.write(\"- **Customer Response**: Actual customer response to changes may differ from historical patterns\\n\")\n",
        "\n",
        "print(\"Comprehensive insights report generated and saved to /content/revenue_optimization/revenue_optimization_insights.md\")\n",
        "\n",
        "# 6. Print completion message\n",
        "print(\"\\nStep 6: Revenue Optimization Analysis completed successfully!\")\n",
        "print(\"The analysis results are ready for review and further steps.\")\n",
        "print(\"###  Business takeaway")\n",
        "print(\"10 % deeper discount  6 % revenue lift (p < 0.05). ")\n",
        "print(\" *243 stores, 3.8 M rows, 40 % faster queries after optimisation.*")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

